{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLLVM for longitudinal data: a pytorch implementation\n",
    "\n",
    "We consider the following model\n",
    "\n",
    "TODO:\n",
    "* ~~learn the nuisance parameters! To learn phi, make the encoder learn the zhats **before** AR(1) is applied, and take the gradient of the decoder. that is, make the decoder have phi as a parameter, and same for var_mu!~~ \n",
    "* ~~allow for gaussian and binary data~~ \n",
    "* ~~Compute the gradient that define the model~~\n",
    "* allow for missing values and impute them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Model Specification\n",
    "\n",
    "Let  $y_{i1t},y_{i2t},\\ldots,y_{ipt}$  be a set of $p$ response or observed variables at time $t,\\ t=1,\\ldots,T$ for  individual $i,\\ i=1,\\ldots,n$. Let $\\mathbf{x}_{it}$ be a set of observed $k$-dimensional covariates at time $t,\\ t=1,\\ldots,T$.\n",
    "\n",
    "Models for multivariate longitudinal data have to account for the three sources of variability\n",
    "present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged\n",
    "associations between different responses at different occasions, and  (iii) the association between repeated measures of the same response\n",
    "over time. The first source of variability is accounted for\n",
    "a time-dependent latent variable $z_{i1}, z_{i2},\\ldots,z_{iT}$. Modeling the temporal evolution of the latent variable accounts for the cross-lagged associations between different responses over time.\n",
    "The third source of variability can be accounted for a set of item-specific random effects $\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'$.\n",
    "\n",
    "According to the GLLVM framework we have\n",
    "\n",
    "\\begin{align*}\n",
    "   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n",
    "   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n",
    "\\end{align*}\n",
    "where $g_j(\\cdot),j=1,\\ldots,p$ is a known {\\it link function}, $\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T$ is the {\\it linear predictor},  and $\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)$ denotes a distribution from the exponential family with mean $\\mu_{ijt}$ and response variable-specific dispersion parameter $\\tau_j$. \\vspace{5pt}\\\\\n",
    "The dynamics of the latent variable over time is modelled through a non-stationary autoregressive model of first order\n",
    "\n",
    "\\begin{equation*}\n",
    "z_{it}=\\phi z_{i,t-1} + \\delta_{it}\n",
    "\\end{equation*}%\n",
    "where  $z_{i1}\\sim N(0,\\sigma^2_{1})$ and $\\delta_{it}\\sim N(0,1)$.  Moreover, we assume the random effects independent of the latent variable and their common distribution $\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)$.\n",
    "\n",
    "\n",
    "### Changes I propose:\n",
    "\n",
    "allow x to depend on time.\n",
    "\n",
    "\n",
    "## Measurement invariance\n",
    "\n",
    "The  latent variable $z_{it}$ has to be the same (same meaning) across occasions.\n",
    "Thus the measurement invariance assumption has to be tested on the data, that is \n",
    "all the measurement parameters  are invariant across occasions, that is $$\\beta_{0jt}=\\beta_{0j} \\ \\textrm{and } \\ \\lambda_{jt}=\n",
    "\\lambda_{j},$$ for all $t$, $t=1, \\ldots, T$ and for all $j$, $j=1,\\ldots, p$.\n",
    "Under this assumption, the model is more parsimonious  and avoids some possible identification problem that might arise with\n",
    "increasing the number of time points.\n",
    "\n",
    "To ensure identification of the model, one necessary condition is that the latent variable has a scale and an origin. %When measurement\n",
    "%invariance of loadings and intercepts is imposed,\n",
    "Scale for  $z_{it}$  can be provided either by fixing one loading at a nonzero value or by\n",
    "fixing the factor variance at a nonzero value. In presence of longitudinal data, the same loading is fixed equal to one at each occasion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast prototyping using Pytorch\n",
    "\n",
    "We model each observation as a tuple of dimension `(T, p)`, common across individuals. Individuals constitute independent observations, which yields the tensor structure `(n, T, q)`. The time dimension `T` appears in the first dimension since it allows for seamless tensor products of the type `(n, T, q) (q, p)`.\n",
    "\n",
    "We need\n",
    "\n",
    "* A model that defines the generative process\n",
    "* A loss function which, upon taking the derivative, re-creates the estimating equations\n",
    "* A way to compute the latent variables. For now, we will use a neural network. Later, we can implement the real function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessed data\n",
    "data = np.load(\"../data/HRS/HRS_3Darray.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "The data has been pre-processed as a 3D array with dimensions given by $(n, k+p, T)$.\n",
    "\n",
    "The final array is a 3D array of shape (n, k+p, T), where \n",
    "* n is the number of observations, \n",
    "* k is the number of covariates,\n",
    "* p is the number of responses to be modeled by the latent variables,\n",
    "* T is the number of time observations\n",
    "\n",
    "The variables are ordered as follows:\n",
    "\n",
    "**Covariates**\n",
    "\n",
    "- age (covariate)\n",
    "- gender (covariate)\n",
    "- education (covariate)\n",
    "\n",
    "**Responses**\n",
    "\n",
    "- month (binary)\n",
    "- day (binary)\n",
    "- year (binary)\n",
    "- day_week (binary)\n",
    "- paper (binary)\n",
    "- cactus (binary)\n",
    "- president (binary)\n",
    "- vice_president (binary)\n",
    "- serie_7 (count)\n",
    "- backwards (ordinal)\n",
    "- recall_immediate (count)\n",
    "- recall_delayed (count)\n",
    "\n",
    "The binary random variables are modelled conditionally with a logistic regression; the ordinal variables with an ordinal logistic regression; the counts with poisson regression.\n",
    "\n",
    "The ordinal responses require an intercept for each level, or \"cutpoints\". In our case, we model this by hard-coding the intercept, but it should be made more general. We assume the effect of the covariates is constant across categories, resulting in a proportional odds model.\n",
    "\n",
    "We need to split the ordinal \"backwards\" into variable into two new variables: backwards_1 and backwards_2. The responses become:\n",
    "\n",
    "**Covariates**\n",
    "\n",
    "- 0: age (covariate)\n",
    "- 1: gender (covariate)\n",
    "- 2: education (covariate)\n",
    "\n",
    "**Responses**\n",
    "\n",
    "- 0: month (binary)\n",
    "- 1: day (binary)\n",
    "- 2: year (binary)\n",
    "- 3: day_week (binary)\n",
    "- 4: paper (binary)\n",
    "- 5: cactus (binary)\n",
    "- 6: president (binary)\n",
    "- 7: vice_president (binary)\n",
    "- 8: serie_7 (count)\n",
    "- 9: backwards_1 (dummy)\n",
    "- 10: backwards_2 (dummy)\n",
    "- 11: recall_immediate (count)\n",
    "- 12: recall_delayed (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the backwards_1 and backwards_2 dummies:\n",
    "backwards_1 = (data[:,12]==1)\n",
    "backwards_2 = (data[:,12, np.newaxis]==2)\n",
    "\n",
    "# insert them\n",
    "data[:,12] = backwards_1\n",
    "data = np.concatenate([data[:,:12], backwards_2, data[:,12:]], axis=1)\n",
    "\n",
    "np.nanmax(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate covariates and responses, and prepare the torch tensors\n",
    "\n",
    "data = np.transpose(data, (0,2,1))\n",
    "data_true = {\n",
    "    'x': torch.from_numpy(data[:,:,:3]).float(),\n",
    "    'y': torch.from_numpy(data[:,:,3:]).float()\n",
    "}\n",
    "\n",
    "response_types = {\n",
    "    'binary': [0,1,2,3,4,5,6,7],\n",
    "    'ordinal': [9, 10],\n",
    "    'counts': [8, 11, 12]\n",
    "}\n",
    "\n",
    "# We define two inverse link functions; one for sampling (the one assumed in the model), and another for computations (the one used to compute gradients)\n",
    "linkinv_sample = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: torch.exp(x) \n",
    "}\n",
    "\n",
    "linkinv_decoder = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: x # see below remarks for the standardization of the \"counts\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x: torch.Size([33146, 9, 3])\n",
      "shape of y: torch.Size([33146, 9, 13])\n"
     ]
    }
   ],
   "source": [
    "# The shape must be (n, T, p)\n",
    "\n",
    "for key, value in data_true.items():\n",
    "    print(f'shape of {key}: {value.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with 33146 observations, 3 covariates, 13 responses, 9 time periods, 1 latent variable per time period.\n"
     ]
    }
   ],
   "source": [
    "n = data_true['x'].shape[0]\n",
    "k = data_true['x'].shape[2]\n",
    "p = data_true['y'].shape[2]\n",
    "T = data_true['y'].shape[1]\n",
    "q = 1\n",
    "\n",
    "print(f'Data with {n} observations, {k} covariates, {p} responses, {T} time periods, {1} latent variable per time period.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  dealing with missing data\n",
    "We deal with missing data in two ways:\n",
    "\n",
    "* The missing values are iteratively imputed by their conditional mean.\n",
    "* To compute the Z, we use the imputed means.\n",
    "* To compute the parameters, we use the imputed means and estimated Z but *provide the mask* so that the update does not take the missing values into account in the computation of the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = {}\n",
    "\n",
    "mask['x'] = torch.isnan(data_true['x'])\n",
    "mask['y'] = torch.isnan(data_true['y'])\n",
    "\n",
    "data_true['x'][mask['x']] = 0\n",
    "data_true['y'][mask['y']] = 0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data standardization\n",
    "\n",
    "To improve numerical stability, we transform the counts as $y\\to \\log(y+1)$. This is reflected in the sampling mechanics. As a result, we use the identity as a link function for the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical stability issues for poisson data\n",
    "\n",
    "data_true['x'][:,:,0] = data_true['x'][:,:,0] /100\n",
    "# For numerical stability, we transform the counts (poisson) as y -> log(y+1)\n",
    "# This is reflected in our sampling mechanism so we don't introduce a bias\n",
    "data_true['y'][:,:,response_types['counts']] = torch.log(data_true['y'][:,:,response_types['counts']] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33146, 9, 3])\n",
      "torch.Size([33146, 9, 13])\n"
     ]
    }
   ],
   "source": [
    "print(data_true['x'].shape)\n",
    "print(data_true['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIMENSIONS_Y = (n, T, p) \n",
    "DIMENSIONS_X = (n, T, k)\n",
    "DIMENSIONS_Z = (n, T, q)\n",
    "DIMENSIONS_U = (n, 1, p)\n",
    "\n",
    "# TODO: refactor to have a module of two modules: encoder and decoder\n",
    "class GLLVM_longitudinal():\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def encoder_fit(self, x, y, d, u, epochs=100, verbose=False):\n",
    "        encoder_loss = nn.MSELoss()\n",
    "        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            (dhat, uhat) = self.encoder(x, y)\n",
    "            loss = encoder_loss(dhat, d) + encoder_loss(uhat, u) # TODO: check that it's correct,maybe Z needs to be weighed more than u\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            encoder_opt.zero_grad()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def sample(self, n, x=None, u=None, d=None):\n",
    "        with torch.no_grad():\n",
    "            \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, u, d, y)\"\"\"\n",
    "\n",
    "            if x is None:\n",
    "                Warning(\"x was set to None for sampling. This is usually unwanted unless k=0.\")\n",
    "                x = torch.randn((n, T, k))\n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, p))\n",
    "            if d is None:\n",
    "                d = torch.randn((n, T, q))\n",
    "\n",
    "            # for normal responses: eps = torch.randn((n, T, p)) * torch.sqrt(self.decoder.var_y)\n",
    "\n",
    "            y = self.decoder(x, d, u)\n",
    "            for response_type, response_id in response_types.items():\n",
    "                if response_type == \"binary\":\n",
    "                    y[:,:,response_id] = torch.bernoulli(y[:,:,response_id])\n",
    "                elif response_type == \"ordinal\":\n",
    "                    y_ordinal = y[:,:,response_id]\n",
    "                    # draw one uniform for the whole vector\n",
    "                    random = torch.rand((*y_ordinal.shape[0:2], 1)) \n",
    "                    # compare with the cumulative probabilities\n",
    "                    ordinal = torch.sum(random < y_ordinal, axis=2)\n",
    "                    ordinal = torch.nn.functional.one_hot(ordinal).squeeze().float()\n",
    "                    ordinal = ordinal[:,:,1:] # discard the first column of the one_hot encoding, as it is superfluous (as a 0)\n",
    "                    y[:,:,response_id] = ordinal\n",
    "                elif response_type == \"counts\":\n",
    "                    y[:,:,response_id] = torch.log(torch.poisson(torch.exp(y[:,:,response_id]))+1) # here we transform the mean using the inverse link - this is due to the transformation of the poisson variable intot a \"gaussin\". For sampling, we actually want the gaussian.\n",
    "                     \n",
    "            return {\"x\":x, \"u\":u, \"d\":d, \"y\":y}\n",
    "    \n",
    "    def impute(self, x, y, mask, nsteps=10):\n",
    "        \"\"\" Impute the missing values provided by the mask (True is missing) and return x imputed\"\"\"\n",
    "        for _ in range(nsteps):\n",
    "            d, u = self.encoder(x,y)\n",
    "            y_decoded = self.decoder(x,d,u)\n",
    "            y[mask] = y_decoded[mask]\n",
    "        return y\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        self.wz = nn.Parameter(torch.randn((q, p)))/10\n",
    "        self.wx = nn.Parameter(torch.randn((T, k, p)))/10\n",
    "        self.bias = nn.Parameter(torch.zeros((1, T, p)))/10\n",
    "        self.phi = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        # nuisance parameters\n",
    "        self.var_u = nn.Parameter(torch.ones((1,1,p)))\n",
    "        self.var_y = nn.Parameter(torch.ones((1,1,p))) # this appears in the loss function, but we know all of them to be 1 by design\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, d, u):\n",
    "        z = self.AR(d)\n",
    "        u = u * torch.sqrt(self.var_u)\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see section \"details of tensorproducts\"\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "        # for the ordinal variables:\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "\n",
    "        # Apply the inverse link to get the conditional expectation\n",
    "        yhat  = torch.zeros_like(linpar)\n",
    "        for response_type, response_id in response_types.items():\n",
    "            yhat[:,:,response_id] = linkinv_decoder[response_type](linpar[:,:,response_id])\n",
    "        # Transform the \n",
    "        return yhat\n",
    "\n",
    "    def AR(self, d):\n",
    "        z = d.clone()\n",
    "\n",
    "        for t in range(1, z.shape[1]):\n",
    "            z[:,t] = z[:,t] + z[:, t-1].clone() * self.phi  # we need to clone else the gradient wants to pass through it\n",
    "        return z\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n",
    "        # output dimension is T*q  + p (for Z and U, respectively)\n",
    "        self.enc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=T*(p+k), out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = T*q + p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n",
    "        zu = self.enc_model(xy)\n",
    "        return self.split_zu(zu)\n",
    "\n",
    "    def split_zu(self, du):\n",
    "        #output dimension of size (T*Z), p\n",
    "        d, u = torch.split(du, [T*q, p], dim=1)\n",
    "        d = d.reshape((d.shape[0], T, q))\n",
    "        u = u.unsqueeze(1)\n",
    "        return (d, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_true = GLLVM_longitudinal()\n",
    "dat_sample = gl_true.sample(n, x=data_true['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.6094, 1.3863],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.3863, 1.3863],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.7918, 2.0794],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.9459, 1.9459],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.7918, 1.9459],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.7918, 1.7918],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 2.0794, 1.7918],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 2.0794, 1.9459],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 2.0794, 1.7918],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.7918, 1.7918],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 2.3026, 2.1972],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.9459, 1.7918]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.6094, 1.6094],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.3863, 0.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 1.0986, 1.3863]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_true['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLLVM_longitudinal()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.decoder.phi *= .8\n",
    "    model.decoder.var_y *= 1.1\n",
    "    model.decoder.var_u *=.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(input, target, var_y, mask, sign=1):\n",
    "    #return sign * torch.sum(input*target / torch.sqrt(var_y), dim=[1,2]).mean()\n",
    "    return sign * torch.sum((input*target / torch.sqrt(var_y)) * mask)/torch.sum(torch.logical_not(mask)) + torch.log(var_y).mean()\n",
    "\n",
    "def evaluate_fit(input, target, mask):\n",
    "    with torch.no_grad():\n",
    "        return torch.sum(torch.pow((input - target)*mask,2))/torch.sum(~mask)\n",
    "    \n",
    "class OneLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y, yhat, ys, yshat, mask):\n",
    "        \"\"\"Computes the loss. hat is recontructed y, ys is simulated\"\"\"\n",
    "        loss_sample = torch.sum(y * yhat * mask) / torch.sum(~mask)\n",
    "        loss_simulated = torch.sum(ys *  yshat * mask) / torch.sum(~mask)\n",
    "\n",
    "        return loss_sample - loss_simulated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8200, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_sim = model.sample(n, x=data_true[\"x\"]) # x are known and fixed\n",
    "# train the encoder on a first pass\n",
    "model.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"d\"], dat_sim[\"u\"], epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "\n",
      "Epoch 1/100, loss_fit = 0.40, encoder_loss = 1.72, phi= 0.8009999990463257, var_u =0.8009999990463257, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 2/100, loss_fit = 0.37, encoder_loss = 1.68, phi= 0.8019998073577881, var_u =0.8019896745681763, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 3/100, loss_fit = 0.36, encoder_loss = 1.62, phi= 0.8029583692550659, var_u =0.8027511835098267, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 4/100, loss_fit = 0.35, encoder_loss = 1.58, phi= 0.8039106130599976, var_u =0.8034477233886719, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 5/100, loss_fit = 0.35, encoder_loss = 1.54, phi= 0.8048828840255737, var_u =0.8042457699775696, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 6/100, loss_fit = 0.34, encoder_loss = 1.51, phi= 0.8058578968048096, var_u =0.8050645589828491, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 7/100, loss_fit = 0.33, encoder_loss = 1.48, phi= 0.8068483471870422, var_u =0.8059373497962952, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 8/100, loss_fit = 0.32, encoder_loss = 1.46, phi= 0.8078476190567017, var_u =0.8068329095840454, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 9/100, loss_fit = 0.34, encoder_loss = 1.44, phi= 0.808857798576355, var_u =0.8077652454376221, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 10/100, loss_fit = 0.33, encoder_loss = 1.43, phi= 0.8098704814910889, var_u =0.8087177872657776, var_y=1.100000023841858\n",
      "tensor(0.)\n",
      "\n",
      "Epoch 11/100, loss_fit = 0.32, encoder_loss = 1.43, phi= 0.8108976483345032, var_u =0.809682309627533, var_y=1.100000023841858\n",
      "tensor(0.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m yshat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecoder(dat_sim[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m], dhat_sim, uhat_sim)\n\u001b[0;32m     30\u001b[0m \u001b[39m# compute the loss\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m loss \u001b[39m=\u001b[39m criterion(yhat, data_true[\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m],  yshat, dat_sim[\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m], mask[\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# we need to retain it because the loss function requires 2 passes through the decoder.. TODO: ACTUALLY THIS IS NOT THE BEHVAIOR I WANT, \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():   \n",
      "File \u001b[1;32mc:\\Users\\Willwhite\\AppData\\Local\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[63], line 15\u001b[0m, in \u001b[0;36mOneLoss.forward\u001b[1;34m(self, y, yhat, ys, yshat, mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y, yhat, ys, yshat, mask):\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\"\"Computes the loss. hat is recontructed y, ys is simulated\"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     loss_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msum(y \u001b[39m*\u001b[39;49m yhat \u001b[39m*\u001b[39;49m mask) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39m~\u001b[39mmask)\n\u001b[0;32m     16\u001b[0m     loss_simulated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(ys \u001b[39m*\u001b[39m  yshat \u001b[39m*\u001b[39m mask) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39m~\u001b[39mmask)\n\u001b[0;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_sample \u001b[39m-\u001b[39m loss_simulated\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = OneLoss()\n",
    "learning_rate = 0.001\n",
    "max_gradient_value = 0.1\n",
    "optimizer = torch.optim.Adam(model.decoder.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "mseLoss = nn.MSELoss()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs +1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(torch.min(data_true[\"y\"]))\n",
    "\n",
    "    # impute and sample\n",
    "    with torch.no_grad():\n",
    "        # data_true[\"y\"] = model.impute(data_true[\"x\"], data_true[\"y\"], mask=mask[\"y\"], nsteps=5)\n",
    "        # simulate data from the current parameter values, Unconditionally (!)\n",
    "        dat_sim = model.sample(n, x=data_true[\"x\"])\n",
    "        # compute the imputing values without the gradients\n",
    "        dhat_sample, uhat_sample = model.encoder(data_true[\"x\"], data_true[\"y\"])\n",
    "        dhat_sim, uhat_sim = model.encoder(dat_sim[\"x\"], dat_sim[\"y\"])\n",
    "\n",
    "    # train the encoder \n",
    "    encoder_loss = model.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"d\"], dat_sim[\"u\"], epochs= 10 ) \n",
    "    \n",
    "    # compute the decoded value\n",
    "    yhat = model.decoder(data_true[\"x\"], dhat_sample, uhat_sample)\n",
    "    yshat = model.decoder(dat_sim[\"x\"], dhat_sim, uhat_sim)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = criterion(yhat, data_true[\"y\"],  yshat, dat_sim[\"y\"], mask[\"y\"])\n",
    "\n",
    "    loss.backward(retain_graph=True) # we need to retain it because the loss function requires 2 passes through the decoder.. TODO: ACTUALLY THIS IS NOT THE BEHVAIOR I WANT, \n",
    "    with torch.no_grad():   \n",
    "        fit = mseLoss(yhat, data_true[\"y\"])\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), max_norm=.1)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}, loss_fit = {fit.item():.2f}, encoder_loss = {encoder_loss.item():.2f}, phi= {model.decoder.phi.item()}, var_u ={model.decoder.var_u[0,0,0].item()}, var_y={model.decoder.var_y[0,0,0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dhat, uhat = model.encoder(data_true[\"x\"], data_true[\"y\"])\n",
    "    yhat = model.decoder(data_true[\"x\"], dhat, uhat)\n",
    "\n",
    "plt.scatter(data_true[\"y\"], yhat)\n",
    "plt.plot([-10, 10], [-10, 10], color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.1015)"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhat = dhat*-1\n",
    "plt.scatter(dhat, data_true[\"d\"])\n",
    "plt.plot([-2, 2], [-2,2], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 12 randomly selected Z\n",
    "index = np.random.choice(range(n), 12, replace=False)\n",
    "fig, axs = plt.subplots(3, 4)\n",
    "\n",
    "zhat= -gl.decoder.AR(dhat).detach()\n",
    "z_true= gl_true.decoder.AR(data_true[\"d\"]).detach()\n",
    "fig.suptitle(\"Ztrue vs Zest across time\")\n",
    "for i in range(12):\n",
    "    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n",
    "    axs[i//4, i%4].plot(z_true[index[i],:, 0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_true = gl_true.decoder.parameters().__next__().detach().squeeze()\n",
    "par_est = gl.decoder.parameters().__next__() .detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(-par_true, par_est)\n",
    "plt.plot([-2,2], [2, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on tensor products calculations\n",
    "We now show the details on the tensor products, for instance for computing `xb @ wx`. `xb` is of size `(n, T, q)` and `wx` is of size `(T, q, p)`. We want a result of size `(n, T, p)`. First we add a dimension for `xb`:\n",
    "\n",
    "`xb.unsqueeze(2)` which yields a dimensions of `(n, T, 1, q)`\n",
    "\n",
    "which we then multiply by `wz`:\n",
    "\n",
    "`(n, T, 1, q) @ (1, q, p)` -> `(n, T, 1, p)`\n",
    "\n",
    "where the first dimension of `wx` has been broadcasted.\n",
    "\n",
    "Finally, we squeeze to obtain `(n, T, p)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on the computation of the forward pass\n",
    "\n",
    "The forward pass depends on $z_{it}$. According to the model, we have\n",
    "\n",
    "$$z_{i1} \\sim N(0,\\sigma_z^2).$$\n",
    "\n",
    "For identifiability reason, we have $z_{i1} = \\delta_{i1}$ where $\\delta_{i1}\\sim N(0,1)$\n",
    "\n",
    "and then\n",
    "\n",
    "$$z_{it+1} = \\phi z_{it} + \\delta{it}$$\n",
    "\n",
    "$$z_{i2} = \\phi \\delta_{i1} + \\delta_{i1}$$\n",
    "\n",
    "$$z_{i4} = \\phi z_{i3} + \\delta_{i3} \\Rightarrow z_{i3} = \\phi^2 \\delta_{i1} + \\phi \\delta_{i2} + \\delta_{i3}$$\n",
    "\n",
    "$$z_{it+1} = \\phi^{t-1}\\delta_{i1} + ... + \\phi \\delta_{it-1} + \\delta_{it}$$\n",
    "\n",
    "\n",
    "So we do not need in-place computations in the forward pass. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02379b91627ead677140c441995c323138285dea806245dca7a173ada35ac023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

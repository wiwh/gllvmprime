{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLLVM for longitudinal data: a pytorch implementation\n",
    "\n",
    "We consider the following model\n",
    "\n",
    "TODO:\n",
    "* learn the nuisance parameters! To learn phi, make the encoder learn the zhats **before** AR(1) is applied, and take the gradient of the decoder. that is, make the decoder have phi as a parameter, and same for var_mu!\n",
    "* allow for gaussian and binary data\n",
    "* Compute the gradient that define the model\n",
    "* allow for missing values and impute them\n",
    "* define mine own loss.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Model Specification\n",
    "\n",
    "Let  $y_{i1t},y_{i2t},\\ldots,y_{ipt}$  be a set of $p$ response or observed variables at time $t,\\ t=1,\\ldots,T$ for  individual $i,\\ i=1,\\ldots,n$. Let $\\mathbf{x}_{it}$ be a set of observed $k$-dimensional covariates at time $t,\\ t=1,\\ldots,T$.\n",
    "\n",
    "Models for multivariate longitudinal data have to account for the three sources of variability\n",
    "present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged\n",
    "associations between different responses at different occasions, and  (iii) the association between repeated measures of the same response\n",
    "over time. The first source of variability is accounted for\n",
    "a time-dependent latent variable $z_{i1}, z_{i2},\\ldots,z_{iT}$. Modeling the temporal evolution of the latent variable accounts for the cross-lagged associations between different responses over time.\n",
    "The third source of variability can be accounted for a set of item-specific random effects $\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'$.\n",
    "\n",
    "According to the GLLVM framework we have\n",
    "\n",
    "\\begin{align*}\n",
    "   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n",
    "   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n",
    "\\end{align*}\n",
    "where $g_j(\\cdot),j=1,\\ldots,p$ is a known {\\it link function}, $\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T$ is the {\\it linear predictor},  and $\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)$ denotes a distribution from the exponential family with mean $\\mu_{ijt}$ and response variable-specific dispersion parameter $\\tau_j$. \\vspace{5pt}\\\\\n",
    "The dynamics of the latent variable over time is modelled through a non-stationary autoregressive model of first order\n",
    "\n",
    "\\begin{equation*}\n",
    "z_{it}=\\phi z_{i,t-1} + \\delta_{it}\n",
    "\\end{equation*}%\n",
    "where  $z_{i1}\\sim N(0,\\sigma^2_{1})$ and $\\delta_{it}\\sim N(0,1)$.  Moreover, we assume the random effects independent of the latent variable and their common distribution $\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)$.\n",
    "\n",
    "\n",
    "\n",
    "## Measurement invariance\n",
    "\n",
    "The  latent variable $z_{it}$ has to be the same (same meaning) across occasions.\n",
    "Thus the measurement invariance assumption has to be tested on the data, that is \n",
    "all the measurement parameters  are invariant across occasions, that is $$\\beta_{0jt}=\\beta_{0j} \\ \\textrm{and } \\ \\lambda_{jt}=\n",
    "\\lambda_{j},$$ for all $t$, $t=1, \\ldots, T$ and for all $j$, $j=1,\\ldots, p$.\n",
    "Under this assumption, the model is more parsimonious  and avoids some possible identification problem that might arise with\n",
    "increasing the number of time points.\n",
    "\n",
    "To ensure identification of the model, one necessary condition is that the latent variable has a scale and an origin. %When measurement\n",
    "%invariance of loadings and intercepts is imposed,\n",
    "Scale for  $z_{it}$  can be provided either by fixing one loading at a nonzero value or by\n",
    "fixing the factor variance at a nonzero value. In presence of longitudinal data, the same loading is fixed equal to one at each occasion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Modeling\n",
    "\n",
    "We model each observation as a tuple of dimension `(T, p)`, common across individuals. Individuals constitute independent observations, which yields the tensor structure `(n, T, q)`. The time dimension `T` appears in the first dimension since it allows for seamless tensor products of the type `(n, T, q) (q, p)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of the `GLLVM` architecture is as follows: we need\n",
    "\n",
    "* a GLLVM module, which is the model, and contains methods to sample, and has an `AR1` module as well as a `CondMean` module: the first creates the auto-regressive sequence, the second \n",
    "* the `AR1` module, given a sample of independent noise `delta` and parameter `phi`, returns the auto-regressive process according to the above formula\n",
    "* the `CondMean` module which computes the conditional mean which is used in the loss function. This depends on the weights `wx`, `wz`, the biases `b`, and t\n",
    "* a sample method to generate the random variables jointly from the GLLVM. This includes the noise `delta`, the individual-specific random intercepts `u`, and the responses of various types. Additionally, for responses (features) types that require a scale parameter, this includes the `scale` parameter.  \n",
    "* an `Encoder` module that maps the features to the `unmodified` latent variables `u` and `d` (before any parameter affects them), which is learned on the samples, who has its own architecture and parameters which we do not directly care about.\n",
    "* our very own M-estimator loss function\n",
    "* interestingly, the number of parameters do not depend on time nor on number of observations.\n",
    "\n",
    "The data is organized in a 3-dimensional tensor `(batch, seq_length, features)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR # scheduler\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "n = 100\n",
    "p = 15\n",
    "T = 20\n",
    "k = 8\n",
    "q = 1\n",
    "\n",
    "DIMENSIONS_Y = (n, T, p) \n",
    "DIMENSIONS_X = (n, T, k)\n",
    "DIMENSIONS_Z = (n, T, q)\n",
    "DIMENSIONS_U = (n, 1, p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Computes the conditional mean of the gllvm.\"\"\"\n",
    "    def __init__(self, num_latents, num_features, num_covariates):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        self.wz = nn.Parameter(torch.randn((num_latents, num_features)))\n",
    "        self.wx = nn.Parameter(torch.randn((num_covariates, num_features)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, num_features)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u, features_id, features_linkinv):\n",
    "        \"\"\"Combine the z, the u, and the x into the conditional means. The z and the u are already sampled, no need to transform them further.\"\"\"\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see section \"details of tensorproducts\"\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "\n",
    "        # Compute the linear predictor\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "\n",
    "        # Create a placeholder for the conditional means\n",
    "        condmean  = torch.zeros_like(linpar)\n",
    "        # The conditional mean is obtained by applying the inverse link to the linear prepdictor.\n",
    "        # For computational efficiency reasons, we apply the inverse link on all response of the same types in one go.\n",
    "        # It is safe here to modify condmean in-place because the replaced elements have not been used in the computational graph before.\n",
    "        for feature_type, id in features_id.items():\n",
    "            condmean[:,:,id] = features_linkinv[feature_type](linpar[:,:,id])\n",
    "\n",
    "        return condmean\n",
    "    \n",
    "\n",
    "class AR1(nn.Module):\n",
    "    \"\"\"Implementation of an AR1 model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the phi parameter (the only parameter of the AR1 process)\n",
    "        self.phi = nn.Parameter(torch.ones(1)*.5)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        # 'd' is the input tensor containing the noise terms of dimensions (num_observations, seq_length, num_features)\n",
    "\n",
    "        # Initialize output list with the first noise term\n",
    "        z_list = [d[:,0,:]]\n",
    "\n",
    "        # Generate the AR(1) sequence by iteratively adding the effect of\n",
    "        # previous term (scaled by phi) and the current noise term\n",
    "        for i in range(1, d.shape[1]):\n",
    "            z_list.append(z_list[i-1] * self.phi + d[:,i,:])\n",
    "\n",
    "        # Concatenate the list to create our new tensor along the time dimensions 1\n",
    "        z = torch.stack(z_list, dim=1)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def compute_d(self, z):\n",
    "        \"\"\"\n",
    "        Given z, return d. This is the inverse function of forward.\n",
    "        \"\"\"\n",
    "        d_list = [z[:,0,:]]\n",
    "\n",
    "        for i in range(1, z.shape[1]):\n",
    "            state = z[:,i-1,] * self.phi\n",
    "            di = z[:,i,:] - state\n",
    "            d_list.append(di)\n",
    "        \n",
    "        d = torch.stack(d_list, dim=1)\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "# The encoder tries to learn the latent variables given the observations. Since the encoder learns on simulated data, it knows the true value of the latent. Given the latent, there is no time dependence \n",
    "# in this model so no need to consider recursive neural network; a simple feed-forwward neural network can do the trick. What's more, no need to consider the time dimension, although we will here. the network has the same parameters anyway.\n",
    "\n",
    "# The encoder needs to encode the random effects (u) and the latent variables (z)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the features into the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_covariates, hidden_size, num_latents):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features + num_covariates, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder_z = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_latents)\n",
    "        )\n",
    "\n",
    "        self.encoder_u = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_features)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward on the whole dataset;  Process the whole sequence at once, leveraging shared parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # concatnenate x and y in the features dimension\n",
    "        xy = torch.cat([x, y], dim=2)\n",
    "        z_hat = []\n",
    "        u_hat = []\n",
    "\n",
    "        for i in range(xy.shape[1]):\n",
    "            # select the ith dimension slice\n",
    "            slice_i = xy[:, i, :]\n",
    "            # apply the forward pass to the slice\n",
    "            intermediate_output = self.encoder(slice_i)\n",
    "            output_z = self.encoder_z(intermediate_output)\n",
    "            output_u = self.encoder_u(intermediate_output)\n",
    "            z_hat.append(output_z)\n",
    "            u_hat.append(output_u)\n",
    "\n",
    "        # concatenate the output and create a new time dimension\n",
    "        z_hat = torch.stack(z_hat, dim=1)\n",
    "        u_hat = torch.stack(u_hat, dim=1)\n",
    "        u_hat = torch.mean(u_hat, dim=1, keepdim=True)\n",
    "\n",
    "        return z_hat, u_hat\n",
    "\n",
    "class GLLVM(nn.Module):\n",
    "    \"\"\"\n",
    "    GLLVM model\n",
    "    \n",
    "    Args:\n",
    "        var_types: a dictionary of key:value pairs with keys one of \"gaussian\", \"poisson\", \"ordinal\", \"binary, and values being a list of indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform):\n",
    "        super().__init__()\n",
    "        # Initialize feature types\n",
    "        self.features_id = features_id\n",
    "        self.features_linkinv_sample = features_linkinv_sample\n",
    "        self.features_linkinv_forward = features_linkinv_forward\n",
    "        self.features_transform = features_transform\n",
    "\n",
    "        # Initialize the model dimensions. Note that the number of observations and the sequence length are not model parameters but characteristics of the data.\n",
    "        self.num_features = num_features\n",
    "        self.num_latents = num_latents\n",
    "        self.num_covariates = num_covariates\n",
    "\n",
    "        # Initialize the modules \n",
    "        self.ar1 = AR1()\n",
    "        self.decoder = Decoder(self.num_latents, self.num_features, self.num_covariates)\n",
    "        self.encoder = Encoder(self.num_features, self.num_covariates, 50, self.num_latents)\n",
    "\n",
    "        # Initialize Parameters\n",
    "        self.bias = nn.Parameter(torch.zeros((1,1,self.num_features))) # one bias term per response, shared accross time and observations\n",
    "        self.var_u = nn.Parameter(torch.ones((1,1,self.num_features)))\n",
    "        self.var_y = nn.Parameter(torch.ones((1,1,self.num_covariates))) # this appears in the loss function, but we know all of them to be 1 by design   \n",
    "    \n",
    "    def autoencode(self, x, y):\n",
    "        z, u = self.encoder(x, y)\n",
    "        d = self.ar1.compute_d(z)\n",
    "        u = u/torch.sqrt(self.var_u)\n",
    "        reconstructed = self(x, d, u)\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "    def impute(self, x, y, mask=None):\n",
    "        \"\"\"Impute the data. If mask is set to none, initialize imputation\"\"\"\n",
    "        self.eval()\n",
    "        # If mask is none, initialize imputation by the mean of the features (in dim 2)\n",
    "        if mask is None:\n",
    "            mask = ( y == np.nan)\n",
    "            y[mask] = torch.nanmean(y, dim=(0,1), keepdim=True).repeat(y.shape[0], y.shape[1], 1)[mask]\n",
    "        else:\n",
    "            y[mask] = self.autoencode(x, y)[mask]\n",
    "        return y, mask\n",
    "\n",
    "\n",
    "    def forward(self, x, d, u):\n",
    "        z = self.ar1(d)\n",
    "        u = u * self.var_u\n",
    "        yhat = self.decoder(x, z, u, self.features_id, self.features_linkinv_forward)\n",
    "        return yhat\n",
    "\n",
    "    def sample(self, n, seq_length, x=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y).\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0. Here k is set to.\")\n",
    "                x = torch.randn((n, seq_length, self.num_covariates))\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, seq_length, self.num_latents))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, self.num_features)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.ar1(d)\n",
    "\n",
    "            condmean = self.decoder(x, z, u, self.features_id, self.features_linkinv_sample)\n",
    "\n",
    "            y = torch.zeros_like(condmean)\n",
    "\n",
    "            # Sample and transform where necessary\n",
    "            for feature_type, id in self.features_id.items():\n",
    "                if feature_type == \"binary\":\n",
    "                    y_sample = torch.bernoulli(condmean[:,:,id])\n",
    "                elif feature_type == \"ordinal\":\n",
    "                    # TODO: re-write this part and comment better\n",
    "                    y_ordinal = condmean[:,:,id]\n",
    "                    # draw one uniform for the whole vector\n",
    "                    random = torch.rand((*y_ordinal.shape[0:2], 1)) \n",
    "                    # compare with the cumulative probabilities\n",
    "                    ordinal = torch.sum(random < y_ordinal, axis=2)\n",
    "                    ordinal = torch.nn.functional.one_hot(ordinal).squeeze().float()\n",
    "                    ordinal = ordinal[:,:,1:] # discard the first column of the one_hot encoding, as it is superfluous (as a 0)\n",
    "                    y_sample = ordinal\n",
    "                elif feature_type == \"counts\":\n",
    "                    y_sample = torch.poisson(condmean[:,:,id]) # here we transform the mean using the inverse link - this is due to the transformation of the poisson variable intot a \"gaussin\". For sampling, we actually want the gaussian.\n",
    "\n",
    "                # Transform if any transformation was specified\n",
    "                if feature_type in self.features_transform:\n",
    "                    y_sample = self.features_transform[feature_type](y_sample)\n",
    "                \n",
    "                y[:,:,id] = y_sample\n",
    "\n",
    "        return {\"x\":x, \"y\":y, \"z\":z, \"u\":u, \"d\":d}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(x, y, z, u, model, optimizer, criterion, num_epochs=100, validation_data=None, verbose=True, random=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and return the model and the history of losses.\n",
    "\n",
    "    :param x: The input data for model\n",
    "    :param y: The labels for input data\n",
    "    :param z: additional input data\n",
    "    :param u: additional input data\n",
    "    :param model: PyTorch model to train\n",
    "    :param optimizer: Optimizer to use in training\n",
    "    :param criterion: Loss function to use in training\n",
    "    :param num_epochs: Number of training epochs. Default is 100.\n",
    "    :param validation_data: Tuple of validation data (x_val, y_val, z_val, u_val). Default is None.\n",
    "    :return: Tuple of trained model and history of losses.\n",
    "    \"\"\"\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if random:\n",
    "            x, y, z, u, _ = model.sample(y.shape[0], y.shape[1], x = x).values()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_z, outputs_u = model.encoder(x, y)\n",
    "\n",
    "        loss_z = criterion(outputs_z, z)\n",
    "        loss_u = criterion(outputs_u, u)\n",
    "\n",
    "        # Compute loss\n",
    "\n",
    "        loss = loss_z + loss_u\n",
    "\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if validation_data is not None:\n",
    "            model.eval() # switch model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                x_val, y_val, z_val, u_val, _ = validation_data.values()\n",
    "                val_z, val_u = model.encoder(x_val, y_val)\n",
    "                val_loss_z =  criterion(val_z, z_val)\n",
    "                val_loss_u = criterion(val_u, u_val)\n",
    "                val_loss = val_loss_z + val_loss_u\n",
    "                val_losses.append(val_loss.item())\n",
    "            model.train() # switch model back to training mode\n",
    "\n",
    "        if verbose and (epoch % 10) == 0:\n",
    "            print(f'Epoch: {epoch}, Training Loss: {loss.item()}, Val loss = {None if validation_data is None else val_loss}')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and3\n",
    "42\n",
    "big test\n",
    "4\n",
    "1234\n",
    "1432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try generating data\n",
    "features_id = {\n",
    "    'binary': [0,1,2,3,4,5,6,7],\n",
    "    'ordinal': [9, 10],\n",
    "    'counts': [8, 11, 12]\n",
    "}\n",
    "\n",
    "# We define two inverse link functions; one for sampling (the one assumed in the model), and another for computations (the one used to compute gradients)\n",
    "features_linkinv_sample = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: torch.exp(x) \n",
    "}\n",
    "\n",
    "# We define two inverse link functions; one for sampling (the one assumed in the model), and another for computations (the one used to compute gradients)\n",
    "features_linkinv_forward = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: x\n",
    "}\n",
    "\n",
    "features_transform = {\n",
    "    #'binary': lambda x: 2* (x - .5),\n",
    "    #'ordinal': lambda x: (x -5)/5,\n",
    "    'counts': lambda x: torch.log(x + 1)\n",
    "}\n",
    "\n",
    "num_latents = 1\n",
    "num_features = 13\n",
    "num_covariates = 3\n",
    "\n",
    "\n",
    "model_true = GLLVM(num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform)\n",
    "\n",
    "model = GLLVM(num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform)\n",
    "from copy import deepcopy\n",
    "model = deepcopy(model_true)\n",
    "\n",
    "# Draw data from the true model\n",
    "data_true = model_true.sample(200, 9)\n",
    "val_data = model_true.sample(200, 9, x=data_true['x'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the encoder\n",
    "\n",
    "How good is the encoder at encoding? train on one dataset, evaluate on another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.encoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder(**data_true, model=model, optimizer=optimizer, criterion=criterion, validation_data=val_data, num_epochs=1000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this\n",
    "def plot_encoded(data, model):\n",
    "    with torch.no_grad():\n",
    "        zhat, uhat = model.encoder(data['x'], data['y'])\n",
    "        plt.scatter(data['z'].numpy(), zhat.detach().numpy())\n",
    "        plt.scatter(data['u'].numpy(), uhat.detach().numpy(), color=\"red\")\n",
    "        plt.title(\"Encoded vs True values\")\n",
    "    plt.show()\n",
    "\n",
    "plot_encoded(val_data, model)\n",
    "plot_encoded(data_true, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "We train the parameters of the decoder and encoder separately: the encoder is trained on simulated data, and the decoder is trained on the sample, and centered in expectation by the simulated data. The training loop goes as follows:\n",
    "\n",
    "* Initialize encoder and decoder losses and optimizers\n",
    "* Create the mask of missing values, and initialize imputation\n",
    "\n",
    "* sample (marginally) from the model (still given the covariates `x`) (no grad)\n",
    "* update the encoder (grad)\n",
    "* encode both the sample and the decoder (no grad)\n",
    "* update the decoder (grad), taking into account the missing value mask\n",
    "* update the imputations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, output, target, mask):\n",
    "        return torch.mean(torch.pow(output -target, 2) * ~mask)\n",
    "\n",
    "class MaskedBCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, output, target, output_sim, target_sim, mask):\n",
    "        return torch.mean((output * target - output_sim * target_sim) * ~mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_encoder = nn.MSELoss()\n",
    "criterion_decoder = MaskedBCLoss()\n",
    "model_evaluation = MaskedMSELoss()\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=0.01)\n",
    "optimizer_decoder = torch.optim.Adam([\n",
    "    {'params':model.decoder.parameters()},\n",
    "    {'params':model.ar1.parameters()},\n",
    "    {'params':model.var_u}\n",
    "    ], lr=0.01)\n",
    "\n",
    "scheduler_encoder = StepLR(optimizer_encoder, step_size=100, gamma=1)\n",
    "scheduler_decoder = StepLR(optimizer_decoder, step_size=100, gamma=1)\n",
    "\n",
    "mask = torch.isnan(data_true['y'])\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "num_encoder_epochs = 200\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    # Sample some fake data\n",
    "    data_sim = model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])\n",
    "    data_sim_encoder = model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])\n",
    "    # Train the encoder\n",
    "    _, train_loss, validation_loss = train_encoder(**data_sim_encoder, model=model, optimizer=optimizer_encoder, criterion=criterion_encoder, num_epochs=num_encoder_epochs, verbose=False)\n",
    "    # Autoencode\n",
    "    output_true = model.autoencode(data_true['x'], data_true['y'])\n",
    "    output_sim = model.autoencode(data_sim['x'], data_sim['y'])\n",
    "    loss = criterion_decoder(output_true, data_true['y'], output_sim, data_sim['y'], mask)\n",
    "    loss.backward()\n",
    "    optimizer_decoder.step()\n",
    "\n",
    "    scheduler_decoder.step()\n",
    "    scheduler_encoder.step()\n",
    "\n",
    "    # Clipping the gradients\n",
    "    # clip_value = .1  # this can be any value of your choosing\n",
    "    # clip_grad_norm_(model.decoder.parameters(), clip_value)\n",
    "\n",
    "    # store parameters\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval = model_evaluation(output_true, data_true['y'], mask).item()\n",
    "        total_loss = train_loss[-1] + eval\n",
    "        if eval < best_loss:\n",
    "            best_loss = eval\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    # Every 10 we restore the best weights\n",
    "    if epoch % 1 == 0:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f'Epoch: {epoch}, total_loss: {total_loss:.2f}, decoder_loss: {eval:.2f}, encoder_loss: {train_loss[-1]:.2f}, best_loss = {best_loss:.2f}, phi:{model.ar1.phi}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the GLLVM module on a simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_true['y'].detach().numpy(), model.autoencode(data_true['x'], data_true['y']).detach().numpy())\n",
    "plt.plot([0,5], [0,5], color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the encoder fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict z:\n",
    "z_hat, u_hat = model.encoder(data_true['x'], data_true['y'])\n",
    "plt.scatter(z_hat.detach().numpy(), data_true['z'].detach().numpy())\n",
    "plt.scatter(u_hat.detach().numpy(), data_true['u'].detach().numpy(), color=\"red\")\n",
    "plt.plot([-3, 3], [-3, 3], color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the model parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param = model.decoder.parameters()\n",
    "param_true = model_true.decoder.parameters()\n",
    "\n",
    "for i, (p_true, p) in enumerate(zip(param_true, param)):\n",
    "    plt.scatter(p_true.detach().numpy(), p.detach().numpy(), color=f'C{i}', label=f'Group {i}')\n",
    "\n",
    "plt.xlabel('True Parameters')\n",
    "plt.ylabel('Estimated Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 12 randomly selected Z\n",
    "index = np.random.choice(range(n), 12, replace=False)\n",
    "fig, axs = plt.subplots(3, 4)\n",
    "\n",
    "zhat = model.encoder(data_true[\"x\"], data_true['y'])[0].detach()\n",
    "\n",
    "fig.suptitle(\"Ztrue vs Zest across time\")\n",
    "for i in range(12):\n",
    "    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n",
    "    axs[i//4, i%4].plot(data_true[\"z\"][index[i],:, 0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(z, id=0, grid=(1,)):\n",
    "    fig, axs = plt.subplots(*grid)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.plot(z[i,:,id])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_obs_v_mean(obs, mean, id=0, grid=(1,)):\n",
    "    fig, axs = plt.subplots(*grid)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.plot(obs[i,:,id])\n",
    "        ax.plot(mean[i,:,id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_id)\n",
    "plot_series(data_true['z'], grid=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: refactor to have a module of two modules: encoder and decoder\n",
    "class GLLVM_longitudinal():\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # nuisance parameters\n",
    "        self.phi = torch.ones(1) * .5\n",
    "        self.var_u = torch.ones((1,1,p))\n",
    "        self.var_y = torch.ones((1,1,p))* .5\n",
    "        self.var_z = torch.ones((1,T,1))\n",
    "\n",
    "    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n",
    "        encoder_loss = nn.MSELoss()\n",
    "        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            (zhat, uhat) = self.encoder(x, y)\n",
    "            loss = encoder_loss(zhat, z) + encoder_loss(uhat, u)\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            encoder_opt.zero_grad()\n",
    "        return loss\n",
    "    \n",
    "    def update_nuisance_parameters(self, y, yhat, z, u, lr=1e-1):\n",
    "        # update phi:\n",
    "        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n",
    "        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n",
    "        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n",
    "        phi_sample = (torch.sum(z[:,1:]*z[:,:(T-1)], dim=1) / torch.sum(torch.pow(z[:,:(T-1)],2), dim=1)).mean()\n",
    "        var_u = torch.mean(torch.pow(u, 2), dim=0, keepdim=True)\n",
    "        var_y = torch.mean(torch.pow(y-yhat,2), dim=[1,2], keepdim=True)\n",
    "\n",
    "        self.phi = self.phi * (1-lr) + lr * phi_sample\n",
    "        self.var_u = self.var_u * (1-lr) + lr * var_u\n",
    "        self.var_y = self.var_y * (1-lr) + lr * var_y\n",
    "\n",
    "\n",
    "    def sample(self, n, x=None, z=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n",
    "                x = torch.randn((n, T, k))\n",
    "            if z is None:\n",
    "                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, T, q))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.AR(z, d)\n",
    "\n",
    "            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n",
    "            y = self.decoder(x, z, u) + eps\n",
    "\n",
    "        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n",
    "    \n",
    "    def AR(self, z, delta):\n",
    "        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n",
    "        for t in range(1, z.shape[1]):\n",
    "            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n",
    "        return z\n",
    "\n",
    "    def sample_z(self):\n",
    "        return None\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        # here i also need phi\n",
    "        self.wz = nn.Parameter(torch.randn((q, p)))\n",
    "        self.wx = nn.Parameter(torch.randn((T, k, p)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, T, p)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u):\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "        return linpar\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n",
    "        # output dimension is T*q  + p (for Z and U, respectively)\n",
    "        self.enc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=T*(p+k), out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = T*q + p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n",
    "        zu = self.enc_model(xy)\n",
    "        return self.split_zu(zu)\n",
    "\n",
    "    def split_zu(self, zu):\n",
    "        #output dimension of size (T*Z), p\n",
    "        z, u = torch.split(zu, [T*q, p], dim=1)\n",
    "        z = z.reshape((z.shape[0], T, q))\n",
    "        u = u.unsqueeze(1)\n",
    "        return (z, u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "n = 100\n",
    "p = 15\n",
    "T = 20\n",
    "k = 8\n",
    "q = 1\n",
    "\n",
    "DIMENSIONS_Y = (n, T, p) \n",
    "DIMENSIONS_X = (n, T, k)\n",
    "DIMENSIONS_Z = (n, T, q)\n",
    "DIMENSIONS_U = (n, 1, p)\n",
    "\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, size):\n",
    "        \"\"\"Return a tensor of shape (size) of indpeendent standard normal random variables.\"\"\"\n",
    "        return(torch.randn(size))\n",
    "\n",
    "class AR1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the phi parameter\n",
    "        self.phi = nn.Parameter(torch.ones(1) * .5)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        # 'd' is the input tensor containing the noise terms \n",
    "\n",
    "        # Initialize output list with the first noise term\n",
    "        z_list = [d[0]]\n",
    "\n",
    "        # Generate AR(1) sequence by iteratively adding the effect of\n",
    "        # previous term (scaled by phi) and the current noise term\n",
    "        for i in range(1, d.shape[0]):\n",
    "            z_list.append(z_list[i-1] * self.phi + d[i])\n",
    "\n",
    "        # Convert the list to a tensor\n",
    "        z = torch.stack(z_list)\n",
    "        \n",
    "        return z\n",
    "\n",
    "class GLLVM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the model parameters: all th intercepts, all the eveeything.\n",
    "\n",
    "\n",
    "# TODO: refactor to have a module of two modules: encoder and decoder\n",
    "class GLLVM_longitudinal():\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # nuisance parameters\n",
    "        self.phi = torch.ones(1) * .5\n",
    "        self.var_u = torch.ones((1,1,p))\n",
    "        self.var_y = torch.ones((1,1,p))* .5\n",
    "        self.var_z = torch.ones((1,T,1))\n",
    "\n",
    "    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n",
    "        encoder_loss = nn.MSELoss()\n",
    "        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            (zhat, uhat) = self.encoder(x, y)\n",
    "            loss = encoder_loss(zhat, z) + encoder_loss(uhat, u)\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            encoder_opt.zero_grad()\n",
    "        return loss\n",
    "    \n",
    "    def update_nuisance_parameters(self, y, yhat, z, u, lr=1e-1):\n",
    "        # update phi:\n",
    "        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n",
    "        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n",
    "        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n",
    "        phi_sample = (torch.sum(z[:,1:]*z[:,:(T-1)], dim=1) / torch.sum(torch.pow(z[:,:(T-1)],2), dim=1)).mean()\n",
    "        var_u = torch.mean(torch.pow(u, 2), dim=0, keepdim=True)\n",
    "        var_y = torch.mean(torch.pow(y-yhat,2), dim=[1,2], keepdim=True)\n",
    "\n",
    "        self.phi = self.phi * (1-lr) + lr * phi_sample\n",
    "        self.var_u = self.var_u * (1-lr) + lr * var_u\n",
    "        self.var_y = self.var_y * (1-lr) + lr * var_y\n",
    "\n",
    "\n",
    "    def sample(self, n, x=None, z=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n",
    "                x = torch.randn((n, T, k))\n",
    "            if z is None:\n",
    "                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, T, q))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.AR(z, d)\n",
    "\n",
    "            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n",
    "            y = self.decoder(x, z, u) + eps\n",
    "\n",
    "        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n",
    "    \n",
    "    def AR(self, z, delta):\n",
    "        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n",
    "        for t in range(1, z.shape[1]):\n",
    "            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n",
    "        return z\n",
    "\n",
    "    def sample_z(self):\n",
    "        return None\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        # here i also need phi\n",
    "        self.wz = nn.Parameter(torch.randn((q, p)))\n",
    "        self.wx = nn.Parameter(torch.randn((T, k, p)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, T, p)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u):\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "        return linpar\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n",
    "        # output dimension is T*q  + p (for Z and U, respectively)\n",
    "        self.enc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=T*(p+k), out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = T*q + p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n",
    "        zu = self.enc_model(xy)\n",
    "        return self.split_zu(zu)\n",
    "\n",
    "    def split_zu(self, zu):\n",
    "        #output dimension of size (T*Z), p\n",
    "        z, u = torch.split(zu, [T*q, p], dim=1)\n",
    "        z = z.reshape((z.shape[0], T, q))\n",
    "        u = u.unsqueeze(1)\n",
    "        return (z, u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_true = GLLVM_longitudinal()\n",
    "dat_true = gl_true.sample(n)\n",
    "\n",
    "gl = GLLVM_longitudinal()\n",
    "\n",
    "gl = copy.deepcopy(gl_true) #making it start with the same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(input, target, sign=1):\n",
    "    return sign * torch.sum(input*target, dim=[1,2]).mean()\n",
    "\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target, input_sim, target_sim):\n",
    "        return torch.mean(input * target - input_sim * target_sim)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_fit(input, target):\n",
    "    with torch.no_grad():\n",
    "        return torch.sum(torch.pow(input - target,2), dim=[1,2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n",
    "\n",
    "# train the encoder\n",
    "\n",
    "gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tup[0] for tup in gl.decoder.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_opt = torch.optim.Adam(gl.decoder.parameters())\n",
    "criterion = MyLoss()\n",
    "optimizer = torch.optim.Adam(gl.decoder.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=1)\n",
    "\n",
    "# Create a copy of the model's parameters at the end of the last epoch\n",
    "reference_params = [param.clone().detach() for param in gl.decoder.parameters()]\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(1, epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n",
    "\n",
    "    # train the encoder\n",
    "\n",
    "    encoder_loss = gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=10)\n",
    "    \n",
    "    # compute SPRIME sample step\n",
    "\n",
    "    # compute imputing values\n",
    "    with torch.no_grad():\n",
    "        zhat_true, uhat_true = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n",
    "        zhat_sim, uhat_sim = gl.encoder(dat_sim[\"x\"], dat_sim[\"y\"])\n",
    "    \n",
    "\n",
    "    linpar_sample = gl.decoder(dat_true[\"x\"], zhat_true, uhat_true)\n",
    "    linpar_sim = gl.decoder(dat_sim[\"x\"], zhat_sim, uhat_sim)\n",
    "    loss = criterion(linpar_sample, dat_true[\"y\"], linpar_sim, dat_sim[\"y\"])\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(gl.decoder.parameters(), max_norm=.1)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Update nuisance parameters\n",
    "    with torch.no_grad():\n",
    "        gl.update_nuisance_parameters(dat_true[\"y\"], linpar_sample, zhat_true, uhat_true, lr=0.0)\n",
    "\n",
    "    # evaluate the model\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss = evaluate_fit(linpar_sample, dat_true[\"y\"])\n",
    "\n",
    "        # Get the learning rates\n",
    "        learning_rates = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "\n",
    "        # Calculate the mean learning rate\n",
    "        mean_learning_rate = sum(learning_rates) / len(learning_rates)\n",
    "\n",
    "        # Compute MSE between reference parameters and current parameters\n",
    "        current_params = gl.decoder.parameters()\n",
    "        mse = sum(torch.sum((param - ref_param)**2) for param, ref_param in zip(current_params, reference_params))\n",
    "        mse /= len(reference_params)  # Divide by the number of parameters\n",
    "    \n",
    "        print(f\"MSE: {mse.item()}\")\n",
    "\n",
    "        # Update reference parameters for the next epoch\n",
    "        reference_params = [param.clone().detach() for param in gl.decoder.parameters()]\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}, loss = {loss:.2f}, encoder_loss = {encoder_loss:.2f}, lr={mean_learning_rate}, delta_par = {mse}, phi= {gl.phi}, var_u ={gl.var_u[0,0,0]}, var_y={gl.var_y[0,0,0]}\")\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        #dat_sim = gl.sample(n)\n",
    "    #linpar = gl.decoder(dat_sim[\"x\"], dat_sim[\"z\"], dat_sim[\"u\"])\n",
    "\n",
    "    # decoder_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    zhat, uhat = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n",
    "    yhat = gl.decoder(dat_true[\"x\"], zhat, uhat)\n",
    "\n",
    "plt.scatter(dat_true[\"y\"], yhat)\n",
    "plt.plot([-20, 20], [-20, 20], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhat = zhat*-1\n",
    "plt.scatter(zhat, dat_true[\"z\"])\n",
    "plt.plot([-10, 10], [-10,10], color=\"red\")\n",
    "plt.plot([-10, 10], [10,-10], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 12 randomly selected Z\n",
    "index = np.random.choice(range(n), 12, replace=False)\n",
    "fig, axs = plt.subplots(3, 4)\n",
    "\n",
    "zhat= -zhat\n",
    "fig.suptitle(\"Ztrue vs Zest across time\")\n",
    "for i in range(12):\n",
    "    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n",
    "    axs[i//4, i%4].plot(dat_true[\"z\"][index[i],:, 0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_true = gl_true.decoder.parameters().__next__().detach().squeeze()\n",
    "par_est = gl.decoder.parameters().__next__() .detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(par_true, par_est)\n",
    "plt.plot([-2,2], [2, -2])\n",
    "plt.plot([-2,2], [-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on tensor products calculations\n",
    "We now show the details on the tensor products, for instance for computing `xb @ wx`. `xb` is of size `(n, T, q)` and `wx` is of size `(T, q, p)`. We want a result of size `(n, T, p)`. First we add a dimension for `xb`:\n",
    "\n",
    "`xb.unsqueeze(2)` which yields a dimensions of `(n, T, 1, q)`\n",
    "\n",
    "which we then multiply by `wz`:\n",
    "\n",
    "`(n, T, 1, q) @ (1, q, p)` -> `(n, T, 1, p)`\n",
    "\n",
    "where the first dimension of `wx` has been broadcasted.\n",
    "\n",
    "Finally, we squeeze to obtain `(n, T, p)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02379b91627ead677140c441995c323138285dea806245dca7a173ada35ac023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

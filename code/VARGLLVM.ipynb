{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR GLLVM\n",
    "\n",
    " ## Model Specification\n",
    "\n",
    "Let  $y_{i1t},y_{i2t},\\ldots,y_{ipt}$  be a set of $p$ response or observed variables at time $t,\\ t=1,\\ldots,T$ for  individual $i,\\ i=1,\\ldots,n$. Let $\\mathbf{x}_{it}$ be a set of observed $k$-dimensional covariates at time $t,\\ t=1,\\ldots,T$.\n",
    "\n",
    "Models for multivariate longitudinal data have to account for the three sources of variability\n",
    "present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged\n",
    "associations between different responses at different occasions, and  (iii) the association between repeated measures of the same response\n",
    "over time. The first source of variability is accounted for\n",
    "a time-dependent latent variable $z_{i1}, z_{i2},\\ldots,z_{iT}$. Modeling the temporal evolution of the latent variable acpoisson for the cross-lagged associations between different responses over time.\n",
    "The third source of variability can be accounted for a set of item-specific random effects $\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'$.\n",
    "\n",
    "According to the GLLVM framework we have\n",
    "\n",
    "\\begin{align*}\n",
    "   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n",
    "   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n",
    "\\end{align*}\n",
    "where $g_j(\\cdot),j=1,\\ldots,p$ is a known {\\it link function}, $\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T$ is the {\\it linear predictor},  and $\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)$ denotes a distribution from the exponential family with mean $\\mu_{ijt}$ and response variable-specific dispersion parameter $\\tau_j$. \\vspace{5pt}\\\\\n",
    "The dynamics of the latent variable over time is modelled through a stationary vector-autoregressive model of first order\n",
    "\n",
    "\n",
    "$$\n",
    "z_{i,t} = Az_{i,t-1} +  \\epsilon_{i,t}\\\\\n",
    "||A||_2 < 1\\\\\n",
    "\\epsilon_{i,t} \\sim N(0, I)\\\\\n",
    "$$\n",
    "\n",
    "initialization for $t=0$:\n",
    "\n",
    "$$\n",
    "z_{i, 1} \\sim N(0, \\Sigma_{z1})\\\\\n",
    "\\Sigma_{z1} = \n",
    "\\begin{pmatrix}\n",
    "    \\sigma_{z1,1}^2&\\\\\n",
    "    &\\sigma_{z1,2}^2\n",
    "\\end{pmatrix}\\\\\n",
    "\\sigma_{z1,i}>0 \\forall i\n",
    "$$\n",
    "\n",
    "Moreover, we assume the random effects independent of the latent variable and their common distribution $\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* check if there is a need to add to define seq_length... i think not: no parameter depends on seq_length. for now we leave it because the encoder may need that, even though I dont think so.\n",
    "* test that the operations in forward are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from VAR1 import VAR1\n",
    "\n",
    "class VARGLLVM(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Autoregressive Generalized Linear Latent Variable Model (VAR GLLVM) for multivariate longitudinal data.\n",
    "    \n",
    "    This class provides a multivariate extension of GLLVM, modeling multivariate responses over time.\n",
    "    The variability in the data is captured through three sources: cross-sectional associations,\n",
    "    cross-lagged associations, and associations between repeated measures of the same response over time.\n",
    "    The model specification integrates a time-dependent latent variable and an item-specific random effect.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_var: int, Number of response variables.\n",
    "    - num_latent: int, Number of latent variables.\n",
    "    - num_covar: int, Number of observed covariates.\n",
    "    - response_types: dict, Mapping of response type to its indices.\n",
    "    - add_intercepts: bool, Whether to include intercepts in the model.\n",
    "    \n",
    "    Key Methods:\n",
    "    - forward: Computes the conditional mean of the model.\n",
    "    - sample: Draws samples from the VARGLLVM model.\n",
    "    - sample_response: Samples from the response distribution based on conditional mean.\n",
    "    - linpar2condmean: Converts linear predictors to conditional means.\n",
    "\n",
    "    Model Specification:\n",
    "    y_{ijt}|μ_{ijt} ~ F_j(y_{ijt}|μ_{ijt}, τ_j)\n",
    "    μ_{ijt} = g_j(β_{0jt} + x_{it}^T*β_{jt} + z_{it}^T*λ_{jt}+u_{ij}*σ_{u_j})\n",
    "    \n",
    "    Where:\n",
    "    - g_j: link function\n",
    "    - μ_{ijt}: mean of the distribution\n",
    "    - F_j: a distribution from the exponential family\n",
    "    - η_{ijt}: linear predictor\n",
    "\n",
    "    Temporal evolution of the latent variable:\n",
    "    z_{it} = A*z_{i,t-1} + ε_{it}\n",
    "    with ε_{it} ~ N(0, I) and initialization z_{i, 1} ~ N(0, Σ_{z1}).\n",
    "    \n",
    "    The random effects are assumed independent of the latent variable and distributed as:\n",
    "    u_{i} ~ N_p(0, I).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_var, num_latent, num_covar, response_types, add_intercepts = True):\n",
    "        super().__init__()\n",
    "        self.response_types =  response_types\n",
    "        self.response_link = {\n",
    "            'bernoulli' : lambda x: torch.logit(x),\n",
    "            'ordinal': lambda x: torch.logit(x),\n",
    "            'poisson': lambda x: torch.log(x)\n",
    "        }\n",
    "        self.response_linkinv = {\n",
    "            'bernoulli': lambda x: 1/(1+torch.exp(-x)),\n",
    "            'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "            'poisson': lambda x: torch.exp(x)\n",
    "        }\n",
    "        self.response_transform = {\n",
    "            'bernoulli' : lambda x: 2*x - 1,\n",
    "            'ordinal': lambda x: 2*x - 1,\n",
    "            'poisson': lambda x: torch.log(x+1)\n",
    "        }\n",
    "\n",
    "        self.num_var = num_var\n",
    "        self.num_latent = num_latent\n",
    "        self.num_covar = num_covar\n",
    "\n",
    "        # Define Parameters\n",
    "        # -----------------\n",
    "        # Parameters for the VAR\n",
    "        self.A = nn.Parameter(torch.zeros((num_latent, num_latent)))\n",
    "        self.logvar_z1 = nn.Parameter(torch.zeros((num_latent,)))\n",
    "\n",
    "        # Parameters for the outcome model\n",
    "        if add_intercepts:\n",
    "            self.intercepts = nn.Parameter(torch.zeros((num_var,)))\n",
    "        else:\n",
    "            self.intercepts = None\n",
    "\n",
    "        self.wz = nn.Parameter(torch.randn((num_latent, num_var)))\n",
    "        self.wx = nn.Parameter(torch.randn((num_covar, num_var)))\n",
    "\n",
    "        # Parameters for the random effects\n",
    "        self.logvar_u = nn.Parameter(torch.zeros((num_var,)))\n",
    "        \n",
    "        # Define Modules\n",
    "        # --------------\n",
    "        self.VAR1 = VAR1(A=self.A, logvar_z1 = self.logvar_z1)\n",
    "\n",
    "    def forward(self, epsilon, u, x = None):\n",
    "        \"\"\"\n",
    "        Compute the conditional mean of a VARGLLVM\n",
    "\n",
    "        Parameters:\n",
    "            - epsilon: shocks for the VAR\n",
    "            - u: shocks for the random effects\n",
    "            - x: covariates\n",
    "        \"\"\"\n",
    "        assert epsilon.shape[2] == self.num_latent, \"bad shape for epsilon\"\n",
    "        assert u.shape[1:] == (1, self.num_var), \"bad shape for u\"\n",
    "\n",
    "        # Computing linpar\n",
    "        # ----------------\n",
    "        linpar = torch.zeros((epsilon.shape[0], epsilon.shape[1], self.num_var))\n",
    "\n",
    "        # add intercepts, one per variable\n",
    "        if self.intercepts is not None:\n",
    "            linpar += self.intercepts[None, None, :]\n",
    "\n",
    "        # add covariates' effects\n",
    "        if x is None:\n",
    "            assert self.num_covar == 0, f'VARGLLVM module expected {self.num_covar} covariates, received {0}.'\n",
    "        else:\n",
    "            assert(x.shape[1:] == (epsilon.shape[1], self.num_covar))\n",
    "            linpar += x @ self.wx\n",
    "\n",
    "        # add latent variables' effects\n",
    "        z = self.VAR1(epsilon)\n",
    "        linpar += z @ self.wz\n",
    "\n",
    "        # finally, add random effects\n",
    "        linpar += u * torch.sqrt(torch.exp(self.logvar_u[None, None, :])) # we add a time dimension: u is the same across time!\n",
    "        \n",
    "        # compute the conditional mean\n",
    "        condmean = self.linpar2condmean(linpar)\n",
    "        return (linpar, condmean)\n",
    "    \n",
    "    def sample(self, num_batch, seq_length,  x = None, epsilon = None, u = None):\n",
    "        \"\"\"\n",
    "        Sample from the VARGLLVM. \n",
    "\n",
    "        Parameters:\n",
    "            - num_batch: number of observational units\n",
    "            - seq_length: length of the sequence. if any of x, epsilon, or u are provided, their seq_length must coincide\n",
    "            - x: tensor of shape (num_batch, seq_length, num_covar). If the VARGLLVM model was initialized with num_covar >= 1, cannot be None.\n",
    "            - epsilon: the shocks for the latent variables of shape (num_batch, seq_length, num_latent). If None, those are drawn iid from N(0, 1).\n",
    "            - u: the shocks for the random effects of shape (num_batch, 1, num_var). If None, those are drawn iid from N(0, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = torch.randn((num_batch, seq_length, self.num_latent))\n",
    "        if u is None:\n",
    "            u = torch.randn((num_batch, 1, self.num_var)) # one per var, but constant across time\n",
    "        \n",
    "        linpar, condmean = self(epsilon, u, x)\n",
    "        y = self.sample_response(condmean)\n",
    "\n",
    "        return (linpar, condmean, y)\n",
    "\n",
    "    def sample_response(self, mean):\n",
    "        device = next(self.parameters()).device\n",
    "        y = torch.zeros_like(mean).to(device)\n",
    "        for response_type, response_id in self.response_types.items():\n",
    "            if response_type == \"bernoulli\":\n",
    "                y[:,:,response_id] = torch.bernoulli(mean[:,:,response_id]).to(device)\n",
    "            elif response_type == \"ordinal\":\n",
    "                cum_probs = mean[:,:,response_id]\n",
    "                # draw one uniform for the whole vector\n",
    "                random = torch.rand((*cum_probs.shape[0:2], 1)).to(device)\n",
    "                # compare with the cumulative probabilities\n",
    "                ordinal = torch.sum(random > cum_probs, dim=2)\n",
    "                ordinal = torch.nn.functional.one_hot(ordinal).squeeze().float()\n",
    "                ordinal = ordinal[:,:,1:] # discard the first column of the one_hot encoding, as it is superfluous (as a 0)\n",
    "                y[:,:,response_id] = ordinal\n",
    "            elif response_type == \"poisson\":\n",
    "                y[:,:,response_id] = torch.poisson(mean[:,:,response_id])\n",
    "        return y\n",
    "    \n",
    "    def linpar2condmean(self, linpar):\n",
    "        mean  = torch.zeros_like(linpar)\n",
    "        for response_type, response_id in self.response_types.items():\n",
    "            mean[:,:,response_id] = self.response_linkinv[response_type](linpar[:,:,response_id])\n",
    "        return mean\n",
    "    def _get_device(self):\n",
    "        return next(self.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "num_var = 13\n",
    "num_latent = 2\n",
    "num_covar = 0\n",
    "seq_length = 10\n",
    "x = torch.randn((n, seq_length, num_covar))\n",
    "\n",
    "response_types = {\n",
    "    'bernoulli': [0,1,2,3,4,5,6,7, 9, 10],\n",
    "    'poisson': [8, 11, 12]\n",
    "}\n",
    "\n",
    "model = VARGLLVM(num_var = num_var,\n",
    "                 num_latent = num_latent,\n",
    "                 num_covar = num_covar,\n",
    "                 response_types = response_types,\n",
    "                 add_intercepts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

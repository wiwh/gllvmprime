{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLLVM for longitudinal data: a pytorch implementation\n",
    "\n",
    "We consider the following model\n",
    "\n",
    "TODO:\n",
    "* learn the nuisance parameters! To learn phi, make the encoder learn the zhats **before** AR(1) is applied, and take the gradient of the decoder. that is, make the decoder have phi as a parameter, and same for var_mu!\n",
    "* allow for gaussian and binary data\n",
    "* Compute the gradient that define the model\n",
    "* allow for missing values and impute them\n",
    "* define mine own loss.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## Model Specification\n",
    "\n",
    "Let  $y_{i1t},y_{i2t},\\ldots,y_{ipt}$  be a set of $p$ response or observed variables at time $t,\\ t=1,\\ldots,T$ for  individual $i,\\ i=1,\\ldots,n$. Let $\\mathbf{x}_{it}$ be a set of observed $k$-dimensional covariates at time $t,\\ t=1,\\ldots,T$.\n",
    "\n",
    "Models for multivariate longitudinal data have to account for the three sources of variability\n",
    "present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged\n",
    "associations between different responses at different occasions, and  (iii) the association between repeated measures of the same response\n",
    "over time. The first source of variability is accounted for\n",
    "a time-dependent latent variable $z_{i1}, z_{i2},\\ldots,z_{iT}$. Modeling the temporal evolution of the latent variable accounts for the cross-lagged associations between different responses over time.\n",
    "The third source of variability can be accounted for a set of item-specific random effects $\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'$.\n",
    "\n",
    "According to the GLLVM framework we have\n",
    "\n",
    "\\begin{align*}\n",
    "   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n",
    "   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n",
    "\\end{align*}\n",
    "where $g_j(\\cdot),j=1,\\ldots,p$ is a known {\\it link function}, $\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T$ is the {\\it linear predictor},  and $\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)$ denotes a distribution from the exponential family with mean $\\mu_{ijt}$ and response variable-specific dispersion parameter $\\tau_j$. \\vspace{5pt}\\\\\n",
    "The dynamics of the latent variable over time is modelled through a non-stationary autoregressive model of first order\n",
    "\n",
    "\\begin{equation*}\n",
    "z_{it}=\\phi z_{i,t-1} + \\delta_{it}\n",
    "\\end{equation*}%\n",
    "where  $z_{i1}\\sim N(0,\\sigma^2_{1})$ and $\\delta_{it}\\sim N(0,1)$.  Moreover, we assume the random effects independent of the latent variable and their common distribution $\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)$.\n",
    "\n",
    "\n",
    "\n",
    "## Measurement invariance\n",
    "\n",
    "The  latent variable $z_{it}$ has to be the same (same meaning) across occasions.\n",
    "Thus the measurement invariance assumption has to be tested on the data, that is \n",
    "all the measurement parameters  are invariant across occasions, that is $$\\beta_{0jt}=\\beta_{0j} \\ \\textrm{and } \\ \\lambda_{jt}=\n",
    "\\lambda_{j},$$ for all $t$, $t=1, \\ldots, T$ and for all $j$, $j=1,\\ldots, p$.\n",
    "Under this assumption, the model is more parsimonious  and avoids some possible identification problem that might arise with\n",
    "increasing the number of time points.\n",
    "\n",
    "To ensure identification of the model, one necessary condition is that the latent variable has a scale and an origin. %When measurement\n",
    "%invariance of loadings and intercepts is imposed,\n",
    "Scale for  $z_{it}$  can be provided either by fixing one loading at a nonzero value or by\n",
    "fixing the factor variance at a nonzero value. In presence of longitudinal data, the same loading is fixed equal to one at each occasion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Modeling\n",
    "\n",
    "We model each observation as a tuple of dimension `(T, p)`, common across individuals. Individuals constitute independent observations, which yields the tensor structure `(n, T, q)`. The time dimension `T` appears in the first dimension since it allows for seamless tensor products of the type `(n, T, q) (q, p)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of the `GLLVM` architecture is as follows: we need\n",
    "\n",
    "* a GLLVM module, which is the model, and contains methods to sample, and has an `AR1` module as well as a `CondMean` module: the first creates the auto-regressive sequence, the second \n",
    "* the `AR1` module, given a sample of independent noise `delta` and parameter `phi`, returns the auto-regressive process according to the above formula\n",
    "* the `CondMean` module which computes the conditional mean which is used in the loss function. This depends on the weights `wx`, `wz`, the biases `b`, and t\n",
    "* a sample method to generate the random variables jointly from the GLLVM. This includes the noise `delta`, the individual-specific random intercepts `u`, and the responses of various types. Additionally, for responses (features) types that require a scale parameter, this includes the `scale` parameter.  \n",
    "* an `Encoder` module that maps the features to the `unmodified` latent variables `u` and `d` (before any parameter affects them), which is learned on the samples, who has its own architecture and parameters which we do not directly care about.\n",
    "* our very own M-estimator loss function\n",
    "* interestingly, the number of parameters do not depend on time nor on number of observations.\n",
    "\n",
    "The data is organized in a 3-dimensional tensor `(batch, seq_length, features)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR # scheduler\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "n = 100\n",
    "p = 15\n",
    "T = 20\n",
    "k = 8\n",
    "q = 1\n",
    "\n",
    "DIMENSIONS_Y = (n, T, p) \n",
    "DIMENSIONS_X = (n, T, k)\n",
    "DIMENSIONS_Z = (n, T, q)\n",
    "DIMENSIONS_U = (n, 1, p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Computes the conditional mean of the gllvm.\"\"\"\n",
    "    def __init__(self, num_latents, num_features, num_covariates):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        self.wz = nn.Parameter(torch.randn((num_latents, num_features)))\n",
    "        self.wx = nn.Parameter(torch.randn((num_covariates, num_features)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, num_features)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u, features_id, features_linkinv):\n",
    "        \"\"\"Combine the z, the u, and the x into the conditional means. The z and the u are already sampled, no need to transform them further.\"\"\"\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see section \"details of tensorproducts\"\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "\n",
    "        # Compute the linear predictor\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "\n",
    "        # Create a placeholder for the conditional means\n",
    "        condmean  = torch.zeros_like(linpar)\n",
    "        # The conditional mean is obtained by applying the inverse link to the linear prepdictor.\n",
    "        # For computational efficiency reasons, we apply the inverse link on all response of the same types in one go.\n",
    "        # It is safe here to modify condmean in-place because the replaced elements have not been used in the computational graph before.\n",
    "        for feature_type, id in features_id.items():\n",
    "            condmean[:,:,id] = features_linkinv[feature_type](linpar[:,:,id])\n",
    "\n",
    "        return condmean\n",
    "    \n",
    "\n",
    "class AR1(nn.Module):\n",
    "    \"\"\"Implementation of an AR1 model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the phi parameter (the only parameter of the AR1 process)\n",
    "        self.phi = nn.Parameter(torch.ones(1)*.5)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        # 'd' is the input tensor containing the noise terms of dimensions (num_observations, seq_length, num_features)\n",
    "\n",
    "        # Initialize output list with the first noise term\n",
    "        z_list = [d[:,0,:]]\n",
    "\n",
    "        # Generate the AR(1) sequence by iteratively adding the effect of\n",
    "        # previous term (scaled by phi) and the current noise term\n",
    "        for i in range(1, d.shape[1]):\n",
    "            z_list.append(z_list[i-1] * self.phi + d[:,i,:])\n",
    "\n",
    "        # Concatenate the list to create our new tensor along the time dimensions 1\n",
    "        z = torch.stack(z_list, dim=1)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def compute_d(self, z):\n",
    "        \"\"\"\n",
    "        Given z, return d. This is the inverse function of forward.\n",
    "        \"\"\"\n",
    "        d_list = [z[:,0,:]]\n",
    "\n",
    "        for i in range(1, z.shape[1]):\n",
    "            state = z[:,i-1,] * self.phi\n",
    "            di = z[:,i,:] - state\n",
    "            d_list.append(di)\n",
    "        \n",
    "        d = torch.stack(d_list, dim=1)\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "# The encoder tries to learn the latent variables given the observations. Since the encoder learns on simulated data, it knows the true value of the latent. Given the latent, there is no time dependence \n",
    "# in this model so no need to consider recursive neural network; a simple feed-forwward neural network can do the trick. What's more, no need to consider the time dimension, although we will here. the network has the same parameters anyway.\n",
    "\n",
    "# The encoder needs to encode the random effects (u) and the latent variables (z)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the features into the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_covariates, hidden_size, num_latents):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_features + num_covariates, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder_z = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_latents)\n",
    "        )\n",
    "\n",
    "        self.encoder_u = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_features)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward on the whole dataset;  Process the whole sequence at once, leveraging shared parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # concatnenate x and y in the features dimension\n",
    "        xy = torch.cat([x, y], dim=2)\n",
    "        z_hat = []\n",
    "        u_hat = []\n",
    "\n",
    "        for i in range(xy.shape[1]):\n",
    "            # select the ith dimension slice\n",
    "            slice_i = xy[:, i, :]\n",
    "            # apply the forward pass to the slice\n",
    "            intermediate_output = self.encoder(slice_i)\n",
    "            output_z = self.encoder_z(intermediate_output)\n",
    "            output_u = self.encoder_u(intermediate_output)\n",
    "            z_hat.append(output_z)\n",
    "            u_hat.append(output_u)\n",
    "\n",
    "        # concatenate the output and create a new time dimension\n",
    "        z_hat = torch.stack(z_hat, dim=1)\n",
    "        u_hat = torch.stack(u_hat, dim=1)\n",
    "        u_hat = torch.mean(u_hat, dim=1, keepdim=True)\n",
    "\n",
    "        return z_hat, u_hat\n",
    "\n",
    "class GLLVM(nn.Module):\n",
    "    \"\"\"\n",
    "    GLLVM model\n",
    "    \n",
    "    Args:\n",
    "        var_types: a dictionary of key:value pairs with keys one of \"gaussian\", \"poisson\", \"ordinal\", \"binary, and values being a list of indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform):\n",
    "        super().__init__()\n",
    "        # Initialize feature types\n",
    "        self.features_id = features_id\n",
    "        self.features_linkinv_sample = features_linkinv_sample\n",
    "        self.features_linkinv_forward = features_linkinv_forward\n",
    "        self.features_transform = features_transform\n",
    "\n",
    "        # Initialize the model dimensions. Note that the number of observations and the sequence length are not model parameters but characteristics of the data.\n",
    "        self.num_features = num_features\n",
    "        self.num_latents = num_latents\n",
    "        self.num_covariates = num_covariates\n",
    "\n",
    "        # Initialize the modules \n",
    "        self.ar1 = AR1()\n",
    "        self.decoder = Decoder(self.num_latents, self.num_features, self.num_covariates)\n",
    "        self.encoder = Encoder(self.num_features, self.num_covariates, 50, self.num_latents)\n",
    "\n",
    "        # Initialize Parameters\n",
    "        self.bias = nn.Parameter(torch.zeros((1,1,self.num_features))) # one bias term per response, shared accross time and observations\n",
    "        self.var_u = nn.Parameter(torch.ones((1,1,self.num_features)))\n",
    "        self.var_y = nn.Parameter(torch.ones((1,1,self.num_covariates))) # this appears in the loss function, but we know all of them to be 1 by design   \n",
    "    \n",
    "    def autoencode(self, x, y):\n",
    "        z, u = self.encoder(x, y)\n",
    "        d = self.ar1.compute_d(z)\n",
    "        u = u/torch.sqrt(self.var_u)\n",
    "        reconstructed = self(x, d, u)\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "    def impute(self, x, y, mask=None):\n",
    "        \"\"\"Impute the data. If mask is set to none, initialize imputation\"\"\"\n",
    "        self.eval()\n",
    "        # If mask is none, initialize imputation by the mean of the features (in dim 2)\n",
    "        if mask is None:\n",
    "            mask = ( y == np.nan)\n",
    "            y[mask] = torch.nanmean(y, dim=(0,1), keepdim=True).repeat(y.shape[0], y.shape[1], 1)[mask]\n",
    "        else:\n",
    "            y[mask] = self.autoencode(x, y)[mask]\n",
    "        return y, mask\n",
    "\n",
    "\n",
    "    def forward(self, x, d, u):\n",
    "        z = self.ar1(d)\n",
    "        u = u * self.var_u\n",
    "        yhat = self.decoder(x, z, u, self.features_id, self.features_linkinv_forward)\n",
    "        return yhat\n",
    "\n",
    "    def sample(self, n, seq_length, x=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y).\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0. Here k is set to.\")\n",
    "                x = torch.randn((n, seq_length, self.num_covariates))\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, seq_length, self.num_latents))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, self.num_features)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.ar1(d)\n",
    "\n",
    "            condmean = self.decoder(x, z, u, self.features_id, self.features_linkinv_sample)\n",
    "\n",
    "            y = torch.zeros_like(condmean)\n",
    "\n",
    "            # Sample and transform where necessary\n",
    "            for feature_type, id in self.features_id.items():\n",
    "                if feature_type == \"binary\":\n",
    "                    y_sample = torch.bernoulli(condmean[:,:,id])\n",
    "                elif feature_type == \"ordinal\":\n",
    "                    # TODO: re-write this part and comment better\n",
    "                    y_ordinal = condmean[:,:,id]\n",
    "                    # draw one uniform for the whole vector\n",
    "                    random = torch.rand((*y_ordinal.shape[0:2], 1)) \n",
    "                    # compare with the cumulative probabilities\n",
    "                    ordinal = torch.sum(random < y_ordinal, axis=2)\n",
    "                    ordinal = torch.nn.functional.one_hot(ordinal).squeeze().float()\n",
    "                    ordinal = ordinal[:,:,1:] # discard the first column of the one_hot encoding, as it is superfluous (as a 0)\n",
    "                    y_sample = ordinal\n",
    "                elif feature_type == \"counts\":\n",
    "                    y_sample = torch.poisson(condmean[:,:,id]) # here we transform the mean using the inverse link - this is due to the transformation of the poisson variable intot a \"gaussin\". For sampling, we actually want the gaussian.\n",
    "\n",
    "                # Transform if any transformation was specified\n",
    "                if feature_type in self.features_transform:\n",
    "                    y_sample = self.features_transform[feature_type](y_sample)\n",
    "                \n",
    "                y[:,:,id] = y_sample\n",
    "\n",
    "        return {\"x\":x, \"y\":y, \"z\":z, \"u\":u, \"d\":d}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(x, y, z, u, model, optimizer, criterion, num_epochs=100, validation_data=None, verbose=True, random=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and return the model and the history of losses.\n",
    "\n",
    "    :param x: The input data for model\n",
    "    :param y: The labels for input data\n",
    "    :param z: additional input data\n",
    "    :param u: additional input data\n",
    "    :param model: PyTorch model to train\n",
    "    :param optimizer: Optimizer to use in training\n",
    "    :param criterion: Loss function to use in training\n",
    "    :param num_epochs: Number of training epochs. Default is 100.\n",
    "    :param validation_data: Tuple of validation data (x_val, y_val, z_val, u_val). Default is None.\n",
    "    :return: Tuple of trained model and history of losses.\n",
    "    \"\"\"\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if random:\n",
    "            x, y, z, u, _ = model.sample(y.shape[0], y.shape[1], x = x).values()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_z, outputs_u = model.encoder(x, y)\n",
    "\n",
    "        loss_z = criterion(outputs_z, z)\n",
    "        loss_u = criterion(outputs_u, u)\n",
    "\n",
    "        # Compute loss\n",
    "\n",
    "        loss = loss_z + loss_u\n",
    "\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if validation_data is not None:\n",
    "            model.eval() # switch model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                x_val, y_val, z_val, u_val, _ = validation_data.values()\n",
    "                val_z, val_u = model.encoder(x_val, y_val)\n",
    "                val_loss_z =  criterion(val_z, z_val)\n",
    "                val_loss_u = criterion(val_u, u_val)\n",
    "                val_loss = val_loss_z + val_loss_u\n",
    "                val_losses.append(val_loss.item())\n",
    "            model.train() # switch model back to training mode\n",
    "\n",
    "        if verbose and (epoch % 10) == 0:\n",
    "            print(f'Epoch: {epoch}, Training Loss: {loss.item()}, Val loss = {None if validation_data is None else val_loss}')\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try generating data\n",
    "features_id = {\n",
    "    'binary': [0,1,2,3,4,5,6,7],\n",
    "    'ordinal': [9, 10],\n",
    "    'counts': [8, 11, 12]\n",
    "}\n",
    "\n",
    "# We define two inverse link functions; one for sampling (the one assumed in the model), and another for computations (the one used to compute gradients)\n",
    "features_linkinv_sample = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: torch.exp(x) \n",
    "}\n",
    "\n",
    "# We define two inverse link functions; one for sampling (the one assumed in the model), and another for computations (the one used to compute gradients)\n",
    "features_linkinv_forward = {\n",
    "    'binary': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'ordinal': lambda x: 1/(1+torch.exp(-x)),\n",
    "    'counts': lambda x: x\n",
    "}\n",
    "\n",
    "features_transform = {\n",
    "    #'binary': lambda x: 2* (x - .5),\n",
    "    #'ordinal': lambda x: (x -5)/5,\n",
    "    'counts': lambda x: torch.log(x + 1)\n",
    "}\n",
    "\n",
    "num_latents = 1\n",
    "num_features = 13\n",
    "num_covariates = 3\n",
    "\n",
    "\n",
    "model_true = GLLVM(num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform)\n",
    "\n",
    "model = GLLVM(num_latents, num_features, num_covariates, features_id, features_linkinv_sample, features_linkinv_forward, features_transform)\n",
    "from copy import deepcopy\n",
    "model = deepcopy(model_true)\n",
    "\n",
    "# Draw data from the true model\n",
    "data_true = model_true.sample(200, 9)\n",
    "val_data = model_true.sample(200, 9, x=data_true['x'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the encoder\n",
    "\n",
    "How good is the encoder at encoding? train on one dataset, evaluate on another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.encoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.3555562496185303, Val loss = 2.289383888244629\n",
      "Epoch: 10, Training Loss: 2.279022216796875, Val loss = 2.2188591957092285\n",
      "Epoch: 20, Training Loss: 2.1575980186462402, Val loss = 2.053225040435791\n",
      "Epoch: 30, Training Loss: 1.793822169303894, Val loss = 1.6832876205444336\n",
      "Epoch: 40, Training Loss: 1.3890771865844727, Val loss = 1.3994807004928589\n",
      "Epoch: 50, Training Loss: 1.2958533763885498, Val loss = 1.303268551826477\n",
      "Epoch: 60, Training Loss: 1.2596155405044556, Val loss = 1.2560166120529175\n",
      "Epoch: 70, Training Loss: 1.2722269296646118, Val loss = 1.2247809171676636\n",
      "Epoch: 80, Training Loss: 1.2079123258590698, Val loss = 1.1966116428375244\n",
      "Epoch: 90, Training Loss: 1.1565247774124146, Val loss = 1.1767849922180176\n",
      "Epoch: 100, Training Loss: 1.1880595684051514, Val loss = 1.1522541046142578\n",
      "Epoch: 110, Training Loss: 1.2044222354888916, Val loss = 1.1352417469024658\n",
      "Epoch: 120, Training Loss: 1.1622223854064941, Val loss = 1.1244322061538696\n",
      "Epoch: 130, Training Loss: 1.0851924419403076, Val loss = 1.1080322265625\n",
      "Epoch: 140, Training Loss: 1.1315596103668213, Val loss = 1.1049824953079224\n",
      "Epoch: 150, Training Loss: 1.099403738975525, Val loss = 1.0913550853729248\n",
      "Epoch: 160, Training Loss: 1.1335749626159668, Val loss = 1.0903955698013306\n",
      "Epoch: 170, Training Loss: 1.0802721977233887, Val loss = 1.0797197818756104\n",
      "Epoch: 180, Training Loss: 1.0548944473266602, Val loss = 1.0795741081237793\n",
      "Epoch: 190, Training Loss: 1.091558575630188, Val loss = 1.0708223581314087\n",
      "Epoch: 200, Training Loss: 1.088852882385254, Val loss = 1.0704751014709473\n",
      "Epoch: 210, Training Loss: 1.0484066009521484, Val loss = 1.0605666637420654\n",
      "Epoch: 220, Training Loss: 1.082438588142395, Val loss = 1.0542867183685303\n",
      "Epoch: 230, Training Loss: 1.0335055589675903, Val loss = 1.0483336448669434\n",
      "Epoch: 240, Training Loss: 1.029476523399353, Val loss = 1.0441060066223145\n",
      "Epoch: 250, Training Loss: 1.0019381046295166, Val loss = 1.032546877861023\n",
      "Epoch: 260, Training Loss: 1.062813639640808, Val loss = 1.0196622610092163\n",
      "Epoch: 270, Training Loss: 1.0173259973526, Val loss = 1.0138416290283203\n",
      "Epoch: 280, Training Loss: 1.0445674657821655, Val loss = 1.0074927806854248\n",
      "Epoch: 290, Training Loss: 1.0691272020339966, Val loss = 1.004593849182129\n",
      "Epoch: 300, Training Loss: 0.9974870681762695, Val loss = 0.999696671962738\n",
      "Epoch: 310, Training Loss: 1.0019162893295288, Val loss = 0.9908825159072876\n",
      "Epoch: 320, Training Loss: 0.9973716735839844, Val loss = 0.9928898811340332\n",
      "Epoch: 330, Training Loss: 1.0212645530700684, Val loss = 0.9798653721809387\n",
      "Epoch: 340, Training Loss: 1.0053026676177979, Val loss = 0.9786154627799988\n",
      "Epoch: 350, Training Loss: 0.9700687527656555, Val loss = 0.9842862486839294\n",
      "Epoch: 360, Training Loss: 0.9769579768180847, Val loss = 0.9669886827468872\n",
      "Epoch: 370, Training Loss: 1.0003694295883179, Val loss = 0.9638906717300415\n",
      "Epoch: 380, Training Loss: 0.9992818832397461, Val loss = 0.9610286951065063\n",
      "Epoch: 390, Training Loss: 0.9636033773422241, Val loss = 0.9630017876625061\n",
      "Epoch: 400, Training Loss: 0.953857958316803, Val loss = 0.9471019506454468\n",
      "Epoch: 410, Training Loss: 1.0079888105392456, Val loss = 0.9678761959075928\n",
      "Epoch: 420, Training Loss: 0.9216519594192505, Val loss = 0.9457249641418457\n",
      "Epoch: 430, Training Loss: 0.9894619584083557, Val loss = 0.9399905204772949\n",
      "Epoch: 440, Training Loss: 1.0069077014923096, Val loss = 0.9443179368972778\n",
      "Epoch: 450, Training Loss: 0.9728281497955322, Val loss = 0.9389387965202332\n",
      "Epoch: 460, Training Loss: 0.9788689613342285, Val loss = 0.9337867498397827\n",
      "Epoch: 470, Training Loss: 0.9499390721321106, Val loss = 0.9436620473861694\n",
      "Epoch: 480, Training Loss: 0.9392143487930298, Val loss = 0.9377987384796143\n",
      "Epoch: 490, Training Loss: 0.9145255088806152, Val loss = 0.9256581664085388\n",
      "Epoch: 500, Training Loss: 0.9015676975250244, Val loss = 0.9331665635108948\n",
      "Epoch: 510, Training Loss: 0.9483482837677002, Val loss = 0.9209352731704712\n",
      "Epoch: 520, Training Loss: 0.9208051562309265, Val loss = 0.91758131980896\n",
      "Epoch: 530, Training Loss: 0.9286639094352722, Val loss = 0.9255513548851013\n",
      "Epoch: 540, Training Loss: 1.042062520980835, Val loss = 0.9106922149658203\n",
      "Epoch: 550, Training Loss: 0.9964500665664673, Val loss = 0.9065878987312317\n",
      "Epoch: 560, Training Loss: 0.9202839136123657, Val loss = 0.909165620803833\n",
      "Epoch: 570, Training Loss: 0.9713712930679321, Val loss = 0.9036363363265991\n",
      "Epoch: 580, Training Loss: 0.9679722785949707, Val loss = 0.910723865032196\n",
      "Epoch: 590, Training Loss: 0.9328250885009766, Val loss = 0.9018006324768066\n",
      "Epoch: 600, Training Loss: 0.8792732954025269, Val loss = 0.9027202725410461\n",
      "Epoch: 610, Training Loss: 0.8792245388031006, Val loss = 0.893352746963501\n",
      "Epoch: 620, Training Loss: 0.8784517645835876, Val loss = 0.8956747651100159\n",
      "Epoch: 630, Training Loss: 0.9046604633331299, Val loss = 0.9029658436775208\n",
      "Epoch: 640, Training Loss: 0.9138436317443848, Val loss = 0.8976787328720093\n",
      "Epoch: 650, Training Loss: 0.8927084803581238, Val loss = 0.8969441652297974\n",
      "Epoch: 660, Training Loss: 0.9392831325531006, Val loss = 0.8937174677848816\n",
      "Epoch: 670, Training Loss: 0.9418858885765076, Val loss = 0.8950930237770081\n",
      "Epoch: 680, Training Loss: 0.9400845170021057, Val loss = 0.8889147043228149\n",
      "Epoch: 690, Training Loss: 0.9355772733688354, Val loss = 0.8843239545822144\n",
      "Epoch: 700, Training Loss: 0.899305522441864, Val loss = 0.8906950950622559\n",
      "Epoch: 710, Training Loss: 0.9118854999542236, Val loss = 0.8756001591682434\n",
      "Epoch: 720, Training Loss: 0.8938004970550537, Val loss = 0.8925082683563232\n",
      "Epoch: 730, Training Loss: 0.8882694244384766, Val loss = 0.8784686326980591\n",
      "Epoch: 740, Training Loss: 0.8392341732978821, Val loss = 0.8888407349586487\n",
      "Epoch: 750, Training Loss: 0.8999155759811401, Val loss = 0.8695768117904663\n",
      "Epoch: 760, Training Loss: 0.8686791658401489, Val loss = 0.867604672908783\n",
      "Epoch: 770, Training Loss: 0.9217720031738281, Val loss = 0.87547767162323\n",
      "Epoch: 780, Training Loss: 0.9213012456893921, Val loss = 0.8587989807128906\n",
      "Epoch: 790, Training Loss: 0.8654391765594482, Val loss = 0.856870174407959\n",
      "Epoch: 800, Training Loss: 0.87838214635849, Val loss = 0.8563688397407532\n",
      "Epoch: 810, Training Loss: 0.9000176191329956, Val loss = 0.8544499278068542\n",
      "Epoch: 820, Training Loss: 0.8620272874832153, Val loss = 0.8552147746086121\n",
      "Epoch: 830, Training Loss: 0.8623538017272949, Val loss = 0.8525424003601074\n",
      "Epoch: 840, Training Loss: 0.8484839200973511, Val loss = 0.8475637435913086\n",
      "Epoch: 850, Training Loss: 0.8716112375259399, Val loss = 0.8514041304588318\n",
      "Epoch: 860, Training Loss: 0.8640010952949524, Val loss = 0.8401069641113281\n",
      "Epoch: 870, Training Loss: 0.8648256063461304, Val loss = 0.8346364498138428\n",
      "Epoch: 880, Training Loss: 0.8019059896469116, Val loss = 0.8309618234634399\n",
      "Epoch: 890, Training Loss: 0.8555269241333008, Val loss = 0.832306444644928\n",
      "Epoch: 900, Training Loss: 0.8678289651870728, Val loss = 0.8282290697097778\n",
      "Epoch: 910, Training Loss: 0.8603498339653015, Val loss = 0.8299392461776733\n",
      "Epoch: 920, Training Loss: 0.8372530937194824, Val loss = 0.8388922214508057\n",
      "Epoch: 930, Training Loss: 0.8340708613395691, Val loss = 0.8266890645027161\n",
      "Epoch: 940, Training Loss: 0.8756039142608643, Val loss = 0.8209328651428223\n",
      "Epoch: 950, Training Loss: 0.8387010097503662, Val loss = 0.8178489208221436\n",
      "Epoch: 960, Training Loss: 0.8251733183860779, Val loss = 0.8147200345993042\n",
      "Epoch: 970, Training Loss: 0.8597961664199829, Val loss = 0.8279020190238953\n",
      "Epoch: 980, Training Loss: 0.8272156119346619, Val loss = 0.8220334053039551\n",
      "Epoch: 990, Training Loss: 0.8293024897575378, Val loss = 0.8184217214584351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GLLVM(\n",
       "   (ar1): AR1()\n",
       "   (decoder): Decoder()\n",
       "   (encoder): Encoder(\n",
       "     (encoder): Sequential(\n",
       "       (0): Linear(in_features=16, out_features=50, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (encoder_z): Sequential(\n",
       "       (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "       (3): ReLU()\n",
       "       (4): Linear(in_features=50, out_features=1, bias=True)\n",
       "     )\n",
       "     (encoder_u): Sequential(\n",
       "       (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "       (3): ReLU()\n",
       "       (4): Linear(in_features=50, out_features=13, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " [2.3555562496185303,\n",
       "  2.364841938018799,\n",
       "  2.26399564743042,\n",
       "  2.3544061183929443,\n",
       "  2.247199058532715,\n",
       "  2.31443190574646,\n",
       "  2.2559638023376465,\n",
       "  2.2599849700927734,\n",
       "  2.2586607933044434,\n",
       "  2.1991100311279297,\n",
       "  2.279022216796875,\n",
       "  2.2847487926483154,\n",
       "  2.272914409637451,\n",
       "  2.1423728466033936,\n",
       "  2.2507925033569336,\n",
       "  2.1520087718963623,\n",
       "  2.1616716384887695,\n",
       "  2.1940252780914307,\n",
       "  2.0680363178253174,\n",
       "  2.01802396774292,\n",
       "  2.1575980186462402,\n",
       "  2.096508026123047,\n",
       "  2.112717866897583,\n",
       "  2.0849967002868652,\n",
       "  2.00691294670105,\n",
       "  1.9390240907669067,\n",
       "  2.0116183757781982,\n",
       "  1.8371245861053467,\n",
       "  1.810387372970581,\n",
       "  1.7242392301559448,\n",
       "  1.793822169303894,\n",
       "  1.6668106317520142,\n",
       "  1.5613927841186523,\n",
       "  1.6313217878341675,\n",
       "  1.6117041110992432,\n",
       "  1.526229977607727,\n",
       "  1.52889883518219,\n",
       "  1.4472928047180176,\n",
       "  1.3733651638031006,\n",
       "  1.396693468093872,\n",
       "  1.3890771865844727,\n",
       "  1.3989381790161133,\n",
       "  1.4344074726104736,\n",
       "  1.3793106079101562,\n",
       "  1.3392118215560913,\n",
       "  1.3865405321121216,\n",
       "  1.3440525531768799,\n",
       "  1.3233286142349243,\n",
       "  1.3023993968963623,\n",
       "  1.3399056196212769,\n",
       "  1.2958533763885498,\n",
       "  1.3242948055267334,\n",
       "  1.3158338069915771,\n",
       "  1.2196495532989502,\n",
       "  1.2549355030059814,\n",
       "  1.2649242877960205,\n",
       "  1.2439411878585815,\n",
       "  1.2963593006134033,\n",
       "  1.282569408416748,\n",
       "  1.2533814907073975,\n",
       "  1.2596155405044556,\n",
       "  1.256385087966919,\n",
       "  1.1937912702560425,\n",
       "  1.239257574081421,\n",
       "  1.2777506113052368,\n",
       "  1.250833511352539,\n",
       "  1.2867891788482666,\n",
       "  1.233494520187378,\n",
       "  1.192086935043335,\n",
       "  1.1892181634902954,\n",
       "  1.2722269296646118,\n",
       "  1.2275264263153076,\n",
       "  1.184005618095398,\n",
       "  1.2529858350753784,\n",
       "  1.2324014902114868,\n",
       "  1.171910285949707,\n",
       "  1.1798235177993774,\n",
       "  1.2345032691955566,\n",
       "  1.2247110605239868,\n",
       "  1.219245433807373,\n",
       "  1.2079123258590698,\n",
       "  1.1527466773986816,\n",
       "  1.2082189321517944,\n",
       "  1.2150845527648926,\n",
       "  1.1969695091247559,\n",
       "  1.1792858839035034,\n",
       "  1.1808178424835205,\n",
       "  1.2315926551818848,\n",
       "  1.2448471784591675,\n",
       "  1.1690768003463745,\n",
       "  1.1565247774124146,\n",
       "  1.1713327169418335,\n",
       "  1.1617133617401123,\n",
       "  1.18438720703125,\n",
       "  1.1712474822998047,\n",
       "  1.1411844491958618,\n",
       "  1.226783037185669,\n",
       "  1.1791027784347534,\n",
       "  1.205517053604126,\n",
       "  1.1728179454803467,\n",
       "  1.1880595684051514,\n",
       "  1.2146542072296143,\n",
       "  1.2122035026550293,\n",
       "  1.189789891242981,\n",
       "  1.117447018623352,\n",
       "  1.1309394836425781,\n",
       "  1.1532883644104004,\n",
       "  1.1297364234924316,\n",
       "  1.1208642721176147,\n",
       "  1.1731007099151611,\n",
       "  1.2044222354888916,\n",
       "  1.1588677167892456,\n",
       "  1.169865608215332,\n",
       "  1.1188114881515503,\n",
       "  1.135364294052124,\n",
       "  1.119500994682312,\n",
       "  1.192365050315857,\n",
       "  1.1162067651748657,\n",
       "  1.1816304922103882,\n",
       "  1.1294856071472168,\n",
       "  1.1622223854064941,\n",
       "  1.1445260047912598,\n",
       "  1.1757289171218872,\n",
       "  1.1867929697036743,\n",
       "  1.1406718492507935,\n",
       "  1.0855823755264282,\n",
       "  1.0488799810409546,\n",
       "  1.1553133726119995,\n",
       "  1.157789707183838,\n",
       "  1.1019847393035889,\n",
       "  1.0851924419403076,\n",
       "  1.1359825134277344,\n",
       "  1.1417577266693115,\n",
       "  1.1706194877624512,\n",
       "  1.1360379457473755,\n",
       "  1.135185718536377,\n",
       "  1.1283501386642456,\n",
       "  1.0945059061050415,\n",
       "  1.1555535793304443,\n",
       "  1.1046481132507324,\n",
       "  1.1315596103668213,\n",
       "  1.0913777351379395,\n",
       "  1.1106178760528564,\n",
       "  1.1316455602645874,\n",
       "  1.1284493207931519,\n",
       "  1.115370750427246,\n",
       "  1.1045899391174316,\n",
       "  1.1064848899841309,\n",
       "  1.0928984880447388,\n",
       "  1.073832631111145,\n",
       "  1.099403738975525,\n",
       "  1.1576471328735352,\n",
       "  1.1104124784469604,\n",
       "  1.0692501068115234,\n",
       "  1.0790671110153198,\n",
       "  1.1899802684783936,\n",
       "  1.121513843536377,\n",
       "  1.1119585037231445,\n",
       "  1.1434094905853271,\n",
       "  1.1253507137298584,\n",
       "  1.1335749626159668,\n",
       "  1.1270394325256348,\n",
       "  1.1329524517059326,\n",
       "  1.1500658988952637,\n",
       "  1.1152666807174683,\n",
       "  1.175417184829712,\n",
       "  1.09521484375,\n",
       "  1.0730245113372803,\n",
       "  1.1017831563949585,\n",
       "  1.099295973777771,\n",
       "  1.0802721977233887,\n",
       "  1.1190621852874756,\n",
       "  1.1410632133483887,\n",
       "  1.1065329313278198,\n",
       "  1.0058696269989014,\n",
       "  1.0555609464645386,\n",
       "  1.102448582649231,\n",
       "  1.1066222190856934,\n",
       "  1.033337116241455,\n",
       "  1.1304359436035156,\n",
       "  1.0548944473266602,\n",
       "  1.0986214876174927,\n",
       "  1.0544008016586304,\n",
       "  1.1142432689666748,\n",
       "  1.0932202339172363,\n",
       "  1.0868538618087769,\n",
       "  1.0470861196517944,\n",
       "  1.135241150856018,\n",
       "  1.0599536895751953,\n",
       "  1.1032780408859253,\n",
       "  1.091558575630188,\n",
       "  1.0670721530914307,\n",
       "  1.0930602550506592,\n",
       "  1.1265711784362793,\n",
       "  1.0700143575668335,\n",
       "  1.0905300378799438,\n",
       "  1.0599021911621094,\n",
       "  1.0995129346847534,\n",
       "  1.0843310356140137,\n",
       "  1.0775421857833862,\n",
       "  1.088852882385254,\n",
       "  1.0901210308074951,\n",
       "  1.0935534238815308,\n",
       "  1.1474671363830566,\n",
       "  1.061438798904419,\n",
       "  1.0799617767333984,\n",
       "  1.088270902633667,\n",
       "  1.1311500072479248,\n",
       "  1.0572768449783325,\n",
       "  1.0836302042007446,\n",
       "  1.0484066009521484,\n",
       "  1.0883814096450806,\n",
       "  1.0863672494888306,\n",
       "  1.1222150325775146,\n",
       "  1.0228374004364014,\n",
       "  1.0818548202514648,\n",
       "  1.0503991842269897,\n",
       "  1.10794198513031,\n",
       "  1.0786380767822266,\n",
       "  1.0184192657470703,\n",
       "  1.082438588142395,\n",
       "  1.0832982063293457,\n",
       "  1.1063432693481445,\n",
       "  1.075000286102295,\n",
       "  1.0571658611297607,\n",
       "  1.0515246391296387,\n",
       "  1.0461020469665527,\n",
       "  1.0517246723175049,\n",
       "  1.0287357568740845,\n",
       "  1.096567153930664,\n",
       "  1.0335055589675903,\n",
       "  1.0833626985549927,\n",
       "  1.0094387531280518,\n",
       "  1.0241405963897705,\n",
       "  1.0279319286346436,\n",
       "  1.0392459630966187,\n",
       "  1.0245691537857056,\n",
       "  1.0367414951324463,\n",
       "  1.1070798635482788,\n",
       "  1.0240389108657837,\n",
       "  1.029476523399353,\n",
       "  1.039362907409668,\n",
       "  1.0474047660827637,\n",
       "  1.0751605033874512,\n",
       "  1.0420348644256592,\n",
       "  0.9916378855705261,\n",
       "  1.036321759223938,\n",
       "  1.027657151222229,\n",
       "  1.0412930250167847,\n",
       "  1.0906559228897095,\n",
       "  1.0019381046295166,\n",
       "  1.0085519552230835,\n",
       "  1.0170449018478394,\n",
       "  1.018710970878601,\n",
       "  1.0657140016555786,\n",
       "  1.0500905513763428,\n",
       "  1.051527500152588,\n",
       "  1.042221188545227,\n",
       "  1.0235297679901123,\n",
       "  0.997779130935669,\n",
       "  1.062813639640808,\n",
       "  1.1209137439727783,\n",
       "  1.0645672082901,\n",
       "  1.0471680164337158,\n",
       "  1.0176118612289429,\n",
       "  1.0406482219696045,\n",
       "  0.9960184693336487,\n",
       "  1.018025279045105,\n",
       "  0.9981746673583984,\n",
       "  0.9919840097427368,\n",
       "  1.0173259973526,\n",
       "  1.0662423372268677,\n",
       "  1.0187374353408813,\n",
       "  1.1362468004226685,\n",
       "  1.02171790599823,\n",
       "  1.031620979309082,\n",
       "  1.0098910331726074,\n",
       "  0.9878459572792053,\n",
       "  1.0063600540161133,\n",
       "  1.0188884735107422,\n",
       "  1.0445674657821655,\n",
       "  1.0537039041519165,\n",
       "  1.014256238937378,\n",
       "  0.9722694158554077,\n",
       "  1.0189545154571533,\n",
       "  1.0075287818908691,\n",
       "  1.0075390338897705,\n",
       "  0.997869074344635,\n",
       "  1.0234785079956055,\n",
       "  0.9926021099090576,\n",
       "  1.0691272020339966,\n",
       "  0.9439310431480408,\n",
       "  1.0047318935394287,\n",
       "  0.9815002679824829,\n",
       "  0.9995837211608887,\n",
       "  1.032617449760437,\n",
       "  1.0004971027374268,\n",
       "  0.9689290523529053,\n",
       "  1.0145364999771118,\n",
       "  0.9869950413703918,\n",
       "  0.9974870681762695,\n",
       "  1.0248348712921143,\n",
       "  1.0259193181991577,\n",
       "  1.0499038696289062,\n",
       "  1.029022216796875,\n",
       "  1.0119661092758179,\n",
       "  1.0263519287109375,\n",
       "  1.0269358158111572,\n",
       "  0.9972832798957825,\n",
       "  1.0482889413833618,\n",
       "  1.0019162893295288,\n",
       "  1.0376412868499756,\n",
       "  1.0231764316558838,\n",
       "  0.9821661710739136,\n",
       "  1.0413551330566406,\n",
       "  0.9850592017173767,\n",
       "  0.9759806394577026,\n",
       "  0.9872997999191284,\n",
       "  1.0075366497039795,\n",
       "  1.0058112144470215,\n",
       "  0.9973716735839844,\n",
       "  1.0134097337722778,\n",
       "  0.9897774457931519,\n",
       "  0.9996511340141296,\n",
       "  0.943358302116394,\n",
       "  0.9677205085754395,\n",
       "  0.9982234239578247,\n",
       "  0.9792439937591553,\n",
       "  0.9747116565704346,\n",
       "  0.9565416574478149,\n",
       "  1.0212645530700684,\n",
       "  1.0137792825698853,\n",
       "  0.9898648858070374,\n",
       "  0.9251848459243774,\n",
       "  1.0107942819595337,\n",
       "  0.9945331811904907,\n",
       "  1.0191878080368042,\n",
       "  1.0059596300125122,\n",
       "  1.0107136964797974,\n",
       "  0.9736337065696716,\n",
       "  1.0053026676177979,\n",
       "  0.9589722752571106,\n",
       "  0.9892793893814087,\n",
       "  1.0050508975982666,\n",
       "  0.9772093892097473,\n",
       "  0.958823561668396,\n",
       "  0.9609187841415405,\n",
       "  0.9687600135803223,\n",
       "  1.0038723945617676,\n",
       "  1.0017609596252441,\n",
       "  0.9700687527656555,\n",
       "  0.9895111322402954,\n",
       "  1.0144038200378418,\n",
       "  0.9620204567909241,\n",
       "  1.0215027332305908,\n",
       "  0.9679771661758423,\n",
       "  0.9657773971557617,\n",
       "  1.0108225345611572,\n",
       "  0.977286159992218,\n",
       "  0.9966408014297485,\n",
       "  0.9769579768180847,\n",
       "  0.9786846041679382,\n",
       "  1.017637848854065,\n",
       "  0.9999195337295532,\n",
       "  0.9939867854118347,\n",
       "  0.9568095207214355,\n",
       "  0.9202530384063721,\n",
       "  0.9813008308410645,\n",
       "  1.0049999952316284,\n",
       "  0.9406155347824097,\n",
       "  1.0003694295883179,\n",
       "  1.024609088897705,\n",
       "  0.9269098043441772,\n",
       "  1.0100315809249878,\n",
       "  0.9653743505477905,\n",
       "  0.9987945556640625,\n",
       "  1.0242668390274048,\n",
       "  1.0026335716247559,\n",
       "  0.9449048042297363,\n",
       "  0.9597741961479187,\n",
       "  0.9992818832397461,\n",
       "  0.9943273067474365,\n",
       "  0.9697258472442627,\n",
       "  0.9499294757843018,\n",
       "  1.026485800743103,\n",
       "  0.9901363253593445,\n",
       "  0.9818038940429688,\n",
       "  1.0032869577407837,\n",
       "  0.9303441047668457,\n",
       "  0.9774261713027954,\n",
       "  0.9636033773422241,\n",
       "  0.935808539390564,\n",
       "  0.9330412745475769,\n",
       "  0.9401978254318237,\n",
       "  0.9845260381698608,\n",
       "  0.911547064781189,\n",
       "  1.0129363536834717,\n",
       "  0.9887513518333435,\n",
       "  0.9576247334480286,\n",
       "  0.9654788970947266,\n",
       "  0.953857958316803,\n",
       "  0.9622241258621216,\n",
       "  0.9835517406463623,\n",
       "  0.9456112384796143,\n",
       "  0.9780031442642212,\n",
       "  0.960561990737915,\n",
       "  1.0263912677764893,\n",
       "  0.932030200958252,\n",
       "  0.9137591123580933,\n",
       "  1.0035442113876343,\n",
       "  1.0079888105392456,\n",
       "  0.9096792936325073,\n",
       "  0.9805883169174194,\n",
       "  0.9470269680023193,\n",
       "  0.91074538230896,\n",
       "  0.9075478911399841,\n",
       "  1.0276663303375244,\n",
       "  1.0193136930465698,\n",
       "  0.9718900322914124,\n",
       "  0.9440373182296753,\n",
       "  0.9216519594192505,\n",
       "  0.9819537997245789,\n",
       "  0.966982901096344,\n",
       "  0.999889612197876,\n",
       "  0.9830424785614014,\n",
       "  0.9339600801467896,\n",
       "  0.9567975997924805,\n",
       "  0.9673467874526978,\n",
       "  0.9898583889007568,\n",
       "  0.9436026811599731,\n",
       "  0.9894619584083557,\n",
       "  0.9790078997612,\n",
       "  1.008493423461914,\n",
       "  0.9824079871177673,\n",
       "  0.9812756776809692,\n",
       "  0.9325118660926819,\n",
       "  1.000577688217163,\n",
       "  0.9658594727516174,\n",
       "  0.9240220785140991,\n",
       "  0.9742883443832397,\n",
       "  1.0069077014923096,\n",
       "  0.9922263026237488,\n",
       "  0.9833599328994751,\n",
       "  0.9375105500221252,\n",
       "  0.9440158605575562,\n",
       "  0.9846460819244385,\n",
       "  0.9634342193603516,\n",
       "  0.9322589635848999,\n",
       "  0.9489405155181885,\n",
       "  0.9313212037086487,\n",
       "  0.9728281497955322,\n",
       "  0.9313357472419739,\n",
       "  0.9716519117355347,\n",
       "  0.9793562889099121,\n",
       "  0.9622407555580139,\n",
       "  0.9772573709487915,\n",
       "  0.9874615669250488,\n",
       "  0.9479202628135681,\n",
       "  0.9678300619125366,\n",
       "  0.9414032697677612,\n",
       "  0.9788689613342285,\n",
       "  0.9421577453613281,\n",
       "  0.9404166340827942,\n",
       "  0.9846255779266357,\n",
       "  0.9361533522605896,\n",
       "  0.9900866746902466,\n",
       "  0.9301436543464661,\n",
       "  0.9949027299880981,\n",
       "  0.9775292873382568,\n",
       "  0.9971827268600464,\n",
       "  0.9499390721321106,\n",
       "  0.9336583018302917,\n",
       "  0.95438551902771,\n",
       "  0.9824903011322021,\n",
       "  0.9399058818817139,\n",
       "  0.9628058671951294,\n",
       "  0.9787278175354004,\n",
       "  0.978293776512146,\n",
       "  0.9220528602600098,\n",
       "  0.966801643371582,\n",
       "  0.9392143487930298,\n",
       "  0.9619045257568359,\n",
       "  0.958312451839447,\n",
       "  0.9979058504104614,\n",
       "  1.007641315460205,\n",
       "  0.9164751768112183,\n",
       "  0.9403811097145081,\n",
       "  0.9394190907478333,\n",
       "  0.9487709403038025,\n",
       "  0.9186254143714905,\n",
       "  0.9145255088806152,\n",
       "  0.924206554889679,\n",
       "  0.9657718539237976,\n",
       "  0.9686506390571594,\n",
       "  0.9017714858055115,\n",
       "  0.9227672219276428,\n",
       "  0.972989559173584,\n",
       "  0.9450851678848267,\n",
       "  0.979525625705719,\n",
       "  0.9578915238380432,\n",
       "  0.9015676975250244,\n",
       "  0.9038543105125427,\n",
       "  0.9852306842803955,\n",
       "  0.928784966468811,\n",
       "  0.8943163156509399,\n",
       "  0.9618163704872131,\n",
       "  0.9677123427391052,\n",
       "  0.9786245822906494,\n",
       "  0.92921382188797,\n",
       "  0.9512795209884644,\n",
       "  0.9483482837677002,\n",
       "  0.9636335372924805,\n",
       "  0.9806733131408691,\n",
       "  0.9050503373146057,\n",
       "  0.9120559096336365,\n",
       "  0.9680423736572266,\n",
       "  0.9824273586273193,\n",
       "  0.9325146675109863,\n",
       "  0.9644688367843628,\n",
       "  0.9587788581848145,\n",
       "  0.9208051562309265,\n",
       "  0.9233750104904175,\n",
       "  0.8976328372955322,\n",
       "  0.9677772521972656,\n",
       "  0.9544318914413452,\n",
       "  0.9269829988479614,\n",
       "  0.9487806558609009,\n",
       "  0.938819408416748,\n",
       "  0.9163120985031128,\n",
       "  0.926315426826477,\n",
       "  0.9286639094352722,\n",
       "  0.9598751068115234,\n",
       "  0.9024202823638916,\n",
       "  0.9528276920318604,\n",
       "  0.9262199997901917,\n",
       "  0.9493364095687866,\n",
       "  0.9168898463249207,\n",
       "  0.9350851774215698,\n",
       "  0.8688110709190369,\n",
       "  0.9382301568984985,\n",
       "  1.042062520980835,\n",
       "  0.9540450572967529,\n",
       "  0.9658413529396057,\n",
       "  0.910344660282135,\n",
       "  0.9074781537055969,\n",
       "  0.8868716955184937,\n",
       "  0.9561739563941956,\n",
       "  0.9499242901802063,\n",
       "  0.9325098991394043,\n",
       "  0.9321735501289368,\n",
       "  0.9964500665664673,\n",
       "  0.8527079224586487,\n",
       "  0.9406467080116272,\n",
       "  0.9002164602279663,\n",
       "  0.8939205408096313,\n",
       "  0.9025707244873047,\n",
       "  0.9679398536682129,\n",
       "  0.9094247221946716,\n",
       "  0.9383312463760376,\n",
       "  0.8673015832901001,\n",
       "  0.9202839136123657,\n",
       "  0.8946168422698975,\n",
       "  0.8889734745025635,\n",
       "  0.9267090559005737,\n",
       "  0.9097324013710022,\n",
       "  0.9397447109222412,\n",
       "  0.9189165830612183,\n",
       "  0.8812178373336792,\n",
       "  0.8960517644882202,\n",
       "  0.9145764708518982,\n",
       "  0.9713712930679321,\n",
       "  0.9062504768371582,\n",
       "  0.9072167277336121,\n",
       "  0.9018885493278503,\n",
       "  0.9080517292022705,\n",
       "  0.9772747755050659,\n",
       "  0.9458917379379272,\n",
       "  0.8976020216941833,\n",
       "  0.9477323293685913,\n",
       "  0.9284019470214844,\n",
       "  0.9679722785949707,\n",
       "  0.8859640955924988,\n",
       "  0.880757212638855,\n",
       "  0.9682475924491882,\n",
       "  0.9381135702133179,\n",
       "  0.9187623858451843,\n",
       "  0.9378819465637207,\n",
       "  0.9292773008346558,\n",
       "  0.9510837197303772,\n",
       "  0.9193028807640076,\n",
       "  0.9328250885009766,\n",
       "  0.9043812155723572,\n",
       "  0.927189826965332,\n",
       "  0.8847683072090149,\n",
       "  0.872404932975769,\n",
       "  0.9225766658782959,\n",
       "  0.9040944576263428,\n",
       "  0.8770972490310669,\n",
       "  0.8936927318572998,\n",
       "  0.8914295434951782,\n",
       "  0.8792732954025269,\n",
       "  0.889222264289856,\n",
       "  0.8813987970352173,\n",
       "  0.9546427130699158,\n",
       "  0.9326308965682983,\n",
       "  0.8994061350822449,\n",
       "  0.9016827940940857,\n",
       "  0.9115475416183472,\n",
       "  0.8983176946640015,\n",
       "  0.9273197650909424,\n",
       "  0.8792245388031006,\n",
       "  0.8776276111602783,\n",
       "  0.8923345804214478,\n",
       "  0.9760228991508484,\n",
       "  0.9749046564102173,\n",
       "  0.8809630274772644,\n",
       "  0.8930404782295227,\n",
       "  0.897497832775116,\n",
       "  0.9006110429763794,\n",
       "  0.9109755158424377,\n",
       "  0.8784517645835876,\n",
       "  0.8972914814949036,\n",
       "  0.9089758396148682,\n",
       "  0.9005892872810364,\n",
       "  0.9287995100021362,\n",
       "  0.9760591983795166,\n",
       "  0.9208734035491943,\n",
       "  0.909561276435852,\n",
       "  0.9408848285675049,\n",
       "  0.9512256383895874,\n",
       "  0.9046604633331299,\n",
       "  0.964239239692688,\n",
       "  0.8775157332420349,\n",
       "  0.907166600227356,\n",
       "  0.9086592793464661,\n",
       "  0.9151443243026733,\n",
       "  0.9370683431625366,\n",
       "  0.8906363248825073,\n",
       "  0.9253674149513245,\n",
       "  0.9345020651817322,\n",
       "  0.9138436317443848,\n",
       "  0.9840107560157776,\n",
       "  0.8906322121620178,\n",
       "  0.8938084840774536,\n",
       "  0.9588409662246704,\n",
       "  0.9278062582015991,\n",
       "  0.9162246584892273,\n",
       "  0.8730865716934204,\n",
       "  0.8891128301620483,\n",
       "  0.9209979772567749,\n",
       "  0.8927084803581238,\n",
       "  0.9092355966567993,\n",
       "  0.9317883253097534,\n",
       "  0.8801552653312683,\n",
       "  0.952280580997467,\n",
       "  0.9339646100997925,\n",
       "  0.9221545457839966,\n",
       "  0.945655107498169,\n",
       "  0.9076751470565796,\n",
       "  0.8724361658096313,\n",
       "  0.9392831325531006,\n",
       "  0.9111597537994385,\n",
       "  0.980431318283081,\n",
       "  0.9151787757873535,\n",
       "  0.9243013858795166,\n",
       "  0.9007247686386108,\n",
       "  0.8737034201622009,\n",
       "  0.9173758029937744,\n",
       "  0.8896035552024841,\n",
       "  0.857776939868927,\n",
       "  0.9418858885765076,\n",
       "  0.947809100151062,\n",
       "  0.893119215965271,\n",
       "  0.9331831336021423,\n",
       "  0.9458233714103699,\n",
       "  0.8563284873962402,\n",
       "  0.8851854205131531,\n",
       "  0.9410179853439331,\n",
       "  0.9042484164237976,\n",
       "  0.9074790477752686,\n",
       "  0.9400845170021057,\n",
       "  0.8938990235328674,\n",
       "  0.92344069480896,\n",
       "  0.9174973368644714,\n",
       "  0.8895721435546875,\n",
       "  0.8861942887306213,\n",
       "  0.9052600860595703,\n",
       "  0.9028251767158508,\n",
       "  0.9165859818458557,\n",
       "  0.9033697247505188,\n",
       "  0.9355772733688354,\n",
       "  0.8647222518920898,\n",
       "  0.8588874936103821,\n",
       "  0.8625876307487488,\n",
       "  0.9117037057876587,\n",
       "  0.857941210269928,\n",
       "  0.8768138885498047,\n",
       "  0.848797082901001,\n",
       "  0.9259560108184814,\n",
       "  0.8651067614555359,\n",
       "  0.899305522441864,\n",
       "  0.9020689725875854,\n",
       "  0.8592769503593445,\n",
       "  0.8968400359153748,\n",
       "  0.9024257659912109,\n",
       "  0.8996984362602234,\n",
       "  0.8579367399215698,\n",
       "  0.9019243717193604,\n",
       "  0.9184658527374268,\n",
       "  0.9184619188308716,\n",
       "  0.9118854999542236,\n",
       "  0.8715485334396362,\n",
       "  0.9052125215530396,\n",
       "  0.9694027304649353,\n",
       "  0.8822686076164246,\n",
       "  0.8942849636077881,\n",
       "  0.9133340120315552,\n",
       "  0.9074125289916992,\n",
       "  0.9062793254852295,\n",
       "  0.8968875408172607,\n",
       "  0.8938004970550537,\n",
       "  0.9749206304550171,\n",
       "  0.8828603029251099,\n",
       "  0.9402783513069153,\n",
       "  0.8639088869094849,\n",
       "  0.9100905656814575,\n",
       "  0.8606546521186829,\n",
       "  0.8909390568733215,\n",
       "  0.8989588022232056,\n",
       "  0.8766905069351196,\n",
       "  0.8882694244384766,\n",
       "  0.8853010535240173,\n",
       "  0.9285027384757996,\n",
       "  0.8833238482475281,\n",
       "  0.8932764530181885,\n",
       "  0.9077451229095459,\n",
       "  0.9407109022140503,\n",
       "  0.8577805757522583,\n",
       "  0.8770859241485596,\n",
       "  0.9222202301025391,\n",
       "  0.8392341732978821,\n",
       "  0.8966971039772034,\n",
       "  0.867984414100647,\n",
       "  0.8763667345046997,\n",
       "  0.8855089545249939,\n",
       "  0.8994523882865906,\n",
       "  0.848750650882721,\n",
       "  0.8932777047157288,\n",
       "  0.901456892490387,\n",
       "  0.86406010389328,\n",
       "  0.8999155759811401,\n",
       "  0.904017984867096,\n",
       "  0.8734313249588013,\n",
       "  0.8848416805267334,\n",
       "  0.9086095094680786,\n",
       "  0.9025318026542664,\n",
       "  0.8791913986206055,\n",
       "  0.9248276948928833,\n",
       "  0.8523688316345215,\n",
       "  0.9076474905014038,\n",
       "  0.8686791658401489,\n",
       "  0.8873613476753235,\n",
       "  0.9025219678878784,\n",
       "  0.8346545100212097,\n",
       "  0.9102745652198792,\n",
       "  0.8551445007324219,\n",
       "  0.8815606236457825,\n",
       "  0.8747610449790955,\n",
       "  0.9351575374603271,\n",
       "  0.904962420463562,\n",
       "  0.9217720031738281,\n",
       "  0.9113767743110657,\n",
       "  0.8917316794395447,\n",
       "  0.8994524478912354,\n",
       "  0.8567249178886414,\n",
       "  0.898760199546814,\n",
       "  0.8727956414222717,\n",
       "  0.83203125,\n",
       "  0.8213977217674255,\n",
       "  0.9456576108932495,\n",
       "  0.9213012456893921,\n",
       "  0.8598386645317078,\n",
       "  0.8877441883087158,\n",
       "  0.8881223201751709,\n",
       "  0.9006547927856445,\n",
       "  0.8629677891731262,\n",
       "  0.8562662601470947,\n",
       "  0.8648645281791687,\n",
       "  0.8699088096618652,\n",
       "  0.8859865665435791,\n",
       "  0.8654391765594482,\n",
       "  0.8618495464324951,\n",
       "  0.8777202367782593,\n",
       "  0.872901976108551,\n",
       "  0.9379324913024902,\n",
       "  0.9075449109077454,\n",
       "  0.9202808141708374,\n",
       "  0.8911689519882202,\n",
       "  0.8497427701950073,\n",
       "  0.8654854893684387,\n",
       "  0.87838214635849,\n",
       "  0.8692009449005127,\n",
       "  0.8784605264663696,\n",
       "  0.8811250925064087,\n",
       "  0.8760857582092285,\n",
       "  0.8185369372367859,\n",
       "  0.8410348892211914,\n",
       "  0.8459373712539673,\n",
       "  0.8633629083633423,\n",
       "  0.8560214638710022,\n",
       "  0.9000176191329956,\n",
       "  0.8473932147026062,\n",
       "  0.8116623759269714,\n",
       "  0.850970983505249,\n",
       "  0.852897047996521,\n",
       "  0.8775964379310608,\n",
       "  0.8647081255912781,\n",
       "  0.8640090227127075,\n",
       "  0.8659546971321106,\n",
       "  0.8595301508903503,\n",
       "  0.8620272874832153,\n",
       "  0.9210545420646667,\n",
       "  0.8960097432136536,\n",
       "  0.920051097869873,\n",
       "  0.8457551002502441,\n",
       "  0.8948221802711487,\n",
       "  0.8727144002914429,\n",
       "  0.8649100661277771,\n",
       "  0.8705238103866577,\n",
       "  0.8672940731048584,\n",
       "  0.8623538017272949,\n",
       "  0.855638861656189,\n",
       "  0.850277841091156,\n",
       "  0.8429018259048462,\n",
       "  0.8967869281768799,\n",
       "  0.8758362531661987,\n",
       "  0.8406236171722412,\n",
       "  0.8770698308944702,\n",
       "  0.8552159070968628,\n",
       "  0.8654663562774658,\n",
       "  0.8484839200973511,\n",
       "  0.8804026246070862,\n",
       "  0.8527985215187073,\n",
       "  0.87446129322052,\n",
       "  0.8200598955154419,\n",
       "  0.8619400262832642,\n",
       "  0.8736063838005066,\n",
       "  0.9200584888458252,\n",
       "  0.8582789897918701,\n",
       "  0.8679406642913818,\n",
       "  0.8716112375259399,\n",
       "  0.8715375661849976,\n",
       "  0.8275284767150879,\n",
       "  0.9051198959350586,\n",
       "  0.9036718010902405,\n",
       "  0.8521945476531982,\n",
       "  0.8506699800491333,\n",
       "  0.8520210981369019,\n",
       "  0.8102983832359314,\n",
       "  0.8893179893493652,\n",
       "  0.8640010952949524,\n",
       "  0.8387018442153931,\n",
       "  0.8595088720321655,\n",
       "  0.8849946856498718,\n",
       "  0.8356561660766602,\n",
       "  0.8495144844055176,\n",
       "  0.8159881830215454,\n",
       "  0.8635067939758301,\n",
       "  0.8729479312896729,\n",
       "  0.8009923696517944,\n",
       "  0.8648256063461304,\n",
       "  0.8388048410415649,\n",
       "  0.8959289789199829,\n",
       "  0.8193306922912598,\n",
       "  0.8366000652313232,\n",
       "  0.8762342929840088,\n",
       "  0.8646475672721863,\n",
       "  0.8976554870605469,\n",
       "  0.8640477657318115,\n",
       "  0.8583077788352966,\n",
       "  0.8019059896469116,\n",
       "  0.8904242515563965,\n",
       "  0.8221485614776611,\n",
       "  0.8527038097381592,\n",
       "  0.8636596202850342,\n",
       "  0.8702820539474487,\n",
       "  0.8607394695281982,\n",
       "  0.8971463441848755,\n",
       "  0.8267984986305237,\n",
       "  0.8838993906974792,\n",
       "  0.8555269241333008,\n",
       "  0.8475754261016846,\n",
       "  0.8806074857711792,\n",
       "  0.833570659160614,\n",
       "  0.8623273968696594,\n",
       "  0.8308958411216736,\n",
       "  0.8574000597000122,\n",
       "  0.8482698202133179,\n",
       "  0.8236221075057983,\n",
       "  0.8838744163513184,\n",
       "  0.8678289651870728,\n",
       "  0.8581720590591431,\n",
       "  0.8674119710922241,\n",
       "  0.8824385404586792,\n",
       "  0.828390896320343,\n",
       "  0.8720844984054565,\n",
       "  0.8045555353164673,\n",
       "  0.8412458896636963,\n",
       "  0.8264055848121643,\n",
       "  0.8331442475318909,\n",
       "  0.8603498339653015,\n",
       "  0.8496406674385071,\n",
       "  0.8057727813720703,\n",
       "  0.8491461873054504,\n",
       "  0.8142300844192505,\n",
       "  0.8731689453125,\n",
       "  0.8573654294013977,\n",
       "  0.8363475203514099,\n",
       "  0.8344777822494507,\n",
       "  0.8280805945396423,\n",
       "  0.8372530937194824,\n",
       "  0.8537216186523438,\n",
       "  0.8488720655441284,\n",
       "  0.8728709816932678,\n",
       "  0.8638551235198975,\n",
       "  0.843701958656311,\n",
       "  0.8540171384811401,\n",
       "  0.8635565042495728,\n",
       "  0.882539689540863,\n",
       "  0.844465434551239,\n",
       "  0.8340708613395691,\n",
       "  0.8551398515701294,\n",
       "  0.8298815488815308,\n",
       "  0.788396418094635,\n",
       "  0.8322288393974304,\n",
       "  0.8399602174758911,\n",
       "  0.8218880295753479,\n",
       "  0.8615235090255737,\n",
       "  0.8787453174591064,\n",
       "  0.8531285524368286,\n",
       "  0.8756039142608643,\n",
       "  0.8440699577331543,\n",
       "  0.8255810737609863,\n",
       "  0.8072389960289001,\n",
       "  0.7914925813674927,\n",
       "  0.8381528258323669,\n",
       "  0.8088747262954712,\n",
       "  0.8501173853874207,\n",
       "  0.8212643265724182,\n",
       "  0.8488163948059082,\n",
       "  0.8387010097503662,\n",
       "  0.8353971242904663,\n",
       "  0.8252673149108887,\n",
       "  0.8292772769927979,\n",
       "  0.8478261828422546,\n",
       "  0.825020432472229,\n",
       "  0.7888984084129333,\n",
       "  0.8161131143569946,\n",
       "  0.8233099579811096,\n",
       "  0.8337433338165283,\n",
       "  0.8251733183860779,\n",
       "  0.8275385499000549,\n",
       "  0.849868655204773,\n",
       "  0.8497419953346252,\n",
       "  0.801093339920044,\n",
       "  0.8373794555664062,\n",
       "  0.8388291001319885,\n",
       "  0.8366628885269165,\n",
       "  0.8451171517372131,\n",
       "  0.8404824733734131,\n",
       "  0.8597961664199829,\n",
       "  0.8793203830718994,\n",
       "  0.8362723588943481,\n",
       "  0.8220049142837524,\n",
       "  0.8382331132888794,\n",
       "  0.8485560417175293,\n",
       "  0.8255915641784668,\n",
       "  0.8236613273620605,\n",
       "  0.8221365213394165,\n",
       "  0.8258549571037292,\n",
       "  0.8272156119346619,\n",
       "  0.8129913806915283,\n",
       "  0.8260533809661865,\n",
       "  0.848725438117981,\n",
       "  0.8347393274307251,\n",
       "  0.8570900559425354,\n",
       "  0.7923809885978699,\n",
       "  0.854214072227478,\n",
       "  0.7882794737815857,\n",
       "  0.8132503628730774,\n",
       "  0.8293024897575378,\n",
       "  0.8383257389068604,\n",
       "  0.8298616409301758,\n",
       "  0.8197784423828125,\n",
       "  0.872378945350647,\n",
       "  0.8325714468955994,\n",
       "  0.8511086702346802,\n",
       "  0.7937226295471191,\n",
       "  0.8288419246673584,\n",
       "  0.7965582013130188],\n",
       " [2.289383888244629,\n",
       "  2.2824370861053467,\n",
       "  2.275965452194214,\n",
       "  2.2696800231933594,\n",
       "  2.2633843421936035,\n",
       "  2.2569069862365723,\n",
       "  2.250293731689453,\n",
       "  2.2434020042419434,\n",
       "  2.235983371734619,\n",
       "  2.227855682373047,\n",
       "  2.2188591957092285,\n",
       "  2.2088780403137207,\n",
       "  2.197780132293701,\n",
       "  2.185458183288574,\n",
       "  2.171799659729004,\n",
       "  2.156590223312378,\n",
       "  2.139698028564453,\n",
       "  2.1210250854492188,\n",
       "  2.1004648208618164,\n",
       "  2.0778942108154297,\n",
       "  2.053225040435791,\n",
       "  2.026376724243164,\n",
       "  1.9973020553588867,\n",
       "  1.9658479690551758,\n",
       "  1.931968092918396,\n",
       "  1.8956222534179688,\n",
       "  1.8569109439849854,\n",
       "  1.8159563541412354,\n",
       "  1.7730488777160645,\n",
       "  1.7287653684616089,\n",
       "  1.6832876205444336,\n",
       "  1.6373813152313232,\n",
       "  1.5924160480499268,\n",
       "  1.5493428707122803,\n",
       "  1.5091875791549683,\n",
       "  1.473254680633545,\n",
       "  1.4431073665618896,\n",
       "  1.420497179031372,\n",
       "  1.406702995300293,\n",
       "  1.4002854824066162,\n",
       "  1.3994807004928589,\n",
       "  1.398707389831543,\n",
       "  1.393717885017395,\n",
       "  1.385027289390564,\n",
       "  1.3722903728485107,\n",
       "  1.356299638748169,\n",
       "  1.3394957780838013,\n",
       "  1.3259851932525635,\n",
       "  1.3156499862670898,\n",
       "  1.307179570198059,\n",
       "  1.303268551826477,\n",
       "  1.3019856214523315,\n",
       "  1.301094889640808,\n",
       "  1.2983756065368652,\n",
       "  1.2961324453353882,\n",
       "  1.29246187210083,\n",
       "  1.2882847785949707,\n",
       "  1.2828600406646729,\n",
       "  1.274444580078125,\n",
       "  1.265015959739685,\n",
       "  1.2560166120529175,\n",
       "  1.2515021562576294,\n",
       "  1.249138355255127,\n",
       "  1.248105525970459,\n",
       "  1.2483088970184326,\n",
       "  1.2473739385604858,\n",
       "  1.2411394119262695,\n",
       "  1.2348414659500122,\n",
       "  1.230455994606018,\n",
       "  1.226759672164917,\n",
       "  1.2247809171676636,\n",
       "  1.2254213094711304,\n",
       "  1.2248061895370483,\n",
       "  1.2202707529067993,\n",
       "  1.215447187423706,\n",
       "  1.21115243434906,\n",
       "  1.2082781791687012,\n",
       "  1.2075594663619995,\n",
       "  1.2063010931015015,\n",
       "  1.1998580694198608,\n",
       "  1.1966116428375244,\n",
       "  1.1962004899978638,\n",
       "  1.1965961456298828,\n",
       "  1.1985089778900146,\n",
       "  1.193789005279541,\n",
       "  1.19069242477417,\n",
       "  1.1858251094818115,\n",
       "  1.182611107826233,\n",
       "  1.180969476699829,\n",
       "  1.1778643131256104,\n",
       "  1.1767849922180176,\n",
       "  1.1761822700500488,\n",
       "  1.173086166381836,\n",
       "  1.1706397533416748,\n",
       "  1.1699644327163696,\n",
       "  1.16851007938385,\n",
       "  1.1707427501678467,\n",
       "  1.166711688041687,\n",
       "  1.1588224172592163,\n",
       "  1.1581569910049438,\n",
       "  1.1522541046142578,\n",
       "  1.1563791036605835,\n",
       "  1.1675447225570679,\n",
       "  1.1514747142791748,\n",
       "  1.1475316286087036,\n",
       "  1.1485131978988647,\n",
       "  1.1428345441818237,\n",
       "  1.1504542827606201,\n",
       "  1.1613062620162964,\n",
       "  1.1496548652648926,\n",
       "  1.1352417469024658,\n",
       "  1.1403088569641113,\n",
       "  1.135196566581726,\n",
       "  1.131911039352417,\n",
       "  1.1453512907028198,\n",
       "  1.1486361026763916,\n",
       "  1.1319092512130737,\n",
       "  1.1264454126358032,\n",
       "  1.1238181591033936,\n",
       "  1.121968150138855,\n",
       "  1.1244322061538696,\n",
       "  1.1269376277923584,\n",
       "  1.12189519405365,\n",
       "  1.1172270774841309,\n",
       "  1.1187001466751099,\n",
       "  1.1168879270553589,\n",
       "  1.1127523183822632,\n",
       "  1.1145392656326294,\n",
       "  1.1135807037353516,\n",
       "  1.1105551719665527,\n",
       "  1.1080322265625,\n",
       "  1.1068971157073975,\n",
       "  1.1064344644546509,\n",
       "  1.1064355373382568,\n",
       "  1.1067153215408325,\n",
       "  1.1047439575195312,\n",
       "  1.1017669439315796,\n",
       "  1.101434588432312,\n",
       "  1.1006684303283691,\n",
       "  1.1007033586502075,\n",
       "  1.1049824953079224,\n",
       "  1.101799726486206,\n",
       "  1.096623182296753,\n",
       "  1.0963075160980225,\n",
       "  1.0947195291519165,\n",
       "  1.0936779975891113,\n",
       "  1.0987588167190552,\n",
       "  1.102051854133606,\n",
       "  1.094695806503296,\n",
       "  1.0913143157958984,\n",
       "  1.0913550853729248,\n",
       "  1.0881662368774414,\n",
       "  1.0921801328659058,\n",
       "  1.0993616580963135,\n",
       "  1.0969855785369873,\n",
       "  1.0884158611297607,\n",
       "  1.0888270139694214,\n",
       "  1.08841073513031,\n",
       "  1.0844242572784424,\n",
       "  1.0873428583145142,\n",
       "  1.0903955698013306,\n",
       "  1.0884816646575928,\n",
       "  1.0844485759735107,\n",
       "  1.0840696096420288,\n",
       "  1.0855029821395874,\n",
       "  1.0845706462860107,\n",
       "  1.0831576585769653,\n",
       "  1.0835282802581787,\n",
       "  1.08505380153656,\n",
       "  1.084018588066101,\n",
       "  1.0797197818756104,\n",
       "  1.079857587814331,\n",
       "  1.0793602466583252,\n",
       "  1.0794901847839355,\n",
       "  1.086580753326416,\n",
       "  1.0947033166885376,\n",
       "  1.0834259986877441,\n",
       "  1.0763139724731445,\n",
       "  1.0778872966766357,\n",
       "  1.075693964958191,\n",
       "  1.0795741081237793,\n",
       "  1.0821855068206787,\n",
       "  1.0768520832061768,\n",
       "  1.0725592374801636,\n",
       "  1.0718142986297607,\n",
       "  1.0715059041976929,\n",
       "  1.0716108083724976,\n",
       "  1.0761034488677979,\n",
       "  1.0742738246917725,\n",
       "  1.070487380027771,\n",
       "  1.0708223581314087,\n",
       "  1.0693095922470093,\n",
       "  1.0709097385406494,\n",
       "  1.079516053199768,\n",
       "  1.0733267068862915,\n",
       "  1.0668118000030518,\n",
       "  1.0685126781463623,\n",
       "  1.0643224716186523,\n",
       "  1.0723850727081299,\n",
       "  1.082830548286438,\n",
       "  1.0704751014709473,\n",
       "  1.0635141134262085,\n",
       "  1.0663292407989502,\n",
       "  1.0624284744262695,\n",
       "  1.0617668628692627,\n",
       "  1.068919062614441,\n",
       "  1.0712538957595825,\n",
       "  1.0623672008514404,\n",
       "  1.0607832670211792,\n",
       "  1.0615239143371582,\n",
       "  1.0605666637420654,\n",
       "  1.0587165355682373,\n",
       "  1.0687003135681152,\n",
       "  1.0624393224716187,\n",
       "  1.0548475980758667,\n",
       "  1.0581068992614746,\n",
       "  1.0548921823501587,\n",
       "  1.0541369915008545,\n",
       "  1.064943552017212,\n",
       "  1.0691417455673218,\n",
       "  1.0542867183685303,\n",
       "  1.05472993850708,\n",
       "  1.0577448606491089,\n",
       "  1.0514408349990845,\n",
       "  1.0498288869857788,\n",
       "  1.0551514625549316,\n",
       "  1.0573196411132812,\n",
       "  1.0508686304092407,\n",
       "  1.0483733415603638,\n",
       "  1.0489767789840698,\n",
       "  1.0483336448669434,\n",
       "  1.0472406148910522,\n",
       "  1.0471432209014893,\n",
       "  1.0489438772201538,\n",
       "  1.0497617721557617,\n",
       "  1.0497804880142212,\n",
       "  1.0499439239501953,\n",
       "  1.046888828277588,\n",
       "  1.0449403524398804,\n",
       "  1.046369194984436,\n",
       "  1.0441060066223145,\n",
       "  1.0494730472564697,\n",
       "  1.0504018068313599,\n",
       "  1.0383540391921997,\n",
       "  1.035752773284912,\n",
       "  1.034505009651184,\n",
       "  1.0323364734649658,\n",
       "  1.03300142288208,\n",
       "  1.038421392440796,\n",
       "  1.0371651649475098,\n",
       "  1.032546877861023,\n",
       "  1.0288984775543213,\n",
       "  1.0286312103271484,\n",
       "  1.0254451036453247,\n",
       "  1.0269914865493774,\n",
       "  1.0286885499954224,\n",
       "  1.025575041770935,\n",
       "  1.0219624042510986,\n",
       "  1.0209368467330933,\n",
       "  1.020592451095581,\n",
       "  1.0196622610092163,\n",
       "  1.022055745124817,\n",
       "  1.0226702690124512,\n",
       "  1.0198132991790771,\n",
       "  1.0188088417053223,\n",
       "  1.019974946975708,\n",
       "  1.0189557075500488,\n",
       "  1.0219521522521973,\n",
       "  1.020650029182434,\n",
       "  1.0142840147018433,\n",
       "  1.0138416290283203,\n",
       "  1.01904296875,\n",
       "  1.0268219709396362,\n",
       "  1.0264967679977417,\n",
       "  1.0174777507781982,\n",
       "  1.0153425931930542,\n",
       "  1.0143829584121704,\n",
       "  1.0129119157791138,\n",
       "  1.0135571956634521,\n",
       "  1.0141793489456177,\n",
       "  1.0074927806854248,\n",
       "  1.003989338874817,\n",
       "  1.0036879777908325,\n",
       "  1.0027965307235718,\n",
       "  1.0049136877059937,\n",
       "  1.006712794303894,\n",
       "  1.0070677995681763,\n",
       "  1.0035933256149292,\n",
       "  1.0021443367004395,\n",
       "  1.00469172000885,\n",
       "  1.004593849182129,\n",
       "  1.001265048980713,\n",
       "  1.0045435428619385,\n",
       "  1.004621982574463,\n",
       "  1.0029971599578857,\n",
       "  1.0051333904266357,\n",
       "  1.005253791809082,\n",
       "  0.9983853101730347,\n",
       "  0.9985136389732361,\n",
       "  1.0050462484359741,\n",
       "  0.999696671962738,\n",
       "  1.0047911405563354,\n",
       "  1.0044606924057007,\n",
       "  0.9917040467262268,\n",
       "  0.9957616329193115,\n",
       "  0.9956360459327698,\n",
       "  0.9955374598503113,\n",
       "  0.9956811666488647,\n",
       "  0.991793155670166,\n",
       "  0.98839271068573,\n",
       "  0.9908825159072876,\n",
       "  0.9943062663078308,\n",
       "  0.9925045967102051,\n",
       "  0.9910071492195129,\n",
       "  0.9951905608177185,\n",
       "  0.9938616752624512,\n",
       "  0.9883793592453003,\n",
       "  0.9896957278251648,\n",
       "  0.9902805089950562,\n",
       "  0.9951727986335754,\n",
       "  0.9928898811340332,\n",
       "  0.9853156805038452,\n",
       "  0.9854477047920227,\n",
       "  0.9869307279586792,\n",
       "  0.9839776754379272,\n",
       "  0.9855571985244751,\n",
       "  0.987675666809082,\n",
       "  0.9866472482681274,\n",
       "  0.9832048416137695,\n",
       "  0.9818305969238281,\n",
       "  0.9798653721809387,\n",
       "  0.9822116494178772,\n",
       "  0.9863364696502686,\n",
       "  0.9862001538276672,\n",
       "  0.9828594923019409,\n",
       "  0.9824377298355103,\n",
       "  0.9803641438484192,\n",
       "  0.9757234454154968,\n",
       "  0.9815361499786377,\n",
       "  0.9857507348060608,\n",
       "  0.9786154627799988,\n",
       "  0.9731578230857849,\n",
       "  0.9788399338722229,\n",
       "  0.9899691343307495,\n",
       "  0.9816842079162598,\n",
       "  0.9791897535324097,\n",
       "  0.9789231419563293,\n",
       "  0.9759835004806519,\n",
       "  0.9780029058456421,\n",
       "  0.9854832291603088,\n",
       "  0.9842862486839294,\n",
       "  0.9704416990280151,\n",
       "  0.9688133001327515,\n",
       "  0.9709855914115906,\n",
       "  0.9689687490463257,\n",
       "  0.9765868186950684,\n",
       "  0.9758553504943848,\n",
       "  0.9662811160087585,\n",
       "  0.9664686918258667,\n",
       "  0.9678497314453125,\n",
       "  0.9669886827468872,\n",
       "  0.9790269136428833,\n",
       "  0.974811315536499,\n",
       "  0.9669786095619202,\n",
       "  0.9633641839027405,\n",
       "  0.9621173143386841,\n",
       "  0.9621368050575256,\n",
       "  0.9668092131614685,\n",
       "  0.9680972099304199,\n",
       "  0.9673651456832886,\n",
       "  0.9638906717300415,\n",
       "  0.9636296033859253,\n",
       "  0.9660140872001648,\n",
       "  0.9634568691253662,\n",
       "  0.9632591009140015,\n",
       "  0.9650571346282959,\n",
       "  0.9659535884857178,\n",
       "  0.965216875076294,\n",
       "  0.9643892645835876,\n",
       "  0.9654480218887329,\n",
       "  0.9610286951065063,\n",
       "  0.9608756899833679,\n",
       "  0.9647984504699707,\n",
       "  0.9646323919296265,\n",
       "  0.9654977321624756,\n",
       "  0.9672446250915527,\n",
       "  0.961897611618042,\n",
       "  0.9554430246353149,\n",
       "  0.9636461138725281,\n",
       "  0.9602159857749939,\n",
       "  0.9630017876625061,\n",
       "  0.9708802700042725,\n",
       "  0.9604731798171997,\n",
       "  0.9493910074234009,\n",
       "  0.9529266953468323,\n",
       "  0.9538784027099609,\n",
       "  0.9523515701293945,\n",
       "  0.9618378281593323,\n",
       "  0.9620041847229004,\n",
       "  0.9508439898490906,\n",
       "  0.9471019506454468,\n",
       "  0.9521490335464478,\n",
       "  0.9533028602600098,\n",
       "  0.9535804390907288,\n",
       "  0.9591739773750305,\n",
       "  0.9622690081596375,\n",
       "  0.9546166062355042,\n",
       "  0.9529337882995605,\n",
       "  0.954375147819519,\n",
       "  0.958081841468811,\n",
       "  0.9678761959075928,\n",
       "  0.961360514163971,\n",
       "  0.9526991844177246,\n",
       "  0.9571086168289185,\n",
       "  0.9586240649223328,\n",
       "  0.9475259780883789,\n",
       "  0.9627422094345093,\n",
       "  0.962530255317688,\n",
       "  0.9442416429519653,\n",
       "  0.9437768459320068,\n",
       "  0.9457249641418457,\n",
       "  0.9454002976417542,\n",
       "  0.9550062417984009,\n",
       "  0.9597189426422119,\n",
       "  0.9501755237579346,\n",
       "  0.947385847568512,\n",
       "  0.9448320865631104,\n",
       "  0.9429327249526978,\n",
       "  0.944200873374939,\n",
       "  0.9429728388786316,\n",
       "  0.9399905204772949,\n",
       "  0.9381182789802551,\n",
       "  0.9379376173019409,\n",
       "  0.9388535618782043,\n",
       "  0.9427846074104309,\n",
       "  0.9389171004295349,\n",
       "  0.9380709528923035,\n",
       "  0.9404691457748413,\n",
       "  0.9415571689605713,\n",
       "  0.9440541863441467,\n",
       "  0.9443179368972778,\n",
       "  0.9474835991859436,\n",
       "  0.941265344619751,\n",
       "  0.9375413060188293,\n",
       "  0.9425888657569885,\n",
       "  0.9495663046836853,\n",
       "  0.9406508207321167,\n",
       "  0.9344932436943054,\n",
       "  0.9390764832496643,\n",
       "  0.9384490847587585,\n",
       "  0.9389387965202332,\n",
       "  0.9458100199699402,\n",
       "  0.944991946220398,\n",
       "  0.9394044280052185,\n",
       "  0.93506920337677,\n",
       "  0.9387187957763672,\n",
       "  0.9405650496482849,\n",
       "  0.934751033782959,\n",
       "  0.9372164607048035,\n",
       "  0.9361953735351562,\n",
       "  0.9337867498397827,\n",
       "  0.9360024929046631,\n",
       "  0.9360209703445435,\n",
       "  0.9335317611694336,\n",
       "  0.9340906143188477,\n",
       "  0.9361655116081238,\n",
       "  0.9394552707672119,\n",
       "  0.9390924572944641,\n",
       "  0.9372844099998474,\n",
       "  0.9423802495002747,\n",
       "  0.9436620473861694,\n",
       "  0.9364932775497437,\n",
       "  0.9380506873130798,\n",
       "  0.9409118294715881,\n",
       "  0.938888669013977,\n",
       "  0.9358859062194824,\n",
       "  0.9335647225379944,\n",
       "  0.9344240427017212,\n",
       "  0.9340274333953857,\n",
       "  0.9360201358795166,\n",
       "  0.9377987384796143,\n",
       "  0.931890070438385,\n",
       "  0.9311167001724243,\n",
       "  0.9295613169670105,\n",
       "  0.9283381700515747,\n",
       "  0.9293715357780457,\n",
       "  0.9306895732879639,\n",
       "  0.9286277294158936,\n",
       "  0.9276930689811707,\n",
       "  0.926895797252655,\n",
       "  0.9256581664085388,\n",
       "  0.9265174865722656,\n",
       "  0.93243408203125,\n",
       "  0.9297460317611694,\n",
       "  0.9249833226203918,\n",
       "  0.9243549108505249,\n",
       "  0.9257295727729797,\n",
       "  0.9241697788238525,\n",
       "  0.9237702488899231,\n",
       "  0.9281836152076721,\n",
       "  0.9331665635108948,\n",
       "  0.9321528673171997,\n",
       "  0.9283246994018555,\n",
       "  0.9264606833457947,\n",
       "  0.9272364974021912,\n",
       "  0.9265725612640381,\n",
       "  0.9349517822265625,\n",
       "  0.9372097253799438,\n",
       "  0.9260602593421936,\n",
       "  0.9225761890411377,\n",
       "  0.9209352731704712,\n",
       "  0.9227712154388428,\n",
       "  0.9309789538383484,\n",
       "  0.927327036857605,\n",
       "  0.9181153774261475,\n",
       "  0.9175917506217957,\n",
       "  0.9188921451568604,\n",
       "  0.9178000092506409,\n",
       "  0.9158201217651367,\n",
       "  0.915941596031189,\n",
       "  0.91758131980896,\n",
       "  0.9190912246704102,\n",
       "  0.9155529737472534,\n",
       "  0.9200973510742188,\n",
       "  0.9337159991264343,\n",
       "  0.9278420209884644,\n",
       "  0.921364426612854,\n",
       "  0.9263590574264526,\n",
       "  0.9236763119697571,\n",
       "  0.9211903810501099,\n",
       "  0.9255513548851013,\n",
       "  0.9193920493125916,\n",
       "  0.9122778177261353,\n",
       "  0.9136525988578796,\n",
       "  0.9176040887832642,\n",
       "  0.9246506690979004,\n",
       "  0.9258241653442383,\n",
       "  0.9119678735733032,\n",
       "  0.9150804281234741,\n",
       "  0.9111355543136597,\n",
       "  0.9106922149658203,\n",
       "  0.9204952120780945,\n",
       "  0.9171937704086304,\n",
       "  0.9104552268981934,\n",
       "  0.9129562377929688,\n",
       "  0.9105784296989441,\n",
       "  0.9076805114746094,\n",
       "  0.9103156328201294,\n",
       "  0.9114887118339539,\n",
       "  0.9088103771209717,\n",
       "  0.9065878987312317,\n",
       "  0.9107966423034668,\n",
       "  0.9101876020431519,\n",
       "  0.9090427756309509,\n",
       "  0.9097692370414734,\n",
       "  0.9089963436126709,\n",
       "  0.9070865511894226,\n",
       "  0.9070906639099121,\n",
       "  0.911091148853302,\n",
       "  0.9101142883300781,\n",
       "  0.909165620803833,\n",
       "  0.9071310758590698,\n",
       "  0.9088100790977478,\n",
       "  0.9095973968505859,\n",
       "  0.9047361612319946,\n",
       "  0.9060627222061157,\n",
       "  0.90748530626297,\n",
       "  0.9046254754066467,\n",
       "  0.9039474129676819,\n",
       "  0.9031148552894592,\n",
       "  0.9036363363265991,\n",
       "  0.9038421511650085,\n",
       "  0.907399594783783,\n",
       "  0.9080243110656738,\n",
       "  0.9046655893325806,\n",
       "  0.9024576544761658,\n",
       "  0.9027407765388489,\n",
       "  0.8991518020629883,\n",
       "  0.9024211168289185,\n",
       "  0.918143630027771,\n",
       "  0.910723865032196,\n",
       "  0.902559757232666,\n",
       "  0.9050905704498291,\n",
       "  0.9030202627182007,\n",
       "  0.8975046873092651,\n",
       "  0.8977938294410706,\n",
       "  0.902694582939148,\n",
       "  0.9048506617546082,\n",
       "  0.9037332534790039,\n",
       "  0.9026455879211426,\n",
       "  0.9018006324768066,\n",
       "  0.9006935954093933,\n",
       "  0.9011974334716797,\n",
       "  0.902006983757019,\n",
       "  0.9033693075180054,\n",
       "  0.9038000106811523,\n",
       "  0.901813268661499,\n",
       "  0.9011966586112976,\n",
       "  0.8977922201156616,\n",
       "  0.898532509803772,\n",
       "  0.9027202725410461,\n",
       "  0.9016863107681274,\n",
       "  0.9006808996200562,\n",
       "  0.8990556001663208,\n",
       "  0.8993411660194397,\n",
       "  0.9033108949661255,\n",
       "  0.9103593230247498,\n",
       "  0.9028179049491882,\n",
       "  0.8969542980194092,\n",
       "  0.8959063291549683,\n",
       "  0.893352746963501,\n",
       "  0.8971579670906067,\n",
       "  0.9028526544570923,\n",
       "  0.898829996585846,\n",
       "  0.895534873008728,\n",
       "  0.8964186310768127,\n",
       "  0.8959112167358398,\n",
       "  0.8983235359191895,\n",
       "  0.9039103388786316,\n",
       "  0.9055927991867065,\n",
       "  0.8956747651100159,\n",
       "  0.8999296426773071,\n",
       "  0.900310218334198,\n",
       "  0.8954446911811829,\n",
       "  0.9012036323547363,\n",
       "  0.8995450139045715,\n",
       "  0.8956254720687866,\n",
       "  0.8960752487182617,\n",
       "  0.8930739164352417,\n",
       "  0.8953803777694702,\n",
       "  0.9029658436775208,\n",
       "  0.895794689655304,\n",
       "  0.8948049545288086,\n",
       "  0.8992924094200134,\n",
       "  0.9035335183143616,\n",
       "  0.904790997505188,\n",
       "  0.9064699411392212,\n",
       "  0.8991072177886963,\n",
       "  0.8939700722694397,\n",
       "  0.8991334438323975,\n",
       "  0.8976787328720093,\n",
       "  0.8914282321929932,\n",
       "  0.8978985548019409,\n",
       "  0.9045547842979431,\n",
       "  0.9028092622756958,\n",
       "  0.8936237096786499,\n",
       "  0.8940584659576416,\n",
       "  0.8932347893714905,\n",
       "  0.891475260257721,\n",
       "  0.8929430246353149,\n",
       "  0.8969441652297974,\n",
       "  0.8938196897506714,\n",
       "  0.8901957273483276,\n",
       "  0.8954224586486816,\n",
       "  0.8982438445091248,\n",
       "  0.8957639932632446,\n",
       "  0.8948416709899902,\n",
       "  0.9007048606872559,\n",
       "  0.9010368585586548,\n",
       "  0.8971818685531616,\n",
       "  0.8937174677848816,\n",
       "  0.8930360078811646,\n",
       "  0.8914506435394287,\n",
       "  0.8908515572547913,\n",
       "  0.8902453184127808,\n",
       "  0.8928855657577515,\n",
       "  0.8954740166664124,\n",
       "  0.8945661187171936,\n",
       "  0.8940905928611755,\n",
       "  0.8979789018630981,\n",
       "  0.8950930237770081,\n",
       "  0.8884037733078003,\n",
       "  0.8865026235580444,\n",
       "  0.8872177600860596,\n",
       "  0.8912370800971985,\n",
       "  0.896898627281189,\n",
       "  0.8915932178497314,\n",
       "  0.8853846192359924,\n",
       "  0.8873441219329834,\n",
       "  0.8887051343917847,\n",
       "  0.8889147043228149,\n",
       "  0.9009754657745361,\n",
       "  0.9015226364135742,\n",
       "  0.8925116658210754,\n",
       "  0.8875631093978882,\n",
       "  0.8867361545562744,\n",
       "  0.8849464058876038,\n",
       "  0.8891171216964722,\n",
       "  0.8897755742073059,\n",
       "  0.8865152597427368,\n",
       "  0.8843239545822144,\n",
       "  0.8859902024269104,\n",
       "  0.8841113448143005,\n",
       "  0.8854348659515381,\n",
       "  0.8920923471450806,\n",
       "  0.896740198135376,\n",
       "  0.8908874988555908,\n",
       "  0.890554666519165,\n",
       "  0.894431471824646,\n",
       "  0.8957394361495972,\n",
       "  0.8906950950622559,\n",
       "  0.8841211199760437,\n",
       "  0.8827723860740662,\n",
       "  0.8819041848182678,\n",
       "  0.8809598088264465,\n",
       "  0.8884861469268799,\n",
       "  0.8910527229309082,\n",
       "  0.8850879073143005,\n",
       "  0.883743941783905,\n",
       "  0.8832730650901794,\n",
       "  0.8756001591682434,\n",
       "  0.8788986802101135,\n",
       "  0.8908579349517822,\n",
       "  0.886372983455658,\n",
       "  0.8808096647262573,\n",
       "  0.880541980266571,\n",
       "  0.8818646669387817,\n",
       "  0.8841331005096436,\n",
       "  0.8850207924842834,\n",
       "  0.8936591744422913,\n",
       "  0.8925082683563232,\n",
       "  0.8753279447555542,\n",
       "  0.8748332858085632,\n",
       "  0.8784435391426086,\n",
       "  0.8810001611709595,\n",
       "  0.8838311433792114,\n",
       "  0.884675145149231,\n",
       "  0.8778160810470581,\n",
       "  0.877916693687439,\n",
       "  0.8785278797149658,\n",
       "  0.8784686326980591,\n",
       "  0.8908945322036743,\n",
       "  0.8879432082176208,\n",
       "  0.8759665489196777,\n",
       "  0.8712103366851807,\n",
       "  0.8762857913970947,\n",
       "  0.8730044364929199,\n",
       "  0.8715894818305969,\n",
       "  0.8755441308021545,\n",
       "  0.8813639879226685,\n",
       "  0.8888407349586487,\n",
       "  0.8844256401062012,\n",
       "  0.8770375847816467,\n",
       "  0.8729798793792725,\n",
       "  0.8711497187614441,\n",
       "  0.8698224425315857,\n",
       "  0.8758178353309631,\n",
       "  0.8758758306503296,\n",
       "  0.8708482980728149,\n",
       "  0.8670825362205505,\n",
       "  0.8695768117904663,\n",
       "  0.8748014569282532,\n",
       "  0.8808309435844421,\n",
       "  0.8770620226860046,\n",
       "  0.875697672367096,\n",
       "  0.8748800754547119,\n",
       "  0.8727483749389648,\n",
       "  0.8841187357902527,\n",
       "  0.8799309730529785,\n",
       "  0.8674347400665283,\n",
       "  0.867604672908783,\n",
       "  0.868521511554718,\n",
       "  0.8728065490722656,\n",
       "  0.8758963346481323,\n",
       "  0.8704813718795776,\n",
       "  0.8662242293357849,\n",
       "  0.8612139225006104,\n",
       "  0.8610985279083252,\n",
       "  0.866320013999939,\n",
       "  0.878582239151001,\n",
       "  0.87547767162323,\n",
       "  0.8649609684944153,\n",
       "  0.8637067675590515,\n",
       "  0.8604215979576111,\n",
       "  0.8618803024291992,\n",
       "  0.8703529238700867,\n",
       "  0.874448299407959,\n",
       "  0.8718159198760986,\n",
       "  0.8654802441596985,\n",
       "  0.8599711656570435,\n",
       "  0.8587989807128906,\n",
       "  0.8611215949058533,\n",
       "  0.8698538541793823,\n",
       "  0.8680051565170288,\n",
       "  0.8679263591766357,\n",
       "  0.8669630289077759,\n",
       "  0.8613294959068298,\n",
       "  0.8578206896781921,\n",
       "  0.8567809462547302,\n",
       "  0.8544668555259705,\n",
       "  0.856870174407959,\n",
       "  0.85849529504776,\n",
       "  0.8589937090873718,\n",
       "  0.8608303666114807,\n",
       "  0.860202431678772,\n",
       "  0.8574178814888,\n",
       "  0.8653367161750793,\n",
       "  0.8664606809616089,\n",
       "  0.8541212677955627,\n",
       "  0.8560439348220825,\n",
       "  0.8563688397407532,\n",
       "  0.8530703783035278,\n",
       "  0.8574000000953674,\n",
       "  0.8593339920043945,\n",
       "  0.8512471914291382,\n",
       "  0.8467797636985779,\n",
       "  0.8478987812995911,\n",
       "  0.8476144075393677,\n",
       "  0.8484529256820679,\n",
       "  0.8550596833229065,\n",
       "  0.8544499278068542,\n",
       "  0.8487612009048462,\n",
       "  0.848278284072876,\n",
       "  0.8504343032836914,\n",
       "  0.8485798835754395,\n",
       "  0.8464837670326233,\n",
       "  0.8541367053985596,\n",
       "  0.861066997051239,\n",
       "  0.8566604256629944,\n",
       "  0.8494489192962646,\n",
       "  0.8552147746086121,\n",
       "  0.8505228161811829,\n",
       "  0.8502227067947388,\n",
       "  0.8620136380195618,\n",
       "  0.8647851943969727,\n",
       "  0.8508979082107544,\n",
       "  0.8477122783660889,\n",
       "  0.8470118641853333,\n",
       "  0.844751238822937,\n",
       "  0.850561261177063,\n",
       "  0.8525424003601074,\n",
       "  0.8482354879379272,\n",
       "  0.8481541872024536,\n",
       "  0.8475613594055176,\n",
       "  0.8446775674819946,\n",
       "  0.8504712581634521,\n",
       "  0.8555840253829956,\n",
       "  0.854704737663269,\n",
       "  0.8473832607269287,\n",
       "  0.8490412831306458,\n",
       "  0.8475637435913086,\n",
       "  0.8395847082138062,\n",
       "  0.8475894331932068,\n",
       "  0.8591703176498413,\n",
       "  0.8505367636680603,\n",
       "  0.8410286903381348,\n",
       "  0.8419097065925598,\n",
       "  0.8396055102348328,\n",
       "  0.8369802236557007,\n",
       "  0.8414944410324097,\n",
       "  0.8514041304588318,\n",
       "  0.8472566604614258,\n",
       "  0.8436771035194397,\n",
       "  0.8474472165107727,\n",
       "  0.8444769978523254,\n",
       "  0.8388556241989136,\n",
       "  0.8540753126144409,\n",
       "  0.8515122532844543,\n",
       "  0.8339481949806213,\n",
       "  0.8351823687553406,\n",
       "  0.8401069641113281,\n",
       "  0.8398679494857788,\n",
       "  0.8458524942398071,\n",
       "  0.8438988924026489,\n",
       "  0.8363791704177856,\n",
       "  0.8327938318252563,\n",
       "  0.8299320340156555,\n",
       "  0.8302233219146729,\n",
       "  0.8336751461029053,\n",
       "  0.8367670774459839,\n",
       "  0.8346364498138428,\n",
       "  0.8348876237869263,\n",
       "  0.8326446413993835,\n",
       "  0.8288710117340088,\n",
       "  0.829186201095581,\n",
       "  0.8327867984771729,\n",
       "  0.8344986438751221,\n",
       "  0.8305140137672424,\n",
       "  0.8326079845428467,\n",
       "  0.8324971795082092,\n",
       "  0.8309618234634399,\n",
       "  0.836882472038269,\n",
       "  0.8364284038543701,\n",
       "  0.8266592025756836,\n",
       "  0.8293023705482483,\n",
       "  0.8280366659164429,\n",
       "  0.8267430067062378,\n",
       "  0.828509509563446,\n",
       "  0.8327049016952515,\n",
       "  0.8317717909812927,\n",
       "  0.832306444644928,\n",
       "  0.8307054042816162,\n",
       "  0.8288640975952148,\n",
       "  0.8273150324821472,\n",
       "  0.8261539936065674,\n",
       "  0.8276416063308716,\n",
       "  0.8290037512779236,\n",
       "  0.8322939872741699,\n",
       "  0.8314511775970459,\n",
       "  0.8308314085006714,\n",
       "  0.8282290697097778,\n",
       "  0.8250201940536499,\n",
       "  0.8318020105361938,\n",
       "  0.8415148854255676,\n",
       "  0.8368625044822693,\n",
       "  0.8304647207260132,\n",
       "  0.8310936689376831,\n",
       "  0.8273046612739563,\n",
       "  0.8269585967063904,\n",
       "  0.8309642672538757,\n",
       "  0.8299392461776733,\n",
       "  0.8301676511764526,\n",
       "  0.8333988189697266,\n",
       "  0.8281755447387695,\n",
       "  0.8234890103340149,\n",
       "  0.8293261528015137,\n",
       "  0.8302308320999146,\n",
       "  0.8258762359619141,\n",
       "  0.8323726058006287,\n",
       "  0.8455869555473328,\n",
       "  0.8388922214508057,\n",
       "  0.8270533084869385,\n",
       "  0.8285762071609497,\n",
       "  0.8296664953231812,\n",
       "  0.825594425201416,\n",
       "  0.8334311842918396,\n",
       "  0.8353568315505981,\n",
       "  0.8288921117782593,\n",
       "  0.8261961936950684,\n",
       "  0.8262777328491211,\n",
       "  0.8266890645027161,\n",
       "  0.8266774415969849,\n",
       "  0.8265256881713867,\n",
       "  0.8286072015762329,\n",
       "  0.8305707573890686,\n",
       "  0.8256964683532715,\n",
       "  0.8221547603607178,\n",
       "  0.8212572336196899,\n",
       "  0.8228946328163147,\n",
       "  0.8218762874603271,\n",
       "  0.8209328651428223,\n",
       "  0.8231457471847534,\n",
       "  0.8249273300170898,\n",
       "  0.824174165725708,\n",
       "  0.8232585191726685,\n",
       "  0.8234690427780151,\n",
       "  0.8218563199043274,\n",
       "  0.819657564163208,\n",
       "  0.8205016255378723,\n",
       "  0.8195316195487976,\n",
       "  0.8178489208221436,\n",
       "  0.8191428184509277,\n",
       "  0.8224847912788391,\n",
       "  0.8231490850448608,\n",
       "  0.8199610710144043,\n",
       "  0.8226109743118286,\n",
       "  0.8205656409263611,\n",
       "  0.8168624639511108,\n",
       "  0.8180617690086365,\n",
       "  0.8222432136535645,\n",
       "  0.8147200345993042,\n",
       "  0.8203246593475342,\n",
       "  0.8230112791061401,\n",
       "  0.8206140398979187,\n",
       "  0.8309434056282043,\n",
       "  0.8260028958320618,\n",
       "  0.8148239850997925,\n",
       "  0.8264498710632324,\n",
       "  0.8201260566711426,\n",
       "  0.8172471523284912,\n",
       "  0.8279020190238953,\n",
       "  0.8286064267158508,\n",
       "  0.8160195350646973,\n",
       "  0.8169054985046387,\n",
       "  0.8162237405776978,\n",
       "  0.8140498995780945,\n",
       "  0.8197311162948608,\n",
       "  0.8299366235733032,\n",
       "  0.8243730664253235,\n",
       "  0.8183774948120117,\n",
       "  0.8220334053039551,\n",
       "  0.8211308121681213,\n",
       "  0.8180627226829529,\n",
       "  0.8182108402252197,\n",
       "  0.8160171508789062,\n",
       "  0.818757176399231,\n",
       "  0.8173936009407043,\n",
       "  0.814242959022522,\n",
       "  0.8160337209701538,\n",
       "  0.8213338255882263,\n",
       "  0.8184217214584351,\n",
       "  0.8183784484863281,\n",
       "  0.8193126916885376,\n",
       "  0.8179839849472046,\n",
       "  0.8153930306434631,\n",
       "  0.8183512687683105,\n",
       "  0.814723014831543,\n",
       "  0.8103907108306885,\n",
       "  0.8124743103981018,\n",
       "  0.8098234534263611])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoder(**data_true, model=model, optimizer=optimizer, criterion=criterion, validation_data=val_data, num_epochs=1000, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot this\n",
    "def plot_encoded(data, model):\n",
    "    with torch.no_grad():\n",
    "        zhat, uhat = model.encoder(data['x'], data['y'])\n",
    "        plt.scatter(data['z'].numpy(), zhat.detach().numpy())\n",
    "        plt.scatter(data['u'].numpy(), uhat.detach().numpy(), color=\"red\")\n",
    "        plt.title(\"Encoded vs True values\")\n",
    "    plt.show()\n",
    "\n",
    "plot_encoded(val_data, model)\n",
    "plot_encoded(data_true, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "We train the parameters of the decoder and encoder separately: the encoder is trained on simulated data, and the decoder is trained on the sample, and centered in expectation by the simulated data. The training loop goes as follows:\n",
    "\n",
    "* Initialize encoder and decoder losses and optimizers\n",
    "* Create the mask of missing values, and initialize imputation\n",
    "\n",
    "* sample (marginally) from the model (still given the covariates `x`) (no grad)\n",
    "* update the encoder (grad)\n",
    "* encode both the sample and the decoder (no grad)\n",
    "* update the decoder (grad), taking into account the missing value mask\n",
    "* update the imputations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, output, target, mask):\n",
    "        return torch.mean(torch.pow(output -target, 2) * ~mask)\n",
    "\n",
    "class MaskedBCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, output, target, output_sim, target_sim, mask):\n",
    "        return torch.mean((output * target - output_sim * target_sim) * ~mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 9, 13])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total_loss: 1.71, decoder_loss: 1.08, encoder_loss: 0.62, best_loss = 1.08, phi:Parameter containing:\n",
      "tensor([0.4926], requires_grad=True)\n",
      "Epoch: 1, total_loss: 1.53, decoder_loss: 1.07, encoder_loss: 0.45, best_loss = 1.07, phi:Parameter containing:\n",
      "tensor([0.4937], requires_grad=True)\n",
      "Epoch: 2, total_loss: 1.53, decoder_loss: 1.16, encoder_loss: 0.37, best_loss = 1.07, phi:Parameter containing:\n",
      "tensor([0.4937], requires_grad=True)\n",
      "Epoch: 3, total_loss: 1.53, decoder_loss: 1.18, encoder_loss: 0.35, best_loss = 1.07, phi:Parameter containing:\n",
      "tensor([0.4937], requires_grad=True)\n",
      "Epoch: 4, total_loss: 1.40, decoder_loss: 1.05, encoder_loss: 0.35, best_loss = 1.05, phi:Parameter containing:\n",
      "tensor([0.4945], requires_grad=True)\n",
      "Epoch: 5, total_loss: 1.36, decoder_loss: 1.06, encoder_loss: 0.30, best_loss = 1.05, phi:Parameter containing:\n",
      "tensor([0.4945], requires_grad=True)\n",
      "Epoch: 6, total_loss: 1.43, decoder_loss: 1.09, encoder_loss: 0.34, best_loss = 1.05, phi:Parameter containing:\n",
      "tensor([0.4945], requires_grad=True)\n",
      "Epoch: 7, total_loss: 1.31, decoder_loss: 1.03, encoder_loss: 0.28, best_loss = 1.03, phi:Parameter containing:\n",
      "tensor([0.4921], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "criterion_encoder = nn.MSELoss()\n",
    "criterion_decoder = MaskedBCLoss()\n",
    "model_evaluation = MaskedMSELoss()\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=0.01)\n",
    "optimizer_decoder = torch.optim.Adam([\n",
    "    {'params':model.decoder.parameters()},\n",
    "    {'params':model.ar1.parameters()},\n",
    "    {'params':model.var_u}\n",
    "    ], lr=0.01)\n",
    "\n",
    "scheduler_encoder = StepLR(optimizer_encoder, step_size=100, gamma=1)\n",
    "scheduler_decoder = StepLR(optimizer_decoder, step_size=100, gamma=1)\n",
    "\n",
    "mask = torch.isnan(data_true['y'])\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "num_encoder_epochs = 200\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    # Sample some fake data\n",
    "    data_sim = model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])\n",
    "    data_sim_encoder = model.sample(data_true['y'].shape[0], data_true['y'].shape[1], x = data_true['x'])\n",
    "    # Train the encoder\n",
    "    _, train_loss, validation_loss = train_encoder(**data_sim_encoder, model=model, optimizer=optimizer_encoder, criterion=criterion_encoder, num_epochs=num_encoder_epochs, verbose=False)\n",
    "    # Autoencode\n",
    "    output_true = model.autoencode(data_true['x'], data_true['y'])\n",
    "    output_sim = model.autoencode(data_sim['x'], data_sim['y'])\n",
    "    loss = criterion_decoder(output_true, data_true['y'], output_sim, data_sim['y'], mask)\n",
    "    loss.backward()\n",
    "    optimizer_decoder.step()\n",
    "\n",
    "    scheduler_decoder.step()\n",
    "    scheduler_encoder.step()\n",
    "\n",
    "    # Clipping the gradients\n",
    "    # clip_value = .1  # this can be any value of your choosing\n",
    "    # clip_grad_norm_(model.decoder.parameters(), clip_value)\n",
    "\n",
    "    # store parameters\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval = model_evaluation(output_true, data_true['y'], mask).item()\n",
    "        total_loss = train_loss[-1] + eval\n",
    "        if eval < best_loss:\n",
    "            best_loss = eval\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    # Every 10 we restore the best weights\n",
    "    if epoch % 1 == 0:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f'Epoch: {epoch}, total_loss: {total_loss:.2f}, decoder_loss: {eval:.2f}, encoder_loss: {train_loss[-1]:.2f}, best_loss = {best_loss:.2f}, phi:{model.ar1.phi}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the GLLVM module on a simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_true['y'].detach().numpy(), model.autoencode(data_true['x'], data_true['y']).detach().numpy())\n",
    "plt.plot([0,5], [0,5], color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the encoder fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict z:\n",
    "z_hat, u_hat = model.encoder(data_true['x'], data_true['y'])\n",
    "plt.scatter(z_hat.detach().numpy(), data_true['z'].detach().numpy())\n",
    "plt.scatter(u_hat.detach().numpy(), data_true['u'].detach().numpy(), color=\"red\")\n",
    "plt.plot([-3, 3], [-3, 3], color=\"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the model parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param = model.decoder.parameters()\n",
    "param_true = model_true.decoder.parameters()\n",
    "\n",
    "for i, (p_true, p) in enumerate(zip(param_true, param)):\n",
    "    plt.scatter(p_true.detach().numpy(), p.detach().numpy(), color=f'C{i}', label=f'Group {i}')\n",
    "\n",
    "plt.xlabel('True Parameters')\n",
    "plt.ylabel('Estimated Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 12 randomly selected Z\n",
    "index = np.random.choice(range(n), 12, replace=False)\n",
    "fig, axs = plt.subplots(3, 4)\n",
    "\n",
    "zhat = model.encoder(data_true[\"x\"], data_true['y'])[0].detach()\n",
    "\n",
    "fig.suptitle(\"Ztrue vs Zest across time\")\n",
    "for i in range(12):\n",
    "    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n",
    "    axs[i//4, i%4].plot(data_true[\"z\"][index[i],:, 0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(z, id=0, grid=(1,)):\n",
    "    fig, axs = plt.subplots(*grid)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.plot(z[i,:,id])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_obs_v_mean(obs, mean, id=0, grid=(1,)):\n",
    "    fig, axs = plt.subplots(*grid)\n",
    "    axs = np.array(axs)\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.plot(obs[i,:,id])\n",
    "        ax.plot(mean[i,:,id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_id)\n",
    "plot_series(data_true['z'], grid=(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: refactor to have a module of two modules: encoder and decoder\n",
    "class GLLVM_longitudinal():\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # nuisance parameters\n",
    "        self.phi = torch.ones(1) * .5\n",
    "        self.var_u = torch.ones((1,1,p))\n",
    "        self.var_y = torch.ones((1,1,p))* .5\n",
    "        self.var_z = torch.ones((1,T,1))\n",
    "\n",
    "    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n",
    "        encoder_loss = nn.MSELoss()\n",
    "        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            (zhat, uhat) = self.encoder(x, y)\n",
    "            loss = encoder_loss(zhat, z) + encoder_loss(uhat, u)\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            encoder_opt.zero_grad()\n",
    "        return loss\n",
    "    \n",
    "    def update_nuisance_parameters(self, y, yhat, z, u, lr=1e-1):\n",
    "        # update phi:\n",
    "        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n",
    "        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n",
    "        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n",
    "        phi_sample = (torch.sum(z[:,1:]*z[:,:(T-1)], dim=1) / torch.sum(torch.pow(z[:,:(T-1)],2), dim=1)).mean()\n",
    "        var_u = torch.mean(torch.pow(u, 2), dim=0, keepdim=True)\n",
    "        var_y = torch.mean(torch.pow(y-yhat,2), dim=[1,2], keepdim=True)\n",
    "\n",
    "        self.phi = self.phi * (1-lr) + lr * phi_sample\n",
    "        self.var_u = self.var_u * (1-lr) + lr * var_u\n",
    "        self.var_y = self.var_y * (1-lr) + lr * var_y\n",
    "\n",
    "\n",
    "    def sample(self, n, x=None, z=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n",
    "                x = torch.randn((n, T, k))\n",
    "            if z is None:\n",
    "                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, T, q))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.AR(z, d)\n",
    "\n",
    "            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n",
    "            y = self.decoder(x, z, u) + eps\n",
    "\n",
    "        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n",
    "    \n",
    "    def AR(self, z, delta):\n",
    "        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n",
    "        for t in range(1, z.shape[1]):\n",
    "            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n",
    "        return z\n",
    "\n",
    "    def sample_z(self):\n",
    "        return None\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        # here i also need phi\n",
    "        self.wz = nn.Parameter(torch.randn((q, p)))\n",
    "        self.wx = nn.Parameter(torch.randn((T, k, p)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, T, p)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u):\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "        return linpar\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n",
    "        # output dimension is T*q  + p (for Z and U, respectively)\n",
    "        self.enc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=T*(p+k), out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = T*q + p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n",
    "        zu = self.enc_model(xy)\n",
    "        return self.split_zu(zu)\n",
    "\n",
    "    def split_zu(self, zu):\n",
    "        #output dimension of size (T*Z), p\n",
    "        z, u = torch.split(zu, [T*q, p], dim=1)\n",
    "        z = z.reshape((z.shape[0], T, q))\n",
    "        u = u.unsqueeze(1)\n",
    "        return (z, u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "n = 100\n",
    "p = 15\n",
    "T = 20\n",
    "k = 8\n",
    "q = 1\n",
    "\n",
    "DIMENSIONS_Y = (n, T, p) \n",
    "DIMENSIONS_X = (n, T, k)\n",
    "DIMENSIONS_Z = (n, T, q)\n",
    "DIMENSIONS_U = (n, 1, p)\n",
    "\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, size):\n",
    "        \"\"\"Return a tensor of shape (size) of indpeendent standard normal random variables.\"\"\"\n",
    "        return(torch.randn(size))\n",
    "\n",
    "class AR1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the phi parameter\n",
    "        self.phi = nn.Parameter(torch.ones(1) * .5)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        # 'd' is the input tensor containing the noise terms \n",
    "\n",
    "        # Initialize output list with the first noise term\n",
    "        z_list = [d[0]]\n",
    "\n",
    "        # Generate AR(1) sequence by iteratively adding the effect of\n",
    "        # previous term (scaled by phi) and the current noise term\n",
    "        for i in range(1, d.shape[0]):\n",
    "            z_list.append(z_list[i-1] * self.phi + d[i])\n",
    "\n",
    "        # Convert the list to a tensor\n",
    "        z = torch.stack(z_list)\n",
    "        \n",
    "        return z\n",
    "\n",
    "class GLLVM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the model parameters: all th intercepts, all the eveeything.\n",
    "\n",
    "\n",
    "# TODO: refactor to have a module of two modules: encoder and decoder\n",
    "class GLLVM_longitudinal():\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # nuisance parameters\n",
    "        self.phi = torch.ones(1) * .5\n",
    "        self.var_u = torch.ones((1,1,p))\n",
    "        self.var_y = torch.ones((1,1,p))* .5\n",
    "        self.var_z = torch.ones((1,T,1))\n",
    "\n",
    "    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n",
    "        encoder_loss = nn.MSELoss()\n",
    "        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            (zhat, uhat) = self.encoder(x, y)\n",
    "            loss = encoder_loss(zhat, z) + encoder_loss(uhat, u)\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n",
    "            loss.backward()\n",
    "            encoder_opt.step()\n",
    "            encoder_opt.zero_grad()\n",
    "        return loss\n",
    "    \n",
    "    def update_nuisance_parameters(self, y, yhat, z, u, lr=1e-1):\n",
    "        # update phi:\n",
    "        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n",
    "        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n",
    "        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n",
    "        phi_sample = (torch.sum(z[:,1:]*z[:,:(T-1)], dim=1) / torch.sum(torch.pow(z[:,:(T-1)],2), dim=1)).mean()\n",
    "        var_u = torch.mean(torch.pow(u, 2), dim=0, keepdim=True)\n",
    "        var_y = torch.mean(torch.pow(y-yhat,2), dim=[1,2], keepdim=True)\n",
    "\n",
    "        self.phi = self.phi * (1-lr) + lr * phi_sample\n",
    "        self.var_u = self.var_u * (1-lr) + lr * var_u\n",
    "        self.var_y = self.var_y * (1-lr) + lr * var_y\n",
    "\n",
    "\n",
    "    def sample(self, n, x=None, z=None, u=None, d=None):\n",
    "        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if x is None:\n",
    "                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n",
    "                x = torch.randn((n, T, k))\n",
    "            if z is None:\n",
    "                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n",
    "\n",
    "            if d is None:\n",
    "                d = torch.randn((n, T, q))\n",
    "        \n",
    "            if u is None:\n",
    "                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n",
    "        \n",
    "            z = self.AR(z, d)\n",
    "\n",
    "            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n",
    "            y = self.decoder(x, z, u) + eps\n",
    "\n",
    "        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n",
    "    \n",
    "    def AR(self, z, delta):\n",
    "        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n",
    "        for t in range(1, z.shape[1]):\n",
    "            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n",
    "        return z\n",
    "\n",
    "    def sample_z(self):\n",
    "        return None\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # decoder part (our parameters of interest)\n",
    "        # here i also need phi\n",
    "        self.wz = nn.Parameter(torch.randn((q, p)))\n",
    "        self.wx = nn.Parameter(torch.randn((T, k, p)))\n",
    "        self.bias = nn.Parameter(torch.zeros((1, T, p)))\n",
    "\n",
    "    # decoding\n",
    "    def forward(self, x, z, u):\n",
    "        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n",
    "        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n",
    "        linpar = self.bias + xwx + zwz + u \n",
    "        return linpar\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n",
    "        # output dimension is T*q  + p (for Z and U, respectively)\n",
    "        self.enc_model = nn.Sequential(\n",
    "            nn.Linear(in_features=T*(p+k), out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features = T*q + p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n",
    "        zu = self.enc_model(xy)\n",
    "        return self.split_zu(zu)\n",
    "\n",
    "    def split_zu(self, zu):\n",
    "        #output dimension of size (T*Z), p\n",
    "        z, u = torch.split(zu, [T*q, p], dim=1)\n",
    "        z = z.reshape((z.shape[0], T, q))\n",
    "        u = u.unsqueeze(1)\n",
    "        return (z, u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_true = GLLVM_longitudinal()\n",
    "dat_true = gl_true.sample(n)\n",
    "\n",
    "gl = GLLVM_longitudinal()\n",
    "\n",
    "gl = copy.deepcopy(gl_true) #making it start with the same parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(input, target, sign=1):\n",
    "    return sign * torch.sum(input*target, dim=[1,2]).mean()\n",
    "\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target, input_sim, target_sim):\n",
    "        return torch.mean(input * target - input_sim * target_sim)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_fit(input, target):\n",
    "    with torch.no_grad():\n",
    "        return torch.sum(torch.pow(input - target,2), dim=[1,2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n",
    "\n",
    "# train the encoder\n",
    "\n",
    "gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tup[0] for tup in gl.decoder.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_opt = torch.optim.Adam(gl.decoder.parameters())\n",
    "criterion = MyLoss()\n",
    "optimizer = torch.optim.Adam(gl.decoder.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=1)\n",
    "\n",
    "# Create a copy of the model's parameters at the end of the last epoch\n",
    "reference_params = [param.clone().detach() for param in gl.decoder.parameters()]\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(1, epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n",
    "\n",
    "    # train the encoder\n",
    "\n",
    "    encoder_loss = gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=10)\n",
    "    \n",
    "    # compute SPRIME sample step\n",
    "\n",
    "    # compute imputing values\n",
    "    with torch.no_grad():\n",
    "        zhat_true, uhat_true = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n",
    "        zhat_sim, uhat_sim = gl.encoder(dat_sim[\"x\"], dat_sim[\"y\"])\n",
    "    \n",
    "\n",
    "    linpar_sample = gl.decoder(dat_true[\"x\"], zhat_true, uhat_true)\n",
    "    linpar_sim = gl.decoder(dat_sim[\"x\"], zhat_sim, uhat_sim)\n",
    "    loss = criterion(linpar_sample, dat_true[\"y\"], linpar_sim, dat_sim[\"y\"])\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(gl.decoder.parameters(), max_norm=.1)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Update nuisance parameters\n",
    "    with torch.no_grad():\n",
    "        gl.update_nuisance_parameters(dat_true[\"y\"], linpar_sample, zhat_true, uhat_true, lr=0.0)\n",
    "\n",
    "    # evaluate the model\n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss = evaluate_fit(linpar_sample, dat_true[\"y\"])\n",
    "\n",
    "        # Get the learning rates\n",
    "        learning_rates = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "\n",
    "        # Calculate the mean learning rate\n",
    "        mean_learning_rate = sum(learning_rates) / len(learning_rates)\n",
    "\n",
    "        # Compute MSE between reference parameters and current parameters\n",
    "        current_params = gl.decoder.parameters()\n",
    "        mse = sum(torch.sum((param - ref_param)**2) for param, ref_param in zip(current_params, reference_params))\n",
    "        mse /= len(reference_params)  # Divide by the number of parameters\n",
    "    \n",
    "        print(f\"MSE: {mse.item()}\")\n",
    "\n",
    "        # Update reference parameters for the next epoch\n",
    "        reference_params = [param.clone().detach() for param in gl.decoder.parameters()]\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}, loss = {loss:.2f}, encoder_loss = {encoder_loss:.2f}, lr={mean_learning_rate}, delta_par = {mse}, phi= {gl.phi}, var_u ={gl.var_u[0,0,0]}, var_y={gl.var_y[0,0,0]}\")\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        #dat_sim = gl.sample(n)\n",
    "    #linpar = gl.decoder(dat_sim[\"x\"], dat_sim[\"z\"], dat_sim[\"u\"])\n",
    "\n",
    "    # decoder_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    zhat, uhat = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n",
    "    yhat = gl.decoder(dat_true[\"x\"], zhat, uhat)\n",
    "\n",
    "plt.scatter(dat_true[\"y\"], yhat)\n",
    "plt.plot([-20, 20], [-20, 20], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhat = zhat*-1\n",
    "plt.scatter(zhat, dat_true[\"z\"])\n",
    "plt.plot([-10, 10], [-10,10], color=\"red\")\n",
    "plt.plot([-10, 10], [10,-10], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 12 randomly selected Z\n",
    "index = np.random.choice(range(n), 12, replace=False)\n",
    "fig, axs = plt.subplots(3, 4)\n",
    "\n",
    "zhat= -zhat\n",
    "fig.suptitle(\"Ztrue vs Zest across time\")\n",
    "for i in range(12):\n",
    "    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n",
    "    axs[i//4, i%4].plot(dat_true[\"z\"][index[i],:, 0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_true = gl_true.decoder.parameters().__next__().detach().squeeze()\n",
    "par_est = gl.decoder.parameters().__next__() .detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(par_true, par_est)\n",
    "plt.plot([-2,2], [2, -2])\n",
    "plt.plot([-2,2], [-2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details on tensor products calculations\n",
    "We now show the details on the tensor products, for instance for computing `xb @ wx`. `xb` is of size `(n, T, q)` and `wx` is of size `(T, q, p)`. We want a result of size `(n, T, p)`. First we add a dimension for `xb`:\n",
    "\n",
    "`xb.unsqueeze(2)` which yields a dimensions of `(n, T, 1, q)`\n",
    "\n",
    "which we then multiply by `wz`:\n",
    "\n",
    "`(n, T, 1, q) @ (1, q, p)` -> `(n, T, 1, p)`\n",
    "\n",
    "where the first dimension of `wx` has been broadcasted.\n",
    "\n",
    "Finally, we squeeze to obtain `(n, T, p)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02379b91627ead677140c441995c323138285dea806245dca7a173ada35ac023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# if Z is not given, we generate it
if (is.null(Z)) {
Z <- matrix(rnorm(n*q), n, q)
}
Z <- matrix(rnorm(n*q), n, q)
# compute the linear predictor
linpred <- Z %*% t(Lambda)
means <- linpred
# generate noise: will be used for the gaussian random variables
noise <- matrix(rnorm(n*p), n, p) %*% diag(1/phi) # this is a bad implementation but illustrates well
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli] <- 1/(1+exp(-linpred[,bernoulli]))
p
bernoulli <- 1:5
gaussian <- 6_10
gaussian <- 6:10
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli] <- 1/(1+exp(-linpred[,bernoulli]))
# Placeholder for the responses
Y <- matrix(nrow=n, ncol=p)
# Generate all gaussian:
Y[,gaussian] <- linpred[,gaussian] + noise[,gaussian]
# Generate all Bernoulli
Y[,bernoulli] <- apply(means[,bernoulli], 2, function(prob) rbinom(n, 1, prob))
Y
# Generate data from the model
generator_mixture <- function(n, Lambda, phi, gaussian_idx, bernoulli_idx, Z=NULL){
p <- nrow(Lambda)
q <- ncol(Lambda)
# if Z is not given, we generate it
if (is.null(Z)) {
Z <- matrix(rnorm(n*q), n, q)
}
# compute the linear predictor
linpred <- Z %*% t(Lambda)
means <- linpred
# generate noise: will be used for the gaussian random variables
noise <- matrix(rnorm(n*p), n, p) %*% diag(1/phi) # this is a bad implementation but illustrates well
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli] <- 1/(1+exp(-linpred[,bernoulli]))
# Placeholder for the responses
Y <- matrix(nrow=n, ncol=p)
# Generate all gaussian:
Y[,gaussian] <- linpred[,gaussian] + noise[,gaussian]
# Generate all Bernoulli
Y[,bernoulli] <- apply(means[,bernoulli], 2, function(prob) rbinom(n, 1, prob))
# generate observations
return(list(Y=Y, Z=Z))
}
random_gradient_mixture <- function(Y, Lambda, phi, gaussian_idx, bernoulli_idx) {
n <- nrow(Y)
p <- nrow(Lambda)
q <- ncol(Lambda)
# generate data from the model at current estimate Lambda and phi. this is used to estimate the expectation
data_gen <- generator_mixture(n, Lambda, phi, gaussian_idx, bernoulli_idx)
Y_gen <- data_gen$Y
# for both Y and Y_gen, compute Zhat and Zhat_gen
Zhat <- Z_estimator(Y, Lambda, phi)
Zhat_gen <- Z_estimator(Y_gen, Lambda, phi)
# compute the gradient for Lambda
gradient <- (t(Y) %*% Zhat - t(Y_gen) %*% Zhat_gen)/n
return(gradient)
}
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
gaussian_idx
bernoulli_idx
p
# Generate data from the model
generator_mixture <- function(n, Lambda, phi, gaussian_idx, bernoulli_idx, Z=NULL){
p <- nrow(Lambda)
q <- ncol(Lambda)
# if Z is not given, we generate it
if (is.null(Z)) {
Z <- matrix(rnorm(n*q), n, q)
}
# compute the linear predictor
linpred <- Z %*% t(Lambda)
means <- linpred
# generate noise: will be used for the gaussian random variables
noise <- matrix(rnorm(n*p), n, p) %*% diag(1/phi) # this is a bad implementation but illustrates well
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli_idx] <- 1/(1+exp(-linpred[,bernoulli_idx]))
# Placeholder for the responses
Y <- matrix(nrow=n, ncol=p)
# Generate all gaussian:
Y[,gaussian_idx] <- linpred[,gaussian_idx] + noise[,gaussian_idx]
# Generate all Bernoulli
Y[,bernoulli_idx] <- apply(means[,bernoulli_idx], 2, function(prob) rbinom(n, 1, prob))
# generate observations
return(list(Y=Y, Z=Z))
}
random_gradient_mixture <- function(Y, Lambda, phi, gaussian_idx, bernoulli_idx) {
n <- nrow(Y)
p <- nrow(Lambda)
q <- ncol(Lambda)
# generate data from the model at current estimate Lambda and phi. this is used to estimate the expectation
data_gen <- generator_mixture(n, Lambda, phi, gaussian_idx, bernoulli_idx)
Y_gen <- data_gen$Y
# for both Y and Y_gen, compute Zhat and Zhat_gen
Zhat <- Z_estimator(Y, Lambda, phi)
Zhat_gen <- Z_estimator(Y_gen, Lambda, phi)
# compute the gradient for Lambda
gradient <- (t(Y) %*% Zhat - t(Y_gen) %*% Zhat_gen)/n
return(gradient)
}
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 100
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 100
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
set.seed(1234)
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
set.seed(12345)
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
set.seed(123)
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Generate data from the model
generator_mixture <- function(n, Lambda, phi, gaussian_idx, bernoulli_idx, Z=NULL){
p <- nrow(Lambda)
q <- ncol(Lambda)
# if Z is not given, we generate it
if (is.null(Z)) {
Z <- matrix(rnorm(n*q), n, q)
}
# compute the linear predictor
linpred <- Z %*% t(Lambda)
means <- linpred
# generate noise: will be used for the gaussian random variables
noise <- matrix(rnorm(n*p), n, p) %*% diag(1/phi) # this is a bad implementation but illustrates well
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli_idx] <- 1/(1+exp(-linpred[,bernoulli_idx]))
# Placeholder for the responses
Y <- matrix(nrow=n, ncol=p)
# Generate all gaussian:
Y[,gaussian_idx] <- linpred[,gaussian_idx] + noise[,gaussian_idx]
# Generate all Bernoulli
Y[,bernoulli_idx] <- apply(means[,bernoulli_idx], 2, function(prob) rbinom(n, 1, prob))
# generate observations
return(list(Y=Y, Z=Z))
}
random_gradient_mixture <- function(Y, Lambda, phi, gaussian_idx, bernoulli_idx) {
n <- nrow(Y)
p <- nrow(Lambda)
q <- ncol(Lambda)
# generate data from the model at current estimate Lambda and phi. this is used to estimate the expectation
data_gen <- generator_mixture(n, Lambda, phi, gaussian_idx, bernoulli_idx)
Y_gen <- data_gen$Y
Y[,bernoulli_idx] <- (Y[,bernoulli_idx]-.5)*4
Y_gen[,bernoulli_idx] <- (Y_gen[,bernoulli_idx]-.5)*4
# for both Y and Y_gen, compute Zhat and Zhat_gen
Zhat <- Z_estimator(Y, Lambda, phi)
Zhat_gen <- Z_estimator(Y_gen, Lambda, phi)
# compute the gradient for Lambda
gradient <- (t(Y) %*% Zhat - t(Y_gen) %*% Zhat_gen)/n
return(gradient)
}
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
set.seed(123)
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
# Generate data from the model
generator_mixture <- function(n, Lambda, phi, gaussian_idx, bernoulli_idx, Z=NULL){
p <- nrow(Lambda)
q <- ncol(Lambda)
# if Z is not given, we generate it
if (is.null(Z)) {
Z <- matrix(rnorm(n*q), n, q)
}
# compute the linear predictor
linpred <- Z %*% t(Lambda)
means <- linpred
# generate noise: will be used for the gaussian random variables
noise <- matrix(rnorm(n*p), n, p) %*% diag(1/phi) # this is a bad implementation but illustrates well
# compute the mean of the Bernoulli using the logistic function
means[,bernoulli_idx] <- 1/(1+exp(-linpred[,bernoulli_idx]))
# Placeholder for the responses
Y <- matrix(nrow=n, ncol=p)
# Generate all gaussian:
Y[,gaussian_idx] <- linpred[,gaussian_idx] + noise[,gaussian_idx]
# Generate all Bernoulli
Y[,bernoulli_idx] <- apply(means[,bernoulli_idx], 2, function(prob) rbinom(n, 1, prob))
# generate observations
return(list(Y=Y, Z=Z))
}
random_gradient_mixture <- function(Y, Lambda, phi, gaussian_idx, bernoulli_idx) {
n <- nrow(Y)
p <- nrow(Lambda)
q <- ncol(Lambda)
# generate data from the model at current estimate Lambda and phi. this is used to estimate the expectation
data_gen <- generator_mixture(n, Lambda, phi, gaussian_idx, bernoulli_idx)
Y_gen <- data_gen$Y
# for both Y and Y_gen, compute Zhat and Zhat_gen
Zhat <- Z_estimator(Y, Lambda, phi)
Zhat_gen <- Z_estimator(Y_gen, Lambda, phi)
# compute the gradient for Lambda
gradient <- (t(Y) %*% Zhat - t(Y_gen) %*% Zhat_gen)/n
return(gradient)
}
# Dimensional setting
n <- 1000
p <- 5
q <- 2
gaussian_idx <- 1:3
bernoulli_idx <- 4:5
# True parameters
set.seed(123)
Lambda_0 <- matrix(rnorm(p*q), p, q)
phi_0 <- rep(1, p)
# True data
data_0 <- generator_mixture(n, Lambda_0, phi_0, gaussian_idx, bernoulli_idx)
Y_0 <- data_0$Y
SY <- t(Y_0) %*% Y_0 / n
# Initialize model parameters
Lambda <- matrix(rnorm(p*q), p, q) # random initalization...
phi <- phi_0 # assumed known
# Define a convergence criterion
compute_criterion <- function(SY, Lambda, phi) {
# We take the Frobenius' norm of the difference between the observed covariance against its expectation
mean((Lambda %*% t(Lambda) + diag(phi) - SY)^2) # This is computationally bad and should never be computed in production.
}
# Stochastic approximation Algorithm
num_iterations = 1000
Lambda_hist <- matrix(nrow=num_iterations, ncol=p*q) # storing the values of Lambda at each iteration.
criterion_hist <- rep(NA, num_iterations)
for (i in 1:num_iterations) {
gradient <- random_gradient_mixture(data_0$Y, Lambda, phi, gaussian_idx, bernoulli_idx)
Lambda <- Lambda + 1/sqrt(i) * gradient  # notice the diminishing step size
# Compute the criterion
criterion <- compute_criterion(SY, Lambda, phi)
# Store the iteration results
Lambda_hist[i,] <- as.vector(Lambda)
criterion_hist[i] <- criterion
}
# plotting the results
par(mfrow=c(1,2))
plot(criterion_hist, main="Criterion convergence")
ts.plot(Lambda_hist, main="Lambda convergence")
par(mfrow=c(1,1))
rotate_and_plot_loadings(Lambda, Lambda_0)
